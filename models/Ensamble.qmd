---
title: "Ensamble"
author: "Karol Topor"
format:
  html:
    toc: true
    toc-depth: 6         
    toc-title: "Outline"
    number-sections: true
---

# Load libraries

```{r message=FALSE, warning=FALSE}
if (!require(readr)) install.packages("readr")
if (!require(data.table)) install.packages("data.table")
if (!require(dplyr)) install.packages("dplyr")
if (!require(caret)) install.packages("caret")
if (!require(tictoc)) install.packages("tictoc")
if (!require(Metrics)) install.packages("Metrics")
if (!require(MetricsWeighted)) install.packages("MetricsWeighted")

library(readr)
library(data.table)
library(dplyr)
library(caret)
library(catboost)
library(tictoc)
library(Metrics) 
library(MetricsWeighted)
```

# Load anddata

```{r}
# Load data
data <- read_csv("../data/final_data.csv", 
    col_types = cols(Sale_Date_Raw = col_date(format = "%Y-%m-%d"))
  )

# View data
head(data)
```

## Columns names

```{r}
colnames(data)
```

## Describe the data

```{r}
# describe the data
summary(data)
```

# Modeling

## Prepare data

```{r}
set.seed(42)
deterministic = TRUE
idx   <- createDataPartition(data$Sale_Price_Raw, p = 0.80, list = FALSE)
train <- data[idx,  ]
test  <- data[-idx,  ]

#── feature prep helper ──────────────────────────────────────
prep <- function(df) {
  df %>%
    mutate(
      Sale_Date_Raw = as.numeric(Sale_Date_Raw),
      across(where(is.character), as.factor)
    ) %>%
    select(-Sale_Price_Raw)
}

X_tr <- prep(train); 
y_tr <- train$Sale_Price_Raw

X_te <- prep(test);  
y_te <- test$Sale_Price_Raw

train_pool <- catboost.load_pool(X_tr, label = y_tr)
test_pool <- catboost.load_pool(X_te, label = y_te)
```

## Simple CatBoost model

### CPU

```{r}
tic("CatBoost training on CPU")

model_cpu <- catboost.train(
  learn_pool = train_pool,
  params = list(
    loss_function = "RMSE",
    iterations    = 64,
    learning_rate = 0.1,
    depth         = 3,
    l2_leaf_reg   = 3,
    logging_level = "Silent"
))

# quick sanity check
pred  <- catboost.predict(model_cpu, test_pool)
toc()
cat("RMSE:", round(sqrt(mean((pred - y_te)^2)), 2), "\n")
```

```{r}
catboost.get_feature_importance(model_cpu,
                                pool = NULL,
                                type = "FeatureImportance",
                                thread_count = -1)
```

### GPU
> Training on GPU somehow breakes the rendering of the document, so it's commented out.

```{r}
# tic("CatBoost training on GPU")
# model_gpu <- catboost.train(
#   learn_pool = train_pool,
#   params = list(
#     loss_function = "RMSE",
#     iterations    = 64,
#     learning_rate = 0.1,
#     depth         = 3,
#     l2_leaf_reg   = 3,
#     logging_level = "Silent",
#     task_type = 'GPU'
# ))
# 
# # quick sanity check
# pred  <- catboost.predict(model_gpu, test_pool)
# cat("RMSE:", round(sqrt(mean((pred - y_te)^2)), 2), "\n")
# toc()
```

```{r}
# catboost.get_feature_importance(model_gpu,
#                                 pool = NULL,
#                                 type = "FeatureImportance",
#                                 thread_count = -1)
```

# Cross Validation

```{r}
# X <- data %>% 
#   select(-Sale_Price_Raw) %>% 
#   mutate(
#     Sale_Date_Raw = as.numeric(Sale_Date_Raw),
#     across(where(is.character), as.factor)
#   )
# y <- data$Sale_Price_Raw

# 5-fold CV ----------------------------------------------------------------------
# ctrl <- trainControl(method = "cv", number = 5, repeats = 3)
# 
# grid <- expand.grid(
#   depth         = c(8, 9, 10, 11, 15),
#   learning_rate = c(0.05, 0.03, 0.02, 0.01),
#   iterations    = c(800, 1000, 1200, 1500),
#   l2_leaf_reg   = c(3, 5, 8),
#   rsm = 1,
#   border_count = c(32, 64, 128)
# )
# 
# tic("CatBoost grid search (GPU)")
# cb_fit <- train(
#   x = X, y = y,
#   method     = catboost.caret,
#   tuneGrid   = grid,
#   trControl  = ctrl,
#   metric     = "RMSE",
#   loss_function = "RMSE",          # passed straight to CatBoost
#   task_type     = "GPU",
#   logging_level = "Silent"
# )
# toc()
# 
# cb_fit$bestTune     # best hyper-parameters
# cb_fit$results
```

> This took to long so I used the FH High-Performance Computing Cluster (HPC)

And run a few CV rounds

## First CV round

```{r, echo=FALSE, warning=FALSE}
files   <- list.files("rjobs/res", pattern = "\\.res$", full.names = TRUE)

results <- rbindlist(lapply(files, fread), fill = TRUE)

## 2 · build the ggplot ----
p <- ggplot(
  results,
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

After a first run with 5 hyperparameters the result is hard to interpret but we can clearly see that higher **border_count** yields overall lower RMSE. Therefor taking a closer look at border count 128.

```{r, echo=FALSE, warning=FALSE}
p <- ggplot(
  results[border_count == 128],
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

The filtered results for border_count 128 reveal that **tree depth** 9,10 and 11 show the most promising RMSE values. Furthermore **learning rate** of 0.03 performs best among these three tree depths. **l2_leaf_reg** of 3 is also performing best among these three.

Get model with best RMSE:

```{r, echo=FALSE}
best <- results[which.min(RMSE)]
best
```

## Second CV round
After identifying the best hyper-parameters in the first round, we can now run a second round with higher border count and fewer combinations for other hyper parameters.

```{r, echo=FALSE, warning=FALSE}
files   <- list.files("rjobs/res_02", pattern = "\\.res$", full.names = TRUE)

results <- rbindlist(lapply(files, fread), fill = TRUE)

## 2 · build the ggplot ----
p <- ggplot(
  results,
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

Again we can see that higher **border_count** yields overall lower RMSE. Therefor taking a closer look at border count 512.

```{r, echo=FALSE, warning=FALSE}
## 2 · build the ggplot ----
p <- ggplot(
  results[border_count == 512],
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

Surprisingly, this time a **learning rate** of 0.2 performs best.

Get model with lowest RMSE

```{r, echo=FALSE}
best <- results[which.min(RMSE)]
best
```

## Third VC round
After seeing the big drop between border_count of 256 and 512 I am convinced that we can go further with the border_count.

```{r, echo=FALSE, warning=FALSE}
files   <- list.files("rjobs/res_03", pattern = "\\.res$", full.names = TRUE)

results <- rbindlist(lapply(files, fread), fill = TRUE)

## 2 · build the ggplot ----
p <- ggplot(
  results,
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

First time we see that RMSE does not drop substantially with higher border_count. But we can see that **learning rate** of 0.03 performs best among the three tree depths. **l2_leaf_reg** of 3 is also performing best among these three.

```{r, echo=FALSE, warning=FALSE}
## 2 · build the ggplot ----
p <- ggplot(
  results[border_count == 8192],
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

Get model with lowest RMSE

```{r, echo=FALSE}
best <- results[which.min(RMSE)]
best
```

## Fourth VC round
Further exploring higher border_count.

```{r, echo=FALSE, warning=FALSE}
files   <- list.files("rjobs/res_04", pattern = "\\.res$", full.names = TRUE)

results <- rbindlist(lapply(files, fread), fill = TRUE)

## 2 · build the ggplot ----
p <- ggplot(
  results,
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

Get model with lowest RMSE

```{r, echo=FALSE}
best <- results[which.min(RMSE)]
best
```

## Fitfth VC round
After going back and forth and looking at the results I decided to run a final CV round with a range or **border_count** raging from 1024 (2^10) to 65535 (2^16 - 1 - which is the maximum for this parameter). A **l2_leaf_reg** of 3, **iterations** of 1000 and 1200, **learning rate** of 0.02 and 0.03 and a **three depth** of 10. 

```{r, echo=FALSE, warning=FALSE}
files   <- list.files("rjobs/res_05", pattern = "\\.res$", full.names = TRUE)

results <- rbindlist(lapply(files, fread), fill = TRUE)

## 2 · build the ggplot ----
p <- ggplot(
  results,
  aes(
    x      = factor(border_count),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    #shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  # facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

Get model with lowest RMSE

```{r, echo=FALSE}
best <- results[which.min(RMSE)]
best
```

# Final Model

Based on the results of the CV rounds, we will use the following hyper-parameters for the final model:

```{r}
set.seed(42)
idx   <- createDataPartition(data$Sale_Price_Raw, p = 0.80, list = FALSE)
train <- data[idx,  ]
test  <- data[-idx,  ]

tic("CatBoost training Final Model")

#── feature prep helper ──────────────────────────────────────
prep <- function(df) {
  df %>%
    mutate(
      Sale_Date_Raw = as.numeric(Sale_Date_Raw),
      across(where(is.character), as.factor)
    ) %>%
    select(-Sale_Price_Raw)
}

X_tr <- prep(train); 
y_tr <- train$Sale_Price_Raw

X_te <- prep(test);  
y_te <- test$Sale_Price_Raw

#── CatBoost pools (no cat_features param needed) ─────────────
tr_pool <- catboost.load_pool(X_tr, label = y_tr)
te_pool <- catboost.load_pool(X_te, label = y_te)

#── train final model on GPU ─────────────────────────────────
final_model <- catboost.train(
  learn_pool = tr_pool,
  test_pool  = te_pool,
  params = list(
    depth         = 10,
    learning_rate = 0.03,
    iterations    = 1200,
    loss_function = "RMSE",
    l2_leaf_reg   = 3,
    task_type     = "CPU",
    border_count  = 32768	,
    logging_level = "Silent"
  )
)
toc()

#── evaluate on held-out test set ────────────────────────────
pred <- catboost.predict(final_model, te_pool)
rmse <- sqrt(mean((pred - y_te)^2))
cat("Test RMSE:", round(rmse, 2), "\n")
toc()
```

## Actuall vs. predicted sale prices:

```{r}
plot_df <- data.frame(
  Actual    = y_te,
  Predicted = pred
)

ggplot(plot_df, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.25) +
  geom_abline(slope = 1, intercept = 0,
              linetype = "dashed", color = "red", size = 0.8) +
  labs(
    title = "Predicted vs. Actual Sale Prices",
    x     = "Actual Sale_Price_Raw",
    y     = "Predicted Sale_Price_Raw"
  ) +
  theme_minimal()
```


## Metrics

```{r}
# MAPE (Mean Absolute Percentage Error)
mape_val <- mape(y_te, pred)
mae_val <- mae(y_te, pred)
# RMSE (Root Mean Squared Error)
rmse_val <- rmse(y_te, pred)

# R² (Coefficient of Determination)
r2_val <- R2(pred, y_te)

# Print
cat(sprintf("MAPE: %.4f\nMAE: %.4f\nRMSE: %.4f\nR²: %.4f\n", mape_val, mae_val, rmse_val, r2_val))
```


```{r}

```

