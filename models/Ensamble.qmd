---
title: "Ensamble"
author: "Karol Topor"
format: 
  html:
    toc: true
    toc-depth: 3
    theme: cosmo 
    code-fold: true
    code-summary: "Show/Hide Code"
    df-print: kable
editor: visual
---

# Load libraries

```{r}
if (!require(readr)) install.packages("readr")
if (!require(data.table)) install.packages("data.table")
if (!require(dplyr)) install.packages("dplyr")
if (!require(caret)) install.packages("caret")
if (!require(tictoc)) install.packages("tictoc")

library(readr)
library(data.table)
library(dplyr)
library(caret)
library(catboost)
library(tictoc)
```

# Load and prepare data

```{r}
# Load data
data <- read_csv("../data/final_data.csv", 
    col_types = cols(Sale_Date_Raw = col_date(format = "%Y-%m-%d"))
  )

# View data
head(data)
```

## Columns names

```{r}
colnames(data)
```

## Describe the data

```{r}
# describe the data
summary(data)
```

# Modeling

## Prepare data

```{r}
set.seed(42)
deterministic = TRUE
idx   <- createDataPartition(data$Sale_Price_Raw, p = 0.80, list = FALSE)
train <- data[idx,  ]
test  <- data[-idx,  ]

#── feature prep helper ──────────────────────────────────────
prep <- function(df) {
  df %>%
    mutate(
      Sale_Date_Raw = as.numeric(Sale_Date_Raw),
      across(where(is.character), as.factor)
    ) %>%
    select(-Sale_Price_Raw)
}

X_tr <- prep(train); 
y_tr <- train$Sale_Price_Raw

X_te <- prep(test);  
y_te <- test$Sale_Price_Raw

#── CatBoost pools (no cat_features param needed) ─────────────
train_pool <- catboost.load_pool(X_tr, label = y_tr)
test_pool <- catboost.load_pool(X_te, label = y_te)
```

## Simple CatBoost model

### CPU

```{r}
tic("CatBoost training on CPU")

model_cpu <- catboost.train(
  learn_pool = train_pool,
  params = list(
    loss_function = "RMSE",
    iterations    = 64,
    learning_rate = 0.1,
    depth         = 3,
    l2_leaf_reg   = 3,
    logging_level = "Silent"
))

# quick sanity check
pred  <- catboost.predict(model_cpu, test_pool)
cat("RMSE:", round(sqrt(mean((pred - y_te)^2)), 2), "\n")
toc()
```

```{r}
catboost.get_feature_importance(model_cpu,
                                pool = NULL,
                                type = "FeatureImportance",
                                thread_count = -1)
```

### GPU

```{r}
tic("CatBoost training on GPU")
model_gpu <- catboost.train(
  learn_pool = train_pool,
  params = list(
    loss_function = "RMSE",
    iterations    = 64,
    learning_rate = 0.1,
    depth         = 3,
    l2_leaf_reg   = 3,
    logging_level = "Silent",
    task_type = 'GPU'
))

# quick sanity check
pred  <- catboost.predict(model_gpu, test_pool)
cat("RMSE:", round(sqrt(mean((pred - y_te)^2)), 2), "\n")
toc()
```

```{r}
catboost.get_feature_importance(model_gpu,
                                pool = NULL,
                                type = "FeatureImportance",
                                thread_count = -1)
```

# Cross Validation

```{r}
# tidy predictors ----------------------------------------------------------------
# X <- data %>% 
#   select(-Sale_Price_Raw) %>% 
#   mutate(
#     Sale_Date_Raw = as.numeric(Sale_Date_Raw),
#     across(where(is.character), as.factor)
#   )
# y <- data$Sale_Price_Raw

# 5-fold CV ----------------------------------------------------------------------
# ctrl <- trainControl(method = "cv", number = 5, repeats = 3)
# 
# grid <- expand.grid(
#   depth         = c(8, 9, 10, 11, 15),
#   learning_rate = c(0.05, 0.03, 0.02, 0.01),
#   iterations    = c(800, 1000, 1200, 1500),
#   l2_leaf_reg   = c(3, 5, 8),
#   rsm = 1,
#   border_count = c(32, 64, 128)
# )
# 
# tic("CatBoost grid search (GPU)")
# cb_fit <- train(
#   x = X, y = y,
#   method     = catboost.caret,
#   tuneGrid   = grid,
#   trControl  = ctrl,
#   metric     = "RMSE",
#   loss_function = "RMSE",          # passed straight to CatBoost
#   task_type     = "GPU",
#   logging_level = "Silent"
# )
# toc()
# 
# cb_fit$bestTune     # best hyper-parameters
# cb_fit$results
```

> This took to long so I used the FH High-Performance Computing Cluster (HPC)

And run a few CV rounds

## First CV round

```{r}
files   <- list.files("rjobs/res", pattern = "\\.res$", full.names = TRUE)

results <- rbindlist(lapply(files, fread), fill = TRUE)

## 2 · build the ggplot ----
p <- ggplot(
  results,
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

After taking a closer look at **border_count** 128

```{r}
## 2 · build the ggplot ----
p <- ggplot(
  results[border_count == 128],
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

Get model with best RMSE

```{r}
best <- results[which.min(RMSE)]
best
```

## Second CV round

```{r}
files   <- list.files("rjobs/res_02", pattern = "\\.res$", full.names = TRUE)

results <- rbindlist(lapply(files, fread), fill = TRUE)

## 2 · build the ggplot ----
p <- ggplot(
  results,
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

```{r}
## 2 · build the ggplot ----
p <- ggplot(
  results[border_count == 512],
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

Get model with lowest RMSE

```{r}
best <- results[which.min(RMSE)]
best
```

## Third VC round

```{r}
files   <- list.files("rjobs/res_03", pattern = "\\.res$", full.names = TRUE)

results <- rbindlist(lapply(files, fread), fill = TRUE)

## 2 · build the ggplot ----
p <- ggplot(
  results,
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

```{r}
## 2 · build the ggplot ----
p <- ggplot(
  results[border_count == 8192],
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

Get model with lowest RMSE

```{r}
best <- results[which.min(RMSE)]
best
```

## Fourth VC round

```{r}
files   <- list.files("rjobs/res_04", pattern = "\\.res$", full.names = TRUE)

results <- rbindlist(lapply(files, fread), fill = TRUE)

## 2 · build the ggplot ----
p <- ggplot(
  results,
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

Get model with lowest RMSE

```{r}
best <- results[which.min(RMSE)]
best
```

## Fitfth VC round

```{r}
files   <- list.files("rjobs/res_05", pattern = "\\.res$", full.names = TRUE)

results <- rbindlist(lapply(files, fread), fill = TRUE)

## 2 · build the ggplot ----
p <- ggplot(
  results,
  aes(
    x      = factor(border_count),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    #shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  # facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

Get model with lowest RMSE

```{r}
best <- results[border_count == 4096][which.min(RMSE)]
best
```

# Final Model

Based on the results of the CV rounds, we will use the following hyper-parameters for the final model:

```{r}
set.seed(42)
idx   <- createDataPartition(data$Sale_Price_Raw, p = 0.80, list = FALSE)
train <- data[idx,  ]
test  <- data[-idx,  ]

tic("CatBoost training Final Model")

#── feature prep helper ──────────────────────────────────────
prep <- function(df) {
  df %>%
    mutate(
      Sale_Date_Raw = as.numeric(Sale_Date_Raw),
      across(where(is.character), as.factor)
    ) %>%
    select(-Sale_Price_Raw)
}

X_tr <- prep(train); y_tr <- train$Sale_Price_Raw
X_te <- prep(test);  y_te <- test$Sale_Price_Raw

#── CatBoost pools (no cat_features param needed) ─────────────
tr_pool <- catboost.load_pool(X_tr, label = y_tr)
te_pool <- catboost.load_pool(X_te, label = y_te)

#── train final model on GPU ─────────────────────────────────
final_model <- catboost.train(
  learn_pool = tr_pool,
  test_pool  = te_pool,
  params = list(
    depth         = 10,
    learning_rate = 0.03,
    iterations    = 1200,
    loss_function = "RMSE",
    l2_leaf_reg   = 3,
    task_type     = "GPU",
    border_count  = 32768	,
    logging_level = "Silent"
  )
)
toc()

#── evaluate on held-out test set ────────────────────────────
pred <- catboost.predict(final_model, te_pool)
rmse <- sqrt(mean((pred - y_te)^2))
cat("Test RMSE:", round(rmse, 2), "\n")
toc()


```

```{r}
#── Build comparison dataframe ───────────────────────────────
plot_df <- data.frame(
  Actual    = y_te,
  Predicted = pred
)

#── Plot: Predicted vs. Actual ───────────────────────────────
ggplot(plot_df, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.25) +
  geom_abline(slope = 1, intercept = 0,
              linetype = "dashed", color = "red", size = 0.8) +
  labs(
    title = "Predicted vs. Actual Sale Prices",
    x     = "Actual Sale_Price_Raw",
    y     = "Predicted Sale_Price_Raw"
  ) +
  theme_minimal()

```

```{r}

```
