---
title: "modeling"
format: html
editor: visual
---

1. Load the Complete, Engineered Dataset
We start with the final Parquet file. It's fast and contains all 380k+ valid sales records with all our engineered features.

```{r}
library(DBI)
library(duckdb)
library(arrow)
library(dplyr)
library(knitr)
library(tidyr)
library(ranger)
library(lubridate)
library(kableExtra)
library(mice) # --- ADD THE MICE LIBRARY FOR SMART IMPUTATION ---
library(tidymodels)
library(xgboost)
library(doParallel) # For parallel processing
```


```{r}
# Load the final, clean dataset from the Parquet file
full_data <- read_parquet("../data/stage/full_data_for_modeling.parquet")

cat("--- Data loaded successfully ---\n")
cat("Full dataset loaded. Dimensions:", dim(full_data), "\n")
print("Column names in full_data:")
print(colnames(full_data))

```
```{r}
# --- 0. Load Libraries ---
# Make sure all necessary libraries are loaded.
# It's good practice to load them at the beginning of your script.
library(tidyverse)    # For dplyr, ggplot2, etc. (forcats is part of tidyverse)
library(tidymodels)   # For the core modeling workflow (recipes, parsnip, tune, yardstick, rsample)
library(arrow)        # For reading parquet files
library(doParallel)   # For parallel processing
library(parallel)     # For detectCores
library(foreach)      # For registerDoSEQ() if called explicitly
library(vip)          # For variable importance plots

# --- 2. Initial Data Cleaning & Feature Engineering ---
cat("Starting initial data cleaning and feature engineering...\n")
data_cleaned <- full_data %>%
  mutate(
    # Convert all character columns to factors.
    across(where(is.character), as.factor),
    
    # Ensure all relevant date columns are explicitly converted to Date type.
    Sale_Date = as.Date(Sale_Date),
    Latest_Appraisal_Date = as.Date(Latest_Appraisal_Date),
    Latest_Merge_Event_Date = as.Date(Latest_Merge_Event_Date)
  )
cat("Initial data type conversions done.\n")


# --- 3. Filtering and Target Variable Creation ---
cat("Filtering data and creating target variable...\n")
data_filtered <- data_cleaned %>%
  filter(Total_Building_SqFt > 1) %>%
  filter(Sale_Price > 1000, Sale_Price < 10000000) %>%
  mutate(Price_Per_SqFt = Sale_Price / Total_Building_SqFt) %>%
  filter(Price_Per_SqFt > 1, Price_Per_SqFt < 5000) %>%
  mutate(log_Price_Per_SqFt = log(Price_Per_SqFt))

if (nrow(data_filtered) == 0) {
  stop("Error: No data remaining after filtering. Check filter conditions.")
}
cat("Data dimensions after filtering:", dim(data_filtered), "\n")


# --- 4. Data Splitting ---
cat("Splitting data into training and testing sets...\n")
set.seed(123) # for reproducibility
data_split <- initial_split(data_filtered, prop = 0.80, strata = log_Price_Per_SqFt)
train_data <- training(data_split)
test_data  <- testing(data_split)
cat("Training data dimensions:", dim(train_data), "\n")
cat("Testing data dimensions:", dim(test_data), "\n")


# --- 5. Cross-Validation Folds ---
cat("Creating cross-validation folds...\n")
set.seed(234) # for reproducibility
cv_folds <- vfold_cv(train_data, v = 5, strata = log_Price_Per_SqFt)
cat("Created", cv_folds$splits %>% length(), "CV folds.\n")


# --- 6. Preprocessing Recipe ---
cat("Defining preprocessing recipe...\n")
xgb_recipe <- recipe(log_Price_Per_SqFt ~ ., data = train_data) %>%
  update_role(
    Parcel_Number, ETN, Sale_Date, Price_Per_SqFt,
    Latest_Appraisal_Date, Latest_Merge_Event_Date,
    Sale_Price, # <<<<<<< CRITICAL FIX: Assign Sale_Price an ID role
    new_role = "ID"
  ) %>%
  step_mutate( # Specific imputations for key numeric features
    Total_Bedrooms = if_else(is.na(Total_Bedrooms), 0, Total_Bedrooms),
    Total_Bathrooms = if_else(is.na(Total_Bathrooms), 0, Total_Bathrooms),
    Total_Stories = if_else(is.na(Total_Stories), 1, Total_Stories),
    Num_Buildings_Good_Condition = if_else(is.na(Num_Buildings_Good_Condition), 0, Num_Buildings_Good_Condition)
  ) %>%
  # Handle character to factor conversion first
  step_string2factor(all_nominal_predictors(), -all_outcomes(), -has_role("ID")) %>%
  # Explicitly handle NAs in factors by converting them to a specific level
  # This step is crucial for consistent dummy variable creation
  step_mutate_at(all_nominal_predictors(), fn = ~forcats::fct_explicit_na(., na_level = "Missing_Level")) %>%
  # Now handle novel levels (genuinely new values not seen in training)
  step_novel(all_nominal_predictors(), -all_outcomes(), -has_role("ID"), new_level = "Novel_Category_Level") %>%
  # Group infrequent levels
  step_other(all_nominal_predictors(), -all_outcomes(), -has_role("ID"), threshold = 0.01, other = "Other_Category_Level") %>%
  # Create dummy variables
  step_dummy(all_nominal_predictors(), -all_outcomes(), -has_role("ID"), one_hot = FALSE) %>%
  step_nzv(all_predictors(), -all_outcomes(), -has_role("ID")) %>%
  step_impute_median(all_numeric_predictors(), -all_outcomes(), -has_role("ID"))

cat("Recipe defined.\n")


# --- 7. XGBoost Model Specification for Tuning ---
cat("Defining XGBoost model specification for tuning...\n")
xgb_spec_tune <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  min_n = tune(),
  learn_rate = tune(),
  mtry = tune()
) %>%
  set_engine("xgboost") %>% # importance = "permutation" is not a direct xgb train param, vip handles it.
  set_mode("regression")

cat("XGBoost model specification for tuning defined.\n")


# --- 8. Workflow ---
cat("Creating XGBoost tuning workflow...\n")
xgb_workflow_tune <- workflow() %>%
  add_recipe(xgb_recipe) %>%
  add_model(xgb_spec_tune)

cat("XGBoost tuning workflow defined.\n")


# --- 9. Hyperparameter Grid ---
cat("Creating hyperparameter grid...\n")
set.seed(345)

temp_prepped_recipe <- prep(xgb_recipe, training = train_data)
num_predictors_after_recipe <- ncol(bake(temp_prepped_recipe, new_data = NULL)) - 1

cat("Number of predictors after recipe prepping:", num_predictors_after_recipe, "\n")

xgb_param_set <- extract_parameter_set_dials(xgb_spec_tune)
final_xgb_param_set <- xgb_param_set %>%
  update(mtry = mtry(c(1L, max(1L, num_predictors_after_recipe))))

xgb_grid <- grid_latin_hypercube(
  final_xgb_param_set,
  size = 20
)
cat("Hyperparameter grid created with", nrow(xgb_grid), "combinations.\n")


# --- 10. Parallel Processing Setup ---
cat("Setting up parallel processing...\n")
num_cores_to_use <- 14
if (num_cores_to_use > parallel::detectCores(logical = FALSE)) {
  num_cores_to_use <- parallel::detectCores(logical = FALSE) - 1
}
if (num_cores_to_use < 1) num_cores_to_use <- 1

# if (exists("cl") && inherits(cl, "cluster")) {
#   try(parallel::stopCluster(cl), silent = TRUE) # Try to stop if exists
# }
cl <- parallel::makePSOCKcluster(num_cores_to_use)
doParallel::registerDoParallel(cl)
cat("Parallel processing setup with", num_cores_to_use, "cores.\n")

gc()
cat("Garbage collection run before tuning.\n")


# --- 11. Tune the Model ---
cat("Starting XGBoost tuning... This may take a while.\n")
set.seed(456)
start_time_tuning <- Sys.time()

xgb_tune_results <- tune::tune_grid(
  xgb_workflow_tune,
  resamples = cv_folds,
  grid = xgb_grid,
  metrics = yardstick::metric_set(rmse, mae, rsq),
  control = tune::control_grid(
    save_pred = TRUE,
    verbose = TRUE,
    allow_par = TRUE,
    parallel_over = "everything"
  )
)

end_time_tuning <- Sys.time()
tuning_duration <- end_time_tuning - start_time_tuning
cat("Tuning finished.\n")
cat("Tuning duration:", format(tuning_duration), "\n")

if (exists("cl") && inherits(cl, "cluster")) {
  try(parallel::stopCluster(cl), silent = TRUE)
  cat("Parallel cluster stopped.\n")
}
# Correctly unregister parallel backend
try(registerDoSEQ(), silent = TRUE) # From foreach package, should be available
cat("Parallel backend unregistered (attempted). Switched to sequential processing.\n")

if (nrow(tune::collect_metrics(xgb_tune_results)) == 0) {
    cat("ERROR: All models failed during tuning or no metrics were collected. Showing notes:\n")
    tune::show_notes(xgb_tune_results)
} else {
    cat("Tuning results collected.\n")
}


# --- 12. Analyze Tuning Results & Select Best ---
cat("Tuning results summary:\n")
if (nrow(tune::collect_metrics(xgb_tune_results)) > 0) {
  print(tune::collect_metrics(xgb_tune_results))
  cat("\nBest models based on RMSE:\n")
  tune::show_best(xgb_tune_results, "rmse", n = 5)
  best_xgb_params <- tune::select_best(xgb_tune_results, "rmse")
  cat("\nSelected best hyperparameters for XGBoost (based on RMSE):\n")
  print(best_xgb_params)
} else {
  cat("Skipping analysis of tuning results as no metrics were collected.\n")
  best_xgb_params <- NULL # Ensure it's defined for the next step
}


# --- 13. Finalize Workflow with Best Hyperparameters ---
if (!is.null(best_xgb_params) && nrow(best_xgb_params) > 0) {
  cat("Finalizing workflow with best parameters...\n")
  final_xgb_workflow <- finalize_workflow(
    xgb_workflow_tune,
    best_xgb_params
  )
  cat("Workflow finalized.\n")
} else {
  cat("Skipping workflow finalization as best parameters were not found.\n")
  final_xgb_workflow <- NULL # Ensure it's defined
}


# --- 14. Fit Final Model to Entire Training Data ---
if (!is.null(final_xgb_workflow)) {
  cat("Fitting final XGBoost model to the entire training set...\n")
  start_time_final_fit <- Sys.time()
  final_xgb_fit <- fit(final_xgb_workflow, data = train_data)
  end_time_final_fit <- Sys.time()
  final_fit_duration <- end_time_final_fit - start_time_final_fit
  cat("Final XGBoost model fitted. Duration:", format(final_fit_duration), "\n")
} else {
  cat("Skipping final model fitting as workflow was not finalized.\n")
  final_xgb_fit <- NULL # Ensure it's defined
}


# --- 15. Save the Final Fitted Model ---
if (!is.null(final_xgb_fit)) {
  cat("Saving the final fitted model workflow...\n")
  saveRDS(final_xgb_fit, file = "final_xgb_model_fit.rds")
  cat("Final model saved to final_xgb_model_fit.rds\n")
} else {
  cat("Skipping model saving as final model was not fitted.\n")
}


# --- 16. Evaluate on Test Set ---
if (!is.null(final_xgb_fit)) {
  cat("Evaluating final XGBoost model on the test set...\n")
  # Use a tryCatch for the prediction step as it's where the bake error occurred
  test_predictions <- tryCatch({
    predict(final_xgb_fit, new_data = test_data) %>%
      bind_cols(test_data %>% select(log_Price_Per_SqFt, Price_Per_SqFt, Sale_Price, Total_Building_SqFt)) %>%
      mutate(.pred_original_scale = exp(.pred))
  }, error = function(e) {
    cat("Error during prediction on test set or subsequent piping:\n")
    print(e)
    return(NULL) # Return NULL if prediction fails
  })

  if (!is.null(test_predictions)) {
    xgb_mae_ppsft <- yardstick::mae(test_predictions, truth = Price_Per_SqFt, estimate = .pred_original_scale)
    xgb_rmse_ppsft <- yardstick::rmse(test_predictions, truth = Price_Per_SqFt, estimate = .pred_original_scale)
    xgb_rsq_ppsft <- yardstick::rsq(test_predictions, truth = Price_Per_SqFt, estimate = .pred_original_scale)

    cat("\n--- Tuned XGBoost: Performance on Test Set (Price_Per_SqFt Scale) ---\n")
    cat("MAE (Price_Per_SqFt):", xgb_mae_ppsft$.estimate, "\n")
    cat("RMSE (Price_Per_SqFt):", xgb_rmse_ppsft$.estimate, "\n")
    cat("R-squared (Price_Per_SqFt):", xgb_rsq_ppsft$.estimate, "\n")

    test_predictions_sp <- test_predictions %>%
      mutate(.pred_Sale_Price = .pred_original_scale * Total_Building_SqFt)

    xgb_mae_sp <- yardstick::mae(test_predictions_sp, truth = Sale_Price, estimate = .pred_Sale_Price)
    xgb_rmse_sp <- yardstick::rmse(test_predictions_sp, truth = Sale_Price, estimate = .pred_Sale_Price)
    xgb_rsq_sp <- yardstick::rsq(test_predictions_sp, truth = Sale_Price, estimate = .pred_Sale_Price)

    cat("\n--- Tuned XGBoost: Performance on Test Set (Original Sale_Price Scale) ---\n")
    cat("MAE (Sale_Price):", xgb_mae_sp$.estimate, "\n")
    cat("RMSE (Sale_Price):", xgb_rmse_sp$.estimate, "\n")
    cat("R-squared (Sale_Price):", xgb_rsq_sp$.estimate, "\n")
  } else {
    cat("Skipping test set evaluation as predictions failed.\n")
  }
} else {
  cat("Skipping test set evaluation as final model was not fitted.\n")
}


# --- 17. Feature Importance (from the final model) ---
if (!is.null(final_xgb_fit)) {
  cat("\nCalculating feature importance using vip package...\n")
  importance_plot <- tryCatch({
    final_xgb_fit %>%
      extract_fit_parsnip() %>%
      vip::vip(geom = "col", num_features = 20, aesthetics = list(fill = "steelblue")) +
      theme_minimal(base_size = 12) +
      labs(title = "Top 20 Most Important Features (Tuned XGBoost)",
           x = "Predictor",
           y = "Importance") +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
  }, error = function(e) {
    cat("Error calculating feature importance:\n")
    print(e)
    return(NULL)
  })
  
  if(!is.null(importance_plot)) print(importance_plot)

} else {
  cat("Skipping feature importance as final model was not fitted.\n")
}

cat("\n--- Script Finished ---\n")
```


```{r}
# --- 16. Evaluate on Test Set ---
cat("Evaluating final XGBoost model on the test set...\n")
if (!is.null(final_xgb_fit)) {
  test_predictions <- tryCatch({
    predict(final_xgb_fit, new_data = test_data) %>%
      bind_cols(test_data %>% select(log_Price_Per_SqFt, Price_Per_SqFt, Sale_Price, Total_Building_SqFt, ETN, Parcel_Number, Sale_Date)) %>% # Added ETN, Parcel_Number, Sale_Date for easier joins later
      mutate(
        .pred_original_scale = exp(.pred), # Undo log transformation for Price_Per_SqFt
        .pred_Sale_Price = .pred_original_scale * Total_Building_SqFt # CALCULATE .pred_Sale_Price HERE
      )
  }, error = function(e) {
    cat("Error during prediction on test set or subsequent piping:\n")
    print(e)
    return(NULL) # Return NULL if prediction fails
  })

  if (!is.null(test_predictions)) {
    # Calculate metrics on the original Price_Per_SqFt scale
    xgb_mae_ppsft <- yardstick::mae(test_predictions, truth = Price_Per_SqFt, estimate = .pred_original_scale)
    xgb_rmse_ppsft <- yardstick::rmse(test_predictions, truth = Price_Per_SqFt, estimate = .pred_original_scale)
    xgb_rsq_ppsft <- yardstick::rsq(test_predictions, truth = Price_Per_SqFt, estimate = .pred_original_scale)

    cat("\n--- Tuned XGBoost: Performance on Test Set (Price_Per_SqFt Scale) ---\n")
    cat("MAE (Price_Per_SqFt):", xgb_mae_ppsft$.estimate, "\n")
    cat("RMSE (Price_Per_SqFt):", xgb_rmse_ppsft$.estimate, "\n")
    cat("R-squared (Price_Per_SqFt):", xgb_rsq_ppsft$.estimate, "\n")

    # Calculate metrics on the original Sale_Price scale (using the .pred_Sale_Price column we just created)
    xgb_mae_sp <- yardstick::mae(test_predictions, truth = Sale_Price, estimate = .pred_Sale_Price)
    xgb_rmse_sp <- yardstick::rmse(test_predictions, truth = Sale_Price, estimate = .pred_Sale_Price)
    xgb_rsq_sp <- yardstick::rsq(test_predictions, truth = Sale_Price, estimate = .pred_Sale_Price)

    cat("\n--- Tuned XGBoost: Performance on Test Set (Original Sale_Price Scale) ---\n")
    cat("MAE (Sale_Price):", xgb_mae_sp$.estimate, "\n")
    cat("RMSE (Sale_Price):", xgb_rmse_sp$.estimate, "\n")
    cat("R-squared (Sale_Price):", xgb_rsq_sp$.estimate, "\n")
  } else {
    cat("Skipping test set evaluation as predictions failed.\n")
  }
} else {
  cat("Skipping test set evaluation as final model was not fitted.\n")
}
```



```{r}
# --- 16.5 Error Analysis (New Section) ---
cat("\n--- Starting Error Analysis ---\n")

if (!is.null(final_xgb_fit) && exists("test_predictions") && !is.null(test_predictions)) {
  
  # 1. Calculate Prediction Errors
  # We already have .pred (log_Price_Per_SqFt prediction) and log_Price_Per_SqFt (actual)
  # We also have .pred_original_scale (Price_Per_SqFt prediction) and Price_Per_SqFt (actual)
  # And .pred_Sale_Price and Sale_Price
  
  error_analysis_data <- test_predictions %>%
    mutate(
      # Error on the log_Price_Per_SqFt scale (model's direct target)
      error_log_ppsft = .pred - log_Price_Per_SqFt,
      abs_error_log_ppsft = abs(error_log_ppsft),
      
      # Error on the original Price_Per_SqFt scale
      error_ppsft = .pred_original_scale - Price_Per_SqFt,
      abs_error_ppsft = abs(error_ppsft),
      
      # Error on the original Sale_Price scale
      error_sale_price = .pred_Sale_Price - Sale_Price,
      abs_error_sale_price = abs(error_sale_price),
      
      # Percentage error for Sale_Price (can be sensitive to very low actual prices)
      # Add a small epsilon to Sale_Price to avoid division by zero if any Sale_Price is 0 (though filtered)
      percentage_error_sale_price = (error_sale_price / (Sale_Price + 1e-6)) * 100,
      abs_percentage_error_sale_price = abs(percentage_error_sale_price)
    )

  cat("Top 10 worst predictions by Absolute Error on Sale_Price:\n")
  worst_by_abs_sale_price_error <- error_analysis_data %>%
    arrange(desc(abs_error_sale_price)) %>%
    head(10)
  print(worst_by_abs_sale_price_error %>% 
          select(Parcel_Number, Sale_Price, .pred_Sale_Price, error_sale_price, abs_error_sale_price, 
                 Price_Per_SqFt, .pred_original_scale, error_ppsft, Total_Building_SqFt, Sale_Date)) # Add other relevant features

  cat("\nTop 10 worst predictions by Absolute Percentage Error on Sale_Price (if Sale_Price > 0):\n")
  # Filter out cases where Sale_Price might be extremely small to avoid huge percentages from tiny dollar errors
  worst_by_abs_percent_sale_price_error <- error_analysis_data %>%
    filter(Sale_Price > 1000) %>% # Or some other reasonable threshold
    arrange(desc(abs_percentage_error_sale_price)) %>%
    head(10)
  print(worst_by_abs_percent_sale_price_error %>%
          select(Parcel_Number, Sale_Price, .pred_Sale_Price, error_sale_price, abs_percentage_error_sale_price,
                 Price_Per_SqFt, .pred_original_scale, Total_Building_SqFt, Sale_Date))

  cat("\nTop 10 worst predictions by Absolute Error on Price_Per_SqFt:\n")
  worst_by_abs_ppsft_error <- error_analysis_data %>%
    arrange(desc(abs_error_ppsft)) %>%
    head(10)
  print(worst_by_abs_ppsft_error %>%
          select(Parcel_Number, Price_Per_SqFt, .pred_original_scale, error_ppsft, abs_error_ppsft,
                 Sale_Price, .pred_Sale_Price, Total_Building_SqFt, Sale_Date))

  # 2. Visualize Error Distribution
  error_plot_ppsft <- ggplot(error_analysis_data, aes(x = error_ppsft)) +
    geom_histogram(bins = 50, fill = "skyblue", color = "black") +
    labs(title = "Distribution of Prediction Errors (Price_Per_SqFt)",
         x = "Error (Predicted - Actual Price_Per_SqFt)",
         y = "Frequency") +
    theme_minimal()
  print(error_plot_ppsft)
  # ggsave("error_distribution_ppsft.png", plot = error_plot_ppsft)

  error_plot_sale_price <- ggplot(error_analysis_data, aes(x = error_sale_price)) +
    geom_histogram(bins = 50, fill = "lightcoral", color = "black") +
    scale_x_continuous(labels = scales::comma) + # Format x-axis for better readability
    labs(title = "Distribution of Prediction Errors (Sale_Price)",
         x = "Error (Predicted - Actual Sale_Price)",
         y = "Frequency") +
    theme_minimal()
  print(error_plot_sale_price)
  # ggsave("error_distribution_sale_price.png", plot = error_plot_sale_price)

  # 3. Scatter plot of Actual vs. Predicted
  actual_vs_pred_ppsft <- ggplot(error_analysis_data, aes(x = Price_Per_SqFt, y = .pred_original_scale)) +
    geom_point(alpha = 0.3) +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    labs(title = "Actual vs. Predicted Price_Per_SqFt",
         x = "Actual Price_Per_SqFt",
         y = "Predicted Price_Per_SqFt") +
    coord_cartesian(xlim = range(error_analysis_data$Price_Per_SqFt, na.rm = TRUE), # Ensure ranges are sensible
                    ylim = range(error_analysis_data$.pred_original_scale, na.rm = TRUE)) +
    theme_minimal()
  print(actual_vs_pred_ppsft)
  # ggsave("actual_vs_predicted_ppsft.png", plot = actual_vs_pred_ppsft)

  actual_vs_pred_sale_price <- ggplot(error_analysis_data, aes(x = Sale_Price, y = .pred_Sale_Price)) +
    geom_point(alpha = 0.3) +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    scale_x_continuous(labels = scales::comma) +
    scale_y_continuous(labels = scales::comma) +
    labs(title = "Actual vs. Predicted Sale_Price",
         x = "Actual Sale_Price",
         y = "Predicted Sale_Price") +
    coord_cartesian(xlim = range(error_analysis_data$Sale_Price, na.rm = TRUE),
                    ylim = range(error_analysis_data$.pred_Sale_Price, na.rm = TRUE)) +
    theme_minimal()
  print(actual_vs_pred_sale_price)
  # ggsave("actual_vs_predicted_sale_price.png", plot = actual_vs_pred_sale_price)
  
  # 4. Examine characteristics of worst predictions
  # For the 'worst_by_abs_sale_price_error' or 'worst_by_abs_ppsft_error' dataframes,
  # you would typically pull in more original features from 'test_data' to inspect them.
  # This requires joining 'worst_by_abs_sale_price_error' back to 'test_data' using a unique ID like Parcel_Number and Sale_Date
  # if Parcel_Number alone isn't unique for sales.
  
  # Assuming Parcel_Number and Sale_Date make a unique key for sales in test_data
  # (If ETN is the unique sale ID and is in test_predictions, use that)
  
  # First, ensure the columns used for joining are present in both
  # print(colnames(worst_by_abs_sale_price_error))
  # print(colnames(test_data))

  # If 'ETN' is your unique transaction identifier and is carried through to 'test_predictions':
  if ("ETN" %in% colnames(test_predictions) && "ETN" %in% colnames(test_data)) {
    cat("\nInspecting full data for top 10 worst Sale_Price predictions (by ETN)...\n")
    worst_preds_full_details_etn <- worst_by_abs_sale_price_error %>%
      select(ETN, Sale_Price, .pred_Sale_Price, abs_error_sale_price) %>%
      left_join(test_data, by = "ETN", suffix = c("", ".original_test_data"))
    print(glimpse(worst_preds_full_details_etn)) # Show all columns for these worst predictions
  } else if ("Parcel_Number" %in% colnames(test_predictions) && "Sale_Date" %in% colnames(test_predictions) &&
             "Parcel_Number" %in% colnames(test_data) && "Sale_Date" %in% colnames(test_data)) {
    cat("\nInspecting full data for top 10 worst Sale_Price predictions (by Parcel_Number, Sale_Date)...\n")
    # Ensure Sale_Date is in the same format if joining on it. It should be Date object in both.
    worst_preds_full_details <- worst_by_abs_sale_price_error %>%
      select(Parcel_Number, Sale_Date, Sale_Price, .pred_Sale_Price, abs_error_sale_price) %>%
      left_join(test_data, by = c("Parcel_Number", "Sale_Date"), suffix = c("", ".original_test_data"))
    print(glimpse(worst_preds_full_details)) # Show all columns for these worst predictions
  } else {
    cat("\nCould not join worst predictions back to full test_data for detailed inspection due to missing key columns (ETN or Parcel_Number+Sale_Date).\n")
    cat("Displaying selected columns from error_analysis_data for worst predictions:\n")
    print(worst_by_abs_sale_price_error %>% select(Parcel_Number, ETN, Sale_Date, Sale_Price, .pred_Sale_Price, abs_error_sale_price, Total_Building_SqFt, Property_Type, Neighborhood_Summary, everything()))
  }

} else {
  cat("Skipping error analysis as final model or test predictions are not available.\n")
}

cat("\n--- Error Analysis Finished ---\n")
```


```{r}
# --- 0. Load Libraries ---
# Make sure all necessary libraries are loaded.
# It's good practice to load them at the beginning of your script.
library(tidyverse)    # For dplyr, ggplot2, etc. (forcats is part of tidyverse)
library(tidymodels)   # For the core modeling workflow (recipes, parsnip, tune, yardstick, rsample)
library(arrow)        # For reading parquet files
library(doParallel)   # For parallel processing
library(parallel)     # For detectCores
library(foreach)      # For registerDoSEQ() if called explicitly
library(vip)          # For variable importance plots

# --- 1. Load Data (Assuming full_data is loaded from parquet before this cell) ---
# If not, add:
# cat("Loading full_data.parquet...\n")
# full_data <- arrow::read_parquet("full_data_for_modeling.parquet") # Or whatever your filename is
# cat("Data loaded. Dimensions:", dim(full_data), "\n")

# --- 2. Initial Data Cleaning & Feature Engineering ---
cat("Starting initial data cleaning and feature engineering...\n")
data_cleaned <- full_data %>%
  mutate(
    # Convert all character columns to factors.
    across(where(is.character), as.factor),
    
    # Ensure all relevant date columns are explicitly converted to Date type.
    Sale_Date = as.Date(Sale_Date),
    Latest_Appraisal_Date = as.Date(Latest_Appraisal_Date),
    Latest_Merge_Event_Date = as.Date(Latest_Merge_Event_Date)
  )
cat("Initial data type conversions done.\n")


# --- 3. Filtering and Target Variable Creation ---
cat("Filtering data and creating target variable...\n")

# --- START MODIFICATION FOR SECTION 3 ---
original_row_count_cleaned <- nrow(data_cleaned)
cat("Original row count in data_cleaned:", original_row_count_cleaned, "\n")

data_filtered <- data_cleaned %>%
  # Keep your existing sensible filters
  filter(Total_Building_SqFt > 1) %>%
  filter(Sale_Price > 1000) %>% # Removed upper bound for now, will add specific ones below

  # ADD NEW FILTERS BASED ON ERROR ANALYSIS:
  filter(
    Property_Type %in% c("Residential"), # Or your primary target type(s)
    Sale_Price < 6000000,                # Adjust threshold as needed (e.g., 5M, 7M)
    Total_Building_SqFt < 75000,         # Adjust threshold as needed
    Parcel_Count == 1                    # Consider this to simplify for now
  ) %>%
  # Now apply Price_Per_SqFt filters AFTER the main entity filters
  mutate(Price_Per_SqFt = Sale_Price / Total_Building_SqFt) %>%
  filter(Price_Per_SqFt > 1, Price_Per_SqFt < 5000) %>% # Keep your existing PPSF filters
  mutate(log_Price_Per_SqFt = log(Price_Per_SqFt))

cat("Row count after applying new filters:", nrow(data_filtered), "\n")
if(nrow(data_filtered) < 0.05 * original_row_count_cleaned && original_row_count_cleaned > 0) { # Warning if too much data is filtered
    warning(paste("More than 95% of data filtered out (from", original_row_count_cleaned, "to", nrow(data_filtered), "). Check filter conditions."))
}
if (nrow(data_filtered) == 0) {
  stop("Error: No data remaining after filtering. Check filter conditions.")
}
# --- END MODIFICATION FOR SECTION 3 ---

cat("Data dimensions after filtering:", dim(data_filtered), "\n")


# --- 3.5 (NEW - Optional but Recommended) Check Numeric Predictor Correlations ---
cat("\n--- Checking Numeric Predictor Correlations on data_filtered ---\n")
numeric_cols_for_corr <- data_filtered %>%
  select(where(is.numeric)) %>%
  # Exclude target, its direct transformations, and identifiers that might be numeric by mistake
  select(-any_of(c("log_Price_Per_SqFt", "Price_Per_SqFt", "Sale_Price", 
                   "Parcel_Count", # Already used in filtering, might not be a predictor
                   "ETN", "Parcel_Number", # These should be factors or IDs
                   "Tax_Summary_Tax_Year" # Often treated as categorical or year
                   ))) 

# Remove columns with zero variance or all NAs after filtering
valid_cols <- sapply(numeric_cols_for_corr, function(x) {
  var_x <- var(x, na.rm = TRUE)
  !all(is.na(x)) && !is.na(var_x) && var_x > 0
})
numeric_cols_for_corr <- numeric_cols_for_corr[, valid_cols, drop = FALSE]

if (ncol(numeric_cols_for_corr) > 1) {
  cor_matrix <- cor(numeric_cols_for_corr, use = "pairwise.complete.obs")
  
  # library(tidyr) # Already loaded via tidyverse
  # library(dplyr) # Already loaded via tidyverse
  highly_correlated_pairs <- as.data.frame(cor_matrix) %>%
    tibble::rownames_to_column("var1") %>%
    pivot_longer(-var1, names_to = "var2", values_to = "correlation") %>%
    filter(var1 < var2, abs(correlation) > 0.90) %>% # Threshold e.g. 0.90
    arrange(desc(abs(correlation)))
  
  cat("\nHighly correlated numeric predictor pairs (threshold > 0.90) in data_filtered:\n")
  if(nrow(highly_correlated_pairs) > 0) {
    print(highly_correlated_pairs)
  } else {
    cat("No pairs found above the threshold.\n")
  }
} else {
  cat("\nNot enough valid numeric columns in data_filtered to calculate correlation matrix.\n")
}
# --- END NEW SECTION 3.5 ---


# --- 4. Data Splitting ---
# (Your existing code - ensure it uses data_filtered)
cat("Splitting data into training and testing sets...\n")
set.seed(123) # for reproducibility
data_split <- initial_split(data_filtered, prop = 0.80, strata = log_Price_Per_SqFt) # Uses data_filtered
train_data <- training(data_split)
test_data  <- testing(data_split)
cat("Training data dimensions:", dim(train_data), "\n")
cat("Testing data dimensions:", dim(test_data), "\n")


# --- 5. Cross-Validation Folds ---
# (Your existing code - uses train_data which comes from data_filtered)
cat("Creating cross-validation folds...\n")
set.seed(234) # for reproducibility
cv_folds <- vfold_cv(train_data, v = 5, strata = log_Price_Per_SqFt)
cat("Created", cv_folds$splits %>% length(), "CV folds.\n")


# --- 6. Preprocessing Recipe ---
# (Your existing code - uses train_data)
cat("Defining preprocessing recipe...\n")
xgb_recipe <- recipe(log_Price_Per_SqFt ~ ., data = train_data) %>%
  update_role(
    Parcel_Number, ETN, Sale_Date, Price_Per_SqFt,
    Latest_Appraisal_Date, Latest_Merge_Event_Date,
    Sale_Price, # <<<<<<< CRITICAL FIX: Assign Sale_Price an ID role
    new_role = "ID"
  ) %>%
  step_mutate( # Specific imputations for key numeric features
    Total_Bedrooms = if_else(is.na(Total_Bedrooms), 0, Total_Bedrooms),
    Total_Bathrooms = if_else(is.na(Total_Bathrooms), 0, Total_Bathrooms),
    Total_Stories = if_else(is.na(Total_Stories), 1, Total_Stories),
    Num_Buildings_Good_Condition = if_else(is.na(Num_Buildings_Good_Condition), 0, Num_Buildings_Good_Condition)
  ) %>%
  step_string2factor(all_nominal_predictors(), -all_outcomes(), -has_role("ID")) %>%
  step_mutate_at(all_nominal_predictors(), fn = ~forcats::fct_explicit_na(., na_level = "Missing_Level")) %>%
  step_novel(all_nominal_predictors(), -all_outcomes(), -has_role("ID"), new_level = "Novel_Category_Level") %>%
  step_other(all_nominal_predictors(), -all_outcomes(), -has_role("ID"), threshold = 0.01, other = "Other_Category_Level") %>%
  step_dummy(all_nominal_predictors(), -all_outcomes(), -has_role("ID"), one_hot = FALSE) %>%
  step_nzv(all_predictors(), -all_outcomes(), -has_role("ID")) %>%
  step_impute_median(all_numeric_predictors(), -all_outcomes(), -has_role("ID"))

cat("Recipe defined.\n")


# --- 7. XGBoost Model Specification for Tuning ---
# (Your existing code)
cat("Defining XGBoost model specification for tuning...\n")
xgb_spec_tune <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  min_n = tune(),
  learn_rate = tune(),
  mtry = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
cat("XGBoost model specification for tuning defined.\n")


# --- 8. Workflow ---
# (Your existing code)
cat("Creating XGBoost tuning workflow...\n")
xgb_workflow_tune <- workflow() %>%
  add_recipe(xgb_recipe) %>%
  add_model(xgb_spec_tune)
cat("XGBoost tuning workflow defined.\n")


# --- 9. Hyperparameter Grid ---
# (Your existing code)
cat("Creating hyperparameter grid...\n")
set.seed(345)
temp_prepped_recipe <- prep(xgb_recipe, training = train_data)
num_predictors_after_recipe <- ncol(bake(temp_prepped_recipe, new_data = NULL)) - 1
cat("Number of predictors after recipe prepping:", num_predictors_after_recipe, "\n")
xgb_param_set <- extract_parameter_set_dials(xgb_spec_tune)
final_xgb_param_set <- xgb_param_set %>%
  update(mtry = mtry(c(1L, max(1L, num_predictors_after_recipe))))
xgb_grid <- grid_latin_hypercube(
  final_xgb_param_set,
  size = 20 # Consider reducing size for faster iteration if needed, e.g., 10 or 15
)
cat("Hyperparameter grid created with", nrow(xgb_grid), "combinations.\n")


# --- 10. Parallel Processing Setup ---
# (Your existing code)
cat("Setting up parallel processing...\n")
num_cores_to_use <- 14 # Adjust as needed
if (num_cores_to_use > parallel::detectCores(logical = TRUE)) { # Use logical=TRUE for hyperthreading
  num_cores_to_use <- parallel::detectCores(logical = TRUE) - 1
}
if (num_cores_to_use < 1) num_cores_to_use <- 1
# Ensure any existing cluster is stopped before creating a new one
if (exists("cl") && inherits(cl, "cluster")) {
  try(parallel::stopCluster(cl), silent = TRUE)
}
cl <- parallel::makePSOCKcluster(num_cores_to_use)
doParallel::registerDoParallel(cl)
cat("Parallel processing setup with", num_cores_to_use, "cores.\n")
gc()
cat("Garbage collection run before tuning.\n")


# --- 11. Tune the Model ---
# (Your existing code)
cat("Starting XGBoost tuning... This may take a while.\n")
set.seed(456)
start_time_tuning <- Sys.time()
xgb_tune_results <- tune::tune_grid(
  xgb_workflow_tune,
  resamples = cv_folds,
  grid = xgb_grid,
  metrics = yardstick::metric_set(rmse, mae, rsq),
  control = tune::control_grid(
    save_pred = TRUE,
    verbose = TRUE,
    allow_par = TRUE,
    parallel_over = "everything" # Ensure this is compatible with your setup
  )
)
end_time_tuning <- Sys.time()
tuning_duration <- end_time_tuning - start_time_tuning
cat("Tuning finished.\n")
cat("Tuning duration:", format(tuning_duration), "\n")
if (exists("cl") && inherits(cl, "cluster")) {
  try(parallel::stopCluster(cl), silent = TRUE)
  cat("Parallel cluster stopped.\n")
}
try(registerDoSEQ(), silent = TRUE)
cat("Parallel backend unregistered (attempted). Switched to sequential processing.\n")
if (nrow(tune::collect_metrics(xgb_tune_results)) == 0) {
    cat("ERROR: All models failed during tuning or no metrics were collected. Showing notes:\n")
    try(tune::show_notes(xgb_tune_results), silent = TRUE) # Add try for safety
} else {
    cat("Tuning results collected.\n")
}


# --- 12. Analyze Tuning Results & Select Best ---
# (Your existing code)
cat("Tuning results summary:\n")
if (nrow(tune::collect_metrics(xgb_tune_results)) > 0) {
  print(tune::collect_metrics(xgb_tune_results))
  cat("\nBest models based on RMSE:\n")
  tune::show_best(xgb_tune_results, "rmse", n = 5)
  best_xgb_params <- tune::select_best(xgb_tune_results, "rmse")
  cat("\nSelected best hyperparameters for XGBoost (based on RMSE):\n")
  print(best_xgb_params)
} else {
  cat("Skipping analysis of tuning results as no metrics were collected.\n")
  best_xgb_params <- NULL
}


# --- 13. Finalize Workflow with Best Hyperparameters ---
# (Your existing code)
if (!is.null(best_xgb_params) && nrow(best_xgb_params) > 0) {
  cat("Finalizing workflow with best parameters...\n")
  final_xgb_workflow <- finalize_workflow(
    xgb_workflow_tune,
    best_xgb_params
  )
  cat("Workflow finalized.\n")
} else {
  cat("Skipping workflow finalization as best parameters were not found.\n")
  final_xgb_workflow <- NULL
}


# --- 14. Fit Final Model to Entire Training Data ---
# (Your existing code)
if (!is.null(final_xgb_workflow)) {
  cat("Fitting final XGBoost model to the entire training set...\n")
  start_time_final_fit <- Sys.time()
  final_xgb_fit <- fit(final_xgb_workflow, data = train_data)
  end_time_final_fit <- Sys.time()
  final_fit_duration <- end_time_final_fit - start_time_final_fit
  cat("Final XGBoost model fitted. Duration:", format(final_fit_duration), "\n")
} else {
  cat("Skipping final model fitting as workflow was not finalized.\n")
  final_xgb_fit <- NULL
}


# --- 15. Save the Final Fitted Model ---
# (Your existing code)
if (!is.null(final_xgb_fit)) {
  cat("Saving the final fitted model workflow...\n")
  saveRDS(final_xgb_fit, file = "final_xgb_model_fit_filtered.rds") # Changed filename
  cat("Final model saved to final_xgb_model_fit_filtered.rds\n")
} else {
  cat("Skipping model saving as final model was not fitted.\n")
}


# --- 16. Evaluate on Test Set ---
# --- START MODIFICATION FOR SECTION 16 (Use the corrected version) ---
cat("Evaluating final XGBoost model on the test set...\n")
if (!is.null(final_xgb_fit)) {
  test_predictions <- tryCatch({
    predict(final_xgb_fit, new_data = test_data) %>%
      bind_cols(test_data %>% select(log_Price_Per_SqFt, Price_Per_SqFt, Sale_Price, Total_Building_SqFt, ETN, Parcel_Number, Sale_Date)) %>% # Added ETN, Parcel_Number, Sale_Date
      mutate(
        .pred_original_scale = exp(.pred), 
        .pred_Sale_Price = .pred_original_scale * Total_Building_SqFt # Calculate .pred_Sale_Price
      )
  }, error = function(e) {
    cat("Error during prediction on test set or subsequent piping:\n")
    print(e)
    return(NULL) 
  })

  if (!is.null(test_predictions)) {
    xgb_mae_ppsft <- yardstick::mae(test_predictions, truth = Price_Per_SqFt, estimate = .pred_original_scale)
    xgb_rmse_ppsft <- yardstick::rmse(test_predictions, truth = Price_Per_SqFt, estimate = .pred_original_scale)
    xgb_rsq_ppsft <- yardstick::rsq(test_predictions, truth = Price_Per_SqFt, estimate = .pred_original_scale)

    cat("\n--- Tuned XGBoost: Performance on Test Set (Price_Per_SqFt Scale) ---\n")
    cat("MAE (Price_Per_SqFt):", xgb_mae_ppsft$.estimate, "\n")
    cat("RMSE (Price_Per_SqFt):", xgb_rmse_ppsft$.estimate, "\n")
    cat("R-squared (Price_Per_SqFt):", xgb_rsq_ppsft$.estimate, "\n")

    xgb_mae_sp <- yardstick::mae(test_predictions, truth = Sale_Price, estimate = .pred_Sale_Price)
    xgb_rmse_sp <- yardstick::rmse(test_predictions, truth = Sale_Price, estimate = .pred_Sale_Price)
    xgb_rsq_sp <- yardstick::rsq(test_predictions, truth = Sale_Price, estimate = .pred_Sale_Price)

    cat("\n--- Tuned XGBoost: Performance on Test Set (Original Sale_Price Scale) ---\n")
    cat("MAE (Sale_Price):", xgb_mae_sp$.estimate, "\n")
    cat("RMSE (Sale_Price):", xgb_rmse_sp$.estimate, "\n")
    cat("R-squared (Sale_Price):", xgb_rsq_sp$.estimate, "\n")
  } else {
    cat("Skipping test set evaluation as predictions failed.\n")
  }
} else {
  cat("Skipping test set evaluation as final model was not fitted.\n")
}
# --- END MODIFICATION FOR SECTION 16 ---


# --- 16.5 (NEW) Error Analysis ---
cat("\n--- Starting Error Analysis ---\n")
if (!is.null(final_xgb_fit) && exists("test_predictions") && !is.null(test_predictions) && nrow(test_predictions) > 0) {
  
  error_analysis_data <- test_predictions %>%
    mutate(
      error_log_ppsft = .pred - log_Price_Per_SqFt,
      abs_error_log_ppsft = abs(error_log_ppsft),
      error_ppsft = .pred_original_scale - Price_Per_SqFt,
      abs_error_ppsft = abs(error_ppsft),
      error_sale_price = .pred_Sale_Price - Sale_Price,
      abs_error_sale_price = abs(error_sale_price),
      percentage_error_sale_price = ifelse(Sale_Price == 0, NA, (error_sale_price / Sale_Price) * 100), # Avoid division by zero
      abs_percentage_error_sale_price = abs(percentage_error_sale_price)
    )

  cat("Top 10 worst predictions by Absolute Error on Sale_Price:\n")
  worst_by_abs_sale_price_error <- error_analysis_data %>%
    arrange(desc(abs_error_sale_price)) %>%
    head(10)
  print(worst_by_abs_sale_price_error %>% 
          select(any_of(c("Parcel_Number", "ETN", "Sale_Date", "Sale_Price", ".pred_Sale_Price", "error_sale_price", "abs_error_sale_price", 
                 "Price_Per_SqFt", ".pred_original_scale", "error_ppsft", "Total_Building_SqFt"))))

  cat("\nTop 10 worst predictions by Absolute Percentage Error on Sale_Price (if Sale_Price > 1000):\n")
  worst_by_abs_percent_sale_price_error <- error_analysis_data %>%
    filter(Sale_Price > 1000, !is.na(abs_percentage_error_sale_price)) %>% 
    arrange(desc(abs_percentage_error_sale_price)) %>%
    head(10)
  print(worst_by_abs_percent_sale_price_error %>%
          select(any_of(c("Parcel_Number", "ETN", "Sale_Date", "Sale_Price", ".pred_Sale_Price", "error_sale_price", "abs_percentage_error_sale_price",
                 "Price_Per_SqFt", ".pred_original_scale", "Total_Building_SqFt"))))
  
  # Visualizations (Error Distribution, Actual vs. Predicted)
  if (requireNamespace("ggplot2", quietly = TRUE) && requireNamespace("scales", quietly = TRUE)) {
      error_plot_sale_price <- ggplot(error_analysis_data, aes(x = error_sale_price)) +
        geom_histogram(bins = 50, fill = "lightcoral", color = "black") +
        scale_x_continuous(labels = scales::comma) + 
        labs(title = "Distribution of Prediction Errors (Sale_Price)", x = "Error (Predicted - Actual Sale_Price)", y = "Frequency") +
        theme_minimal()
      print(error_plot_sale_price)

      actual_vs_pred_sale_price <- ggplot(error_analysis_data, aes(x = Sale_Price, y = .pred_Sale_Price)) +
        geom_point(alpha = 0.3) + geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
        scale_x_continuous(labels = scales::comma) + scale_y_continuous(labels = scales::comma) +
        labs(title = "Actual vs. Predicted Sale_Price", x = "Actual Sale_Price", y = "Predicted Sale_Price") +
        theme_minimal()
      print(actual_vs_pred_sale_price)
  } else {
      cat("ggplot2 or scales package not available for plotting error distributions.\n")
  }

  # Detailed inspection of worst predictions
  cat("\nInspecting full data for top 10 worst Sale_Price predictions (by ETN if available)...\n")
  # Ensure test_data has the necessary columns for join and selection
  cols_for_join_inspection <- intersect(names(test_data), 
                                        c("ETN", "Parcel_Number", "Sale_Date", "Property_Type", "Neighborhood_Summary", 
                                          "Total_Bedrooms", "Total_Bathrooms", "Total_Stories", "Main_Building_Quality")) # Add more as needed
  
  if ("ETN" %in% colnames(worst_by_abs_sale_price_error) && "ETN" %in% colnames(test_data)) {
    worst_preds_full_details <- worst_by_abs_sale_price_error %>%
      select(ETN, Sale_Price, .pred_Sale_Price, abs_error_sale_price) %>%
      left_join(test_data %>% select(ETN, all_of(cols_for_join_inspection[!cols_for_join_inspection == "ETN"])), by = "ETN")
    print(glimpse(worst_preds_full_details))
  } else {
    cat("Could not join by ETN for detailed inspection. Displaying selected columns from error_analysis_data:\n")
    print(worst_by_abs_sale_price_error %>% select(any_of(c("Parcel_Number", "ETN", "Sale_Date", "Sale_Price", ".pred_Sale_Price", "abs_error_sale_price", "Total_Building_SqFt", "Property_Type", "Neighborhood_Summary"))))
  }

} else {
  cat("Skipping error analysis as final model or test predictions are not available or empty.\n")
}
cat("\n--- Error Analysis Finished ---\n")
# --- END NEW SECTION 16.5 ---


# --- 17. Feature Importance (from the final model) ---
# (Your existing code)
if (!is.null(final_xgb_fit)) {
  cat("\nCalculating feature importance using vip package...\n")
  importance_plot <- tryCatch({
    final_xgb_fit %>%
      extract_fit_parsnip() %>%
      vip::vip(geom = "col", num_features = 20, aesthetics = list(fill = "steelblue")) + # Ensure num_features is not too high if few predictors
      theme_minimal(base_size = 12) +
      labs(title = "Top 20 Most Important Features (Tuned XGBoost on Filtered Data)",
           x = "Predictor",
           y = "Importance") +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
  }, error = function(e) {
    cat("Error calculating feature importance:\n")
    print(e)
    return(NULL)
  })
  if(!is.null(importance_plot)) print(importance_plot)
} else {
  cat("Skipping feature importance as final model was not fitted.\n")
}

cat("\n--- Script Finished ---\n")
```


```{r}
# --- SAVE KEY OBJECTS AFTER FULL RUN ---
cat("Saving key R objects to disk...\n")

# Create a directory to store these objects if it doesn't exist
output_dir <- "r_objects_after_full_run" 
if (!dir.exists(output_dir)) {
  dir.create(output_dir)
  cat("Created directory:", output_dir, "\n")
}

# 1. The final fitted model workflow
if (exists("final_xgb_fit") && !is.null(final_xgb_fit)) {
  saveRDS(final_xgb_fit, file = file.path(output_dir, "final_xgb_model_fit_filtered.rds"))
  cat("Saved: final_xgb_fit\n")
} else {
  cat("Warning: final_xgb_fit not found or is NULL. Not saved.\n")
}

# 2. The tuning results
if (exists("xgb_tune_results") && !is.null(xgb_tune_results)) {
  saveRDS(xgb_tune_results, file = file.path(output_dir, "xgb_tune_results_filtered.rds"))
  cat("Saved: xgb_tune_results\n")
} else {
  cat("Warning: xgb_tune_results not found or is NULL. Not saved.\n")
}

# 3. The best hyperparameters found
if (exists("best_xgb_params") && !is.null(best_xgb_params)) {
  saveRDS(best_xgb_params, file = file.path(output_dir, "best_xgb_params_filtered.rds"))
  cat("Saved: best_xgb_params\n")
} else {
  cat("Warning: best_xgb_params not found or is NULL. Not saved.\n")
}

# 4. The test set predictions and actuals (very useful for re-analysis)
if (exists("test_predictions") && !is.null(test_predictions) && nrow(test_predictions) > 0) {
  saveRDS(test_predictions, file = file.path(output_dir, "test_predictions_with_actuals_filtered.rds"))
  cat("Saved: test_predictions (with actuals)\n")
} else {
  cat("Warning: test_predictions not found, is NULL, or is empty. Not saved.\n")
}

# 5. The error analysis data (if you want to explore it further without re-calculating)
if (exists("error_analysis_data") && !is.null(error_analysis_data) && nrow(error_analysis_data) > 0) {
  saveRDS(error_analysis_data, file = file.path(output_dir, "error_analysis_data_filtered.rds"))
  cat("Saved: error_analysis_data\n")
} else {
  cat("Warning: error_analysis_data not found, is NULL, or is empty. Not saved.\n")
}

# 6. The data splits (train_data, test_data) - can be large, but useful for exact reproducibility
#    Alternatively, save the data_split object which is smaller.
if (exists("data_split") && !is.null(data_split)) {
  saveRDS(data_split, file = file.path(output_dir, "data_split_object_filtered.rds"))
  cat("Saved: data_split object\n")
} else {
  cat("Warning: data_split object not found or is NULL. Not saved.\n")
}
# If you prefer to save train_data and test_data separately:
# if (exists("train_data") && !is.null(train_data)) {
#   saveRDS(train_data, file = file.path(output_dir, "train_data_filtered.rds"))
#   cat("Saved: train_data\n")
# }
# if (exists("test_data") && !is.null(test_data)) {
#   saveRDS(test_data, file = file.path(output_dir, "test_data_filtered.rds"))
#   cat("Saved: test_data\n")
# }


# 7. The prepped recipe (can be useful for inspection, though often re-prepped)
if (exists("xgb_recipe") && !is.null(xgb_recipe)) { # Save the definition
    saveRDS(xgb_recipe, file = file.path(output_dir, "xgb_recipe_definition_filtered.rds"))
    cat("Saved: xgb_recipe definition\n")
}
if (exists("temp_prepped_recipe") && !is.null(temp_prepped_recipe)) { # Save the prepped version if you have it
    # Note: A prepped recipe contains data-dependent information.
    # It's often better to re-prep the recipe on the specific data (train/test) when needed.
    # However, saving it can be useful for quick inspection of transformations.
    saveRDS(temp_prepped_recipe, file = file.path(output_dir, "temp_prepped_recipe_filtered.rds"))
    cat("Saved: temp_prepped_recipe (from training data)\n")
}


# 8. (Optional) The filtered data itself if it's not too massive and its creation was complex
# if (exists("data_filtered") && !is.null(data_filtered)) {
#   saveRDS(data_filtered, file = file.path(output_dir, "data_filtered.rds"))
#   cat("Saved: data_filtered\n")
# }

cat("--- Finished saving objects ---\n")
cat("Objects saved in directory:", normalizePath(output_dir), "\n")
```

```{r}
# --- Temporary Inspection of 'Confirmed/Uncomfirmed' ---
cat("Inspecting 'Confirmed_Unconfirmed' column in data_filtered (before recipe):\n")
if ("Confirmed_Unconfirmed" %in% names(data_filtered)) { # Check if the column name is exactly this
  print(table(data_filtered$`Confirmed_Unconfirmed`, useNA = "ifany"))
  cat("\nLevels of 'Confirmed_Unconfirmed' after converting to factor:\n")
  print(levels(as.factor(data_filtered$`Confirmed_Unconfirmed`)))
} else {
  cat("'Confirmed_Unconfirmed' column not found with that exact name. Checking for similar names...\n")
  # You might need to list column names if there's a slight variation, e.g. due to cleaning
  # print(names(data_filtered)[grepl("confirm", names(data_filtered), ignore.case = TRUE)])
}
# --- End Temporary Inspection ---
```

```{r}
# --- Cell 1: Load Data from Parquet and Apply Initial Filters ---
# Ensure libraries are loaded
# library(DBI) # Not needed if not using DuckDB
# library(duckdb) # Not needed
library(arrow)     # For reading Parquet files
library(tidyverse)
library(tidymodels)
library(lubridate) # For date functions
library(vip)       # For variable importance

# Define the path to your Parquet file
parquet_file_path <- "../data/stage/full_data_for_modeling.parquet" # Adjust if your filename/path is different

# Load the data from Parquet
cat("Loading data from Parquet file:", parquet_file_path, "...\n")
if (!file.exists(parquet_file_path)) {
  stop("Parquet file not found at: ", parquet_file_path, 
       "\nPlease ensure the file path is correct or run the data engineering script to create it.")
}
data_cleaned <- read_parquet(parquet_file_path)
cat("Loaded", nrow(data_cleaned), "rows and", ncol(data_cleaned), "columns from Parquet.\n")

# Convert to tibble for easier handling (read_parquet often returns a tibble already)
data_cleaned <- as_tibble(data_cleaned)

# --- Initial Data Type Conversions (Review and adapt from your previous script) ---
cat("Starting initial data type conversions...\n")
data_cleaned <- data_cleaned %>%
  mutate(
    # Ensure Sale_Date is Date type. Parquet might preserve types, but good to check/convert.
    # If Sale_Date is already POSIXct or Date from Parquet, ymd() might error or be redundant.
    # Check class(data_cleaned$Sale_Date) first if unsure.
    Sale_Date = as_date(Sale_Date), # as_date is robust for POSIXct/Date/Character
    
    # Ensure Year_Built is numeric
    Year_Built = as.numeric(Year_Built),

    # Ensure other key columns have correct types if Parquet didn't preserve them perfectly
    # Example: Convert character columns that should be factors or numeric
    # Total_Building_SqFt = as.numeric(Total_Building_SqFt), # If it was char
    # Property_Type = as.factor(Property_Type), # If it was char

    # Handle potential NA Parcel_Number or ETN before converting to factor if they are IDs
    # If they are already character from Parquet, this is fine.
    Parcel_Number = as.character(Parcel_Number), 
    ETN = as.character(ETN)
  )
cat("Initial data type conversions done.\n")
# print(sapply(data_cleaned, class)) # Optional: check all column types

# --- Filtering data and creating target variable (from your script) ---
cat("Filtering data and creating target variable...\n")
original_rows <- nrow(data_cleaned)

data_filtered <- data_cleaned %>%
  filter(
    Sale_Price > 1000,
    Sale_Price < 6000000, 
    Total_Building_SqFt > 10,
    !is.na(Sale_Price),
    !is.na(Total_Building_SqFt),
    !is.na(Sale_Date), 
    !is.na(Year_Built)
  ) %>%
  mutate(
    Price_Per_SqFt = Sale_Price / Total_Building_SqFt,
    log_Price_Per_SqFt = log(Price_Per_SqFt)
  ) %>%
  filter(!is.na(log_Price_Per_SqFt) & !is.infinite(log_Price_Per_SqFt))

filtered_rows <- nrow(data_filtered)
cat("Original row count in data_cleaned:", original_rows, "\n")
cat("Row count after applying filters:", filtered_rows, "\n")
cat("Data dimensions after filtering:", paste(dim(data_filtered), collapse = " "), "\n")

cat("NAs in Sale_Date after filtering:", sum(is.na(data_filtered$Sale_Date)), "\n")
cat("NAs in Year_Built after filtering:", sum(is.na(data_filtered$Year_Built)), "\n")

# glimpse(data_filtered)
```

```{r}
# --- Cell 2: Feature Engineering ---
cat("Starting feature engineering...\n")

data_engineered <- data_filtered %>%
  mutate(
    Sale_Year = year(Sale_Date),
    Sale_Month = month(Sale_Date, label = TRUE, abbr = FALSE), # Or as.factor(month(Sale_Date))
    Age = Sale_Year - Year_Built,
    # Add other engineered features here if any, for example:
    # Years_Since_Remodel = ifelse(!is.na(Year_Remodeled) & Year_Remodeled > 0, Sale_Year - Year_Remodeled, NA),
    # Has_Remodel = ifelse(!is.na(Years_Since_Remodel) & Years_Since_Remodel >= 0, 1, 0)
  )
data_engineered <- data_engineered %>%
  filter(Age > 0 | is.na(Age)) # Keep only positive ages (is.na(Age) is just in case, though you have 0 NAs now)

# Then re-check:
cat("Number of properties with Age <= 0 after filtering:", sum(data_engineered$Age <= 0, na.rm = TRUE), "\n")
cat("Data dimensions after Age filtering:", paste(dim(data_engineered), collapse = " "), "\n")

# Sanity checks for new features
cat("Summary of Age:\n")
print(summary(data_engineered$Age))
cat("NAs in Age:", sum(is.na(data_engineered$Age)), "\n")
cat("NAs in Sale_Year:", sum(is.na(data_engineered$Sale_Year)), "\n")
cat("NAs in Sale_Month:", sum(is.na(data_engineered$Sale_Month)), "\n")

# Check for negative or zero ages if Year_Built could be after Sale_Year (data quality issue)
cat("Number of properties with Age <= 0:", sum(data_engineered$Age <= 0, na.rm = TRUE), "\n")
# If you have many, you might want to investigate or filter them:
# data_engineered <- data_engineered %>% filter(Age > 0 | is.na(Age))

cat("Feature engineering complete. Data dimensions:", paste(dim(data_engineered), collapse = " "), "\n")
# glimpse(data_engineered)
```

```{r}
# --- 0. Load Libraries ---
cat("Loading libraries...\n")
library(arrow)
library(tidyverse)
library(tidymodels)
library(lubridate)
library(vip)       # For variable importance plots
library(ranger)    # For the ranger engine for random forest

# --- 1. Load Data and Initial Cleaning/Filtering ---
cat("--- Cell 1: Load Data and Initial Filters ---\n")
parquet_file_path <- "../data/stage/full_data_for_modeling.parquet"

cat("Loading data from Parquet file:", parquet_file_path, "...\n")
if (!file.exists(parquet_file_path)) {
  stop("Parquet file not found: ", parquet_file_path)
}
data_cleaned <- read_parquet(parquet_file_path)
cat("Loaded", nrow(data_cleaned), "rows and", ncol(data_cleaned), "columns from Parquet.\n")

data_cleaned <- as_tibble(data_cleaned)

cat("Starting initial data type conversions...\n")
data_cleaned <- data_cleaned %>%
  mutate(
    Sale_Date = as_date(Sale_Date),
    Year_Built = as.numeric(Year_Built),
    Parcel_Number = as.character(Parcel_Number),
    ETN = as.character(ETN)
    # Add any other specific type conversions if needed based on your data
  )
cat("Initial data type conversions done.\n")

cat("Filtering data and creating target variable...\n")
original_rows <- nrow(data_cleaned)
data_filtered <- data_cleaned %>%
  filter(
    Sale_Price > 1000,
    Sale_Price < 6000000,
    Total_Building_SqFt > 10, # Filter out very small/error GBA
    !is.na(Sale_Price),
    !is.na(Total_Building_SqFt),
    !is.na(Sale_Date),
    !is.na(Year_Built)
  ) %>%
  mutate(
    Price_Per_SqFt = Sale_Price / Total_Building_SqFt,
    log_Price_Per_SqFt = log(Price_Per_SqFt)
  ) %>%
  filter(!is.na(log_Price_Per_SqFt) & !is.infinite(log_Price_Per_SqFt))

filtered_rows <- nrow(data_filtered)
cat("Original row count:", original_rows, "\n")
cat("Row count after initial filters:", filtered_rows, "\n")
cat("Data dimensions after filtering:", paste(dim(data_filtered), collapse = " x "), "\n")

# --- 2. Feature Engineering ---
cat("--- Cell 2: Feature Engineering ---\n")
cat("Starting feature engineering...\n")

data_engineered <- data_filtered %>%
  mutate(
    Sale_Year = year(Sale_Date),
    Sale_Month = as.factor(month(Sale_Date, label = TRUE, abbr = FALSE)), # Factor for modeling
    Age = Sale_Year - Year_Built
    # Consider other features if data is available, e.g., Years_Since_Remodel
  )

cat("Summary of Age before filtering non-positive values:\n")
print(summary(data_engineered$Age))
cat("Number of properties with Age <= 0 before filtering:", sum(data_engineered$Age <= 0, na.rm = TRUE), "\n")

# Filter out records with non-positive or very high unrealistic age
data_engineered <- data_engineered %>%
  filter(Age > 0, Age < 200) # Keep only positive and realistic ages

cat("\n--- After filtering out non-positive/unrealistic Age values ---\n")
cat("Summary of Age:\n")
print(summary(data_engineered$Age))
cat("Number of properties with Age <= 0:", sum(data_engineered$Age <= 0, na.rm = TRUE), "\n")
cat("Feature engineering complete. Data dimensions:", paste(dim(data_engineered), collapse = " x "), "\n")

# --- 3. Data Splitting ---
cat("--- Cell 3: Data Splitting ---\n")
set.seed(123) # For reproducibility
data_split <- initial_split(data_engineered, prop = 0.80, strata = log_Price_Per_SqFt) # 80/20 split
train_data <- training(data_split)
test_data  <- testing(data_split)

cat("Training data dimensions:", paste(dim(train_data), collapse = " x "), "\n")
cat("Testing data dimensions: ", paste(dim(test_data), collapse = " x "), "\n")

# --- 4. Recipe Definition & Preprocessing ---
cat("--- Cell 4: Recipe Definition & Preprocessing ---\n")

outcome_var <- "log_Price_Per_SqFt"
id_vars <- c("ETN", "Parcel_Number", "Sale_Price", "Price_Per_SqFt", "Sale_Date",
             "Latest_Appraisal_Date", "Latest_Merge_Event_Date")

property_recipe <- recipe(train_data) %>%
  update_role(all_of(id_vars), new_role = "ID") %>%
  update_role(all_of(outcome_var), new_role = "outcome") %>%
  update_role(-all_of(c(outcome_var, id_vars)), new_role = "predictor") %>%

  # 1. Handle new factor levels that appear in new data (assessment/test sets) first.
  #    This step is trained on all non-NA levels in the training data.
  #    If a new non-NA level appears in a new sample, it's converted to "Novel_Level_Encountered".
  step_novel(all_nominal_predictors(), new_level = "Novel_Level_Encountered") %>%

  # 2. Convert any remaining actual NA values in nominal predictors to a specific factor level.
  #    This should now only act on true NAs, as novel string values were handled above.
  step_unknown(all_nominal_predictors(), new_level = "Explicit_NA_as_Level") %>%

  # 3. Collapse infrequent factor levels.
  #    This operates on original levels + "Novel_Level_Encountered" + "Explicit_NA_as_Level".
  step_other(all_nominal_predictors(), threshold = 0.01, other = "Other_Infrequent") %>%

  # 4. Impute numeric predictors (remains the same)
  step_impute_median(all_numeric_predictors()) %>%

  # 5. Create dummy variables.
  step_dummy(all_nominal_predictors(), one_hot = FALSE) %>%

  # 6. Remove Zero-Variance Predictors
  step_zv(all_predictors())

# End of pipe for recipe definition
cat("Recipe defined. Summary:\n")
print(property_recipe)

# Prepare the recipe (fit the preprocessors on training data)
cat("Preparing the recipe (fitting preprocessors)...\n")
# It's good practice to prep the recipe separately to inspect it
# prepared_recipe <- prep(property_recipe, training = train_data, verbose = TRUE, log_changes = TRUE)
# print(summary(prepared_recipe)) # See what the recipe did

# --- 5. Model Specification (Random Forest with Ranger) ---
cat("--- Cell 5: Model Specification ---\n")

# Using ranger engine for speed and good defaults
# We can tune mtry and min_n later if needed
rf_spec <- rand_forest(
    mtry = tune(),  # Number of predictors to sample at each split (tune this)
    trees = 1000,   # Number of trees (can increase if needed, but 500-1000 is often good)
    min_n = tune()  # Minimum number of data points in a node to be split (tune this)
  ) %>%
  set_engine("ranger", importance = "permutation", num.threads = parallel::detectCores() -1 ) %>% # Use n-1 cores
  set_mode("regression")

cat("Random Forest model specification created.\n")
print(rf_spec)

# --- 6. Workflow ---
cat("--- Cell 6: Creating Workflow ---\n")
rf_workflow <- workflow() %>%
  add_recipe(property_recipe) %>% # Use the un-prepped recipe here
  add_model(rf_spec)

cat("Workflow created.\n")
print(rf_workflow)

# --- 7. Hyperparameter Tuning (Optional but Recommended) ---
cat("--- Cell 7: Hyperparameter Tuning Setup ---\n")

# Create cross-validation folds from the training data
set.seed(234)
cv_folds <- vfold_cv(train_data, v = 5, strata = log_Price_Per_SqFt) # 5-fold CV

# Define a grid for tuning (example, adjust ranges as needed)
# For mtry, it's often a good idea to try values around sqrt(number_of_predictors)
# We'll estimate number of predictors after recipe is prepped on a small sample
# temp_prep <- prep(property_recipe)
# num_predictors_after_dummy <- ncol(juice(temp_prep)) - length(id_vars) - 1 # -1 for outcome
# mtry_upper_bound <- min(15, num_predictors_after_dummy) # Cap mtry for practicality

# A simpler grid for now, can be expanded
rf_grid <- grid_regular(
  mtry(range = c(5, 20)), # Adjust range based on your number of features after dummification
  min_n(range = c(2, 20)),
  levels = 3 # Number of levels for each hyperparameter (3x3 = 9 combinations)
)
# For a more robust mtry range:
# First, prep the recipe to see how many predictors you'll have after dummification
# temp_prepped_recipe <- prep(property_recipe, training = train_data)
# num_final_predictors <- length(tidy(temp_prepped_recipe)$variable) - length(id_vars) -1 # rough estimate
# mtry_val_max <- min(num_final_predictors, 30) # Don't let mtry be too large
# mtry_val_min <- max(2, floor(sqrt(num_final_predictors)))
# rf_grid <- grid_regular(
#   mtry(range = c(mtry_val_min, mtry_val_max)),
#   min_n(range = c(2, 20)),
#   levels = 3
# )


cat("Tuning grid created with", nrow(rf_grid), "combinations.\n")
print(rf_grid)

# Use tune_grid to find the best hyperparameters
# This can take some time depending on data size and grid size
# library(doParallel) # For parallel processing
# cl <- makePSOCKcluster(parallel::detectCores() - 1)
# registerDoParallel(cl)

cat("Starting hyperparameter tuning (this may take a while)...\n")
rf_tune_results <- tune_grid(
  rf_workflow,
  resamples = cv_folds,
  grid = rf_grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(save_pred = TRUE, verbose = TRUE) # save_pred for later analysis
)

# stopCluster(cl) # Stop parallel cluster if used

cat("Tuning complete.\n")
show_best(rf_tune_results, metric = "rmse", n = 5)
show_best(rf_tune_results, metric = "rsq", n = 5)

best_rf_params <- select_best(rf_tune_results, metric = "rmse")
cat("Best hyperparameters for RMSE:\n")
print(best_rf_params)

# Finalize the workflow with the best hyperparameters
final_rf_workflow <- finalize_workflow(rf_workflow, best_rf_params)

# --- 8. Train Final Model and Evaluate on Test Set ---
cat("--- Cell 8: Train Final Model and Evaluate ---\n")

# Fit the final model on the full training set
cat("Training final model on the entire training set...\n")
final_rf_fit <- fit(final_rf_workflow, data = train_data)

# Alternatively, if you want to fit on the last fold of CV (less common for final model)
# last_rf_fit <- last_fit(final_rf_workflow, data_split, metrics = metric_set(rmse, rsq, mae))
# cat("Performance on test set (from last_fit):\n")
# collect_metrics(last_rf_fit)
# test_predictions_last_fit <- collect_predictions(last_rf_fit)

# Make predictions on the test set
cat("Making predictions on the test set...\n")
test_predictions <- predict(final_rf_fit, new_data = test_data) %>%
  bind_cols(test_data %>% select(log_Price_Per_SqFt, Sale_Price, ETN, Parcel_Number)) # Add actuals and IDs

# Calculate performance metrics
cat("Calculating performance metrics on the test set...\n")
test_metrics <- test_predictions %>%
  metrics(truth = log_Price_Per_SqFt, estimate = .pred)
print(test_metrics)

# For R-squared, it's often more intuitive on the original price scale if possible,
# but since we modeled log_Price_Per_SqFt, we evaluate on that scale.
# To get predictions back on the original price scale:
test_predictions <- test_predictions %>%
  mutate(
    Predicted_Price_Per_SqFt = exp(.pred),
    Predicted_Sale_Price = Predicted_Price_Per_SqFt * (test_data$Total_Building_SqFt) # Assuming Total_Building_SqFt is in test_data
  )

# RMSE on original Sale_Price (for interpretation, but model was optimized for log scale)
# Ensure Total_Building_SqFt is available in test_data if not already included by the recipe
# If Total_Building_SqFt was transformed or removed by recipe, you'd need to join it back
# or ensure it's passed through with an "ID" role.
# For simplicity, let's assume it's directly available in test_data (it should be if not a predictor)
if ("Total_Building_SqFt" %in% names(test_data)) {
  rmse_original_scale <- test_predictions %>%
    yardstick::rmse(truth = Sale_Price, estimate = Predicted_Sale_Price)
  cat("RMSE on original Sale_Price scale (interpret with caution):", rmse_original_scale$.estimate, "\n")
} else {
  cat("Total_Building_SqFt not found directly in test_data for original scale RMSE calculation.\n")
}


# --- 9. Variable Importance ---
cat("--- Cell 9: Variable Importance ---\n")
# Extract the fitted model object from the workflow
fitted_model_object <- extract_fit_parsnip(final_rf_fit)

# Create variable importance plot
# Note: For ranger, importance = "permutation" was set.
# If you used "impurity", the interpretation is different.
cat("Generating variable importance plot...\n")
vip_plot <- vip(fitted_model_object, num_features = 20, geom = "col", aesthetics = list(fill = "midnightblue", alpha = 0.8)) +
  theme_minimal(base_size = 12) +
  labs(title = "Top 20 Most Important Features (Permutation Importance)",
       x = "Predictor",
       y = "Importance")
print(vip_plot)

# --- 10. Further Analysis (Example: Residual Plot) ---
cat("--- Cell 10: Residual Analysis ---\n")
residuals_plot <- ggplot(test_predictions, aes(x = .pred, y = log_Price_Per_SqFt - .pred)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs. Predicted Values (log_Price_Per_SqFt)",
       x = "Predicted log(Price/SqFt)",
       y = "Residuals (Actual - Predicted)") +
  theme_minimal()
print(residuals_plot)

cat("Modeling script finished.\n")

# To save the final model:
# saveRDS(final_rf_fit, "final_random_forest_model.rds")
# loaded_model <- readRDS("final_random_forest_model.rds")
```



```{r}
# --- 0. Load Libraries ---
cat("Loading libraries...\n")
library(arrow)
library(tidyverse)
library(tidymodels)
library(lubridate)
library(vip)       # For variable importance plots
library(xgboost)   # For the xgboost engine

# --- 1. Load Data and Initial Cleaning/Filtering ---
cat("--- Cell 1: Load Data and Initial Filters ---\n")
parquet_file_path <- "../data/stage/full_data_for_modeling.parquet"

cat("Loading data from Parquet file:", parquet_file_path, "...\n")
if (!file.exists(parquet_file_path)) {
  stop("Parquet file not found: ", parquet_file_path)
}
data_cleaned <- read_parquet(parquet_file_path)
cat("Loaded", nrow(data_cleaned), "rows and", ncol(data_cleaned), "columns from Parquet.\n")

data_cleaned <- as_tibble(data_cleaned)

cat("Starting initial data type conversions...\n")
data_cleaned <- data_cleaned %>%
  mutate(
    Sale_Date = as_date(Sale_Date),
    Year_Built = as.numeric(Year_Built),
    Parcel_Number = as.character(Parcel_Number),
    ETN = as.character(ETN)
  )
cat("Initial data type conversions done.\n")

cat("Filtering data and creating target variable...\n")
original_rows <- nrow(data_cleaned)
data_filtered <- data_cleaned %>%
  filter(
    Sale_Price > 1000,
    Sale_Price < 6000000,
    Total_Building_SqFt > 10,
    !is.na(Sale_Price),
    !is.na(Total_Building_SqFt),
    !is.na(Sale_Date),
    !is.na(Year_Built)
  ) %>%
  mutate(
    Price_Per_SqFt = Sale_Price / Total_Building_SqFt,
    log_Price_Per_SqFt = log(Price_Per_SqFt)
  ) %>%
  filter(!is.na(log_Price_Per_SqFt) & !is.infinite(log_Price_Per_SqFt))

filtered_rows <- nrow(data_filtered)
cat("Original row count:", original_rows, "\n")
cat("Row count after initial filters:", filtered_rows, "\n")
cat("Data dimensions after filtering:", paste(dim(data_filtered), collapse = " x "), "\n")

# --- 2. Feature Engineering ---
cat("--- Cell 2: Feature Engineering ---\n")
cat("Starting feature engineering...\n")

data_engineered <- data_filtered %>%
  mutate(
    Sale_Year = year(Sale_Date),
    Sale_Month = as.factor(month(Sale_Date, label = TRUE, abbr = FALSE)),
    Age = Sale_Year - Year_Built
  )

cat("Summary of Age before filtering non-positive values:\n")
print(summary(data_engineered$Age))
cat("Number of properties with Age <= 0 before filtering:", sum(data_engineered$Age <= 0, na.rm = TRUE), "\n")

data_engineered <- data_engineered %>%
  filter(Age > 0, Age < 200)

cat("\n--- After filtering out non-positive/unrealistic Age values ---\n")
cat("Summary of Age:\n")
print(summary(data_engineered$Age))
cat("Number of properties with Age <= 0:", sum(data_engineered$Age <= 0, na.rm = TRUE), "\n")
cat("Feature engineering complete. Data dimensions:", paste(dim(data_engineered), collapse = " x "), "\n")

# --- 3. Data Splitting ---
cat("--- Cell 3: Data Splitting ---\n")
set.seed(123) # Use the SAME seed for a fair comparison with the RF model
data_split <- initial_split(data_engineered, prop = 0.80, strata = log_Price_Per_SqFt)
train_data <- training(data_split)
test_data  <- testing(data_split)

cat("Training data dimensions:", paste(dim(train_data), collapse = " x "), "\n")
cat("Testing data dimensions: ", paste(dim(test_data), collapse = " x "), "\n")
# --- 4. Recipe Definition & Preprocessing (for XGBoost) ---
cat("--- Cell 4: Recipe Definition & Preprocessing (XGBoost) ---\n")

# Define outcome and ID variables (should be identical to RF script)
outcome_var <- "log_Price_Per_SqFt"
id_vars <- c("ETN", "Parcel_Number", "Sale_Price", "Price_Per_SqFt", "Sale_Date",
             "Latest_Appraisal_Date", "Latest_Merge_Event_Date")

# Define the recipe
property_recipe<- recipe(train_data) %>% # Use a distinct name like property_recipe_xgb if running both in same session
  update_role(all_of(id_vars), new_role = "ID") %>%
  update_role(all_of(outcome_var), new_role = "outcome") %>%
  update_role(-all_of(c(outcome_var, id_vars)), new_role = "predictor") %>%

  # 1. Handle new factor levels that appear in new data (assessment/test sets) first.
  step_novel(all_nominal_predictors(), new_level = "Novel_Level_Encountered") %>%

  # 2. Convert any remaining actual NA values in nominal predictors to a specific factor level.
  step_unknown(all_nominal_predictors(), new_level = "Explicit_NA_as_Level") %>%

  # 3. Collapse infrequent factor levels.
  step_other(all_nominal_predictors(), threshold = 0.01, other = "Other_Infrequent") %>%

  # 4. Impute numeric predictors
  step_impute_median(all_numeric_predictors()) %>%

  # 5. Create dummy variables - Using one_hot = TRUE is common for XGBoost
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% # <--- Note: one_hot = TRUE

  # 6. Remove Zero-Variance Predictors
  step_zv(all_predictors())
  # Optional: step_normalize(all_numeric_predictors()) # XGBoost can sometimes benefit

# End of pipe for recipe definition
cat("Recipe defined for XGBoost. Summary:\n")
print(property_recipe_xgb)

# --- 5. Model Specification (XGBoost) ---
cat("--- Cell 5: Model Specification (XGBoost) ---\n")

xgb_spec <- boost_tree(
    trees = 1000,        # Number of boosting rounds (can tune)
    min_n = tune(),      # Min number of data points in a node
    tree_depth = tune(), # Max depth of a tree (e.g., 3-10)
    learn_rate = tune(), # Learning rate (eta) (e.g., 0.01-0.3)
    loss_reduction = tune(), # Gamma, min loss reduction to make a split
    sample_size = tune() # Subsample ratio of the training instance (e.g. 0.5-1)
    # mtry = tune()      # Number of predictors to sample at each split (colsample_bytree)
                         # Often set as a proportion of total predictors
  ) %>%
  set_engine("xgboost", importance = "permutation", num.threads = parallel::detectCores() -1) %>%
  set_mode("regression")

cat("XGBoost model specification created.\n")
print(xgb_spec)

# --- 6. Workflow (XGBoost) ---
cat("--- Cell 6: Creating Workflow (XGBoost) ---\n")
xgb_workflow <- workflow() %>%
  add_recipe(property_recipe) %>%
  add_model(xgb_spec)

cat("Workflow created.\n")
print(xgb_workflow)

# --- Cell 7: Hyperparameter Tuning (XGBoost) ---
cat("--- Cell 7: Hyperparameter Tuning (XGBoost) ---\n")

# Load necessary library for parallel processing
library(doParallel)

# --- Parallel Processing Setup ---
# Detect the number of available cores
num_cores <- detectCores()
cat("Number of available cores:", num_cores, "\n")

# Choose the number of cores to use (e.g., all available, or num_cores - 1 to leave one for system tasks)
# Using num_cores - 1 is often a good practice, especially if it's your personal machine.
# If on a dedicated server/cluster, you might use all.
cores_to_use <- max(1, num_cores - 1) # Ensure at least 1 core is used
cat("Number of cores to use for tuning:", cores_to_use, "\n")

# Register the parallel backend
cl <- makePSOCKcluster(cores_to_use)
registerDoParallel(cl)
cat("Parallel backend registered with", cores_to_use, "cores.\n")

# --- Define Hyperparameter Grid (as before) ---
# (Your xgb_grid definition here - no changes needed)
# Example:
xgb_grid <- grid_latin_hypercube(
  tree_depth(range = c(3L, 10L)),
  min_n(range = c(5L, 25L)),
  learn_rate(range = c(-2, -1)), # log10 transformed, so 0.01 to 0.1
  # mtry is typically a fraction of predictors for XGBoost,
  # but recipes::step_dummy can create many, so let's try a range of counts
  # or use finalize(mtry(), train_processed_for_mtry_check) if you bake the recipe once
  # For now, let's set a placeholder or a sensible range if you know approx # of predictors
  # mtry(range = c(5L, 50L)), # Adjust based on expected number of predictors after dummification
  loss_reduction(range = c(0.0, 0.1)),
  sample_size = sample_prop(range = c(0.5, 1.0)),
  size = 15 # Number of hyperparameter combinations
)
cat("XGBoost tuning grid defined with", nrow(xgb_grid), "combinations.\n")


# --- Tune the Grid (with parallel processing enabled) ---
cat("Starting XGBoost hyperparameter tuning (this may take a while)...\n")
start_time_xgb_tune <- Sys.time()

xgb_tune_results <- tune_grid(
  xgb_workflow,
  resamples = cv_folds,
  grid = xgb_grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(save_pred = TRUE, verbose = TRUE, allow_par = TRUE) # allow_par = TRUE is good practice
)

end_time_xgb_tune <- Sys.time()
cat("XGBoost tuning finished.\n")
print(end_time_xgb_tune - start_time_xgb_tune)

# --- Stop the Parallel Backend (Important!) ---
stopCluster(cl)
registerDoSEQ() # Unregister the parallel backend and revert to sequential
cat("Parallel backend stopped and reverted to sequential.\n")

cat("XGBoost tuning complete.\n")
show_best(xgb_tune_results, metric = "rmse", n = 5)
show_best(xgb_tune_results, metric = "rsq", n = 5)

best_xgb_params <- select_best(xgb_tune_results, metric = "rmse")
cat("Best XGBoost hyperparameters for RMSE:\n")
print(best_xgb_params)

final_xgb_workflow <- finalize_workflow(xgb_workflow, best_xgb_params)

# --- 8. Train Final Model and Evaluate on Test Set (XGBoost) ---
cat("--- Cell 8: Train Final Model and Evaluate (XGBoost) ---\n")
cat("Training final XGBoost model on the entire training set...\n")
final_xgb_fit <- fit(final_xgb_workflow, data = train_data)

cat("Making predictions on the test set with XGBoost...\n")
xgb_test_predictions <- predict(final_xgb_fit, new_data = test_data) %>%
  bind_cols(test_data %>% select(log_Price_Per_SqFt, Sale_Price, ETN, Parcel_Number))

cat("Calculating performance metrics on the test set for XGBoost...\n")
xgb_test_metrics <- xgb_test_predictions %>%
  metrics(truth = log_Price_Per_SqFt, estimate = .pred)
print(xgb_test_metrics)

xgb_test_predictions <- xgb_test_predictions %>%
  mutate(
    Predicted_Price_Per_SqFt = exp(.pred),
    Predicted_Sale_Price = Predicted_Price_Per_SqFt * (test_data$Total_Building_SqFt)
  )

if ("Total_Building_SqFt" %in% names(test_data)) {
  xgb_rmse_original_scale <- xgb_test_predictions %>%
    yardstick::rmse(truth = Sale_Price, estimate = Predicted_Sale_Price)
  cat("XGBoost RMSE on original Sale_Price scale (interpret with caution):", xgb_rmse_original_scale$.estimate, "\n")
} else {
  cat("Total_Building_SqFt not found directly in test_data for original scale RMSE calculation.\n")
}

# --- 9. Variable Importance (XGBoost) ---
cat("--- Cell 9: Variable Importance (XGBoost) ---\n")
xgb_fitted_model_object <- extract_fit_parsnip(final_xgb_fit)

cat("Generating XGBoost variable importance plot...\n")
xgb_vip_plot <- vip(xgb_fitted_model_object, num_features = 20, geom = "col", aesthetics = list(fill = "darkgreen", alpha = 0.8)) +
  theme_minimal(base_size = 12) +
  labs(title = "Top 20 Most Important Features (XGBoost Permutation Importance)",
       x = "Predictor",
       y = "Importance")
print(xgb_vip_plot)

# --- 10. Further Analysis (Example: Residual Plot - XGBoost) ---
cat("--- Cell 10: Residual Analysis (XGBoost) ---\n")
xgb_residuals_plot <- ggplot(xgb_test_predictions, aes(x = .pred, y = log_Price_Per_SqFt - .pred)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs. Predicted Values (XGBoost log_Price_Per_SqFt)",
       x = "Predicted log(Price/SqFt)",
       y = "Residuals (Actual - Predicted)") +
  theme_minimal()
print(xgb_residuals_plot)

cat("XGBoost modeling script finished.\n")

# To save the final XGBoost model:
# saveRDS(final_xgb_fit, "final_xgboost_model.rds")
```


```{r}
# --- 0. Load Libraries ---
# Ensure all necessary packages are installed:
# install.packages(c("arrow", "dplyr", "lubridate", "xgboost", "Metrics", "ggplot2", "stringr", "forcats", "fastDummies", "pdp"))

library(arrow)
library(dplyr)
library(lubridate)
library(xgboost)
library(Metrics) # For MAE, RMSE
library(ggplot2)
library(stringr) # For string manipulations if needed
library(forcats) # For factor manipulation
library(fastDummies) # For creating dummy variables if not using XGBoost's internal handling
library(pdp)     # For partial dependence plots

# --- 1. Load Data ---
cat("Loading data...\n")
full_data <- tryCatch({
  read_parquet("../data/stage/full_data_for_modeling.parquet")
}, error = function(e) {
  cat("Error loading full_data_for_modeling.parquet:", e$message, "\n")
  cat("Please ensure the data engineering script has been run successfully and the file exists in the working directory.\n")
  return(NULL)
})

if (is.null(full_data)) {
  stop("Halting script due to data loading error.")
}

cat("Data loaded successfully. Dimensions:", dim(full_data), "\n")
# print(head(full_data)) # Optional: print head to verify

# --- 2. Initial Data Cleaning & Feature Engineering ---
cat("Starting initial data cleaning and feature engineering...\n")

# Convert character columns that should be numeric (handle NAs introduced by coercion)
numeric_cols_to_convert <- c(
  "Assessed_Value_Current_Year", "Taxable_Value_Current_Year", "Market_Value_Improvement_Current_Year",
  "Market_Value_Land_Current_Year", "Most_Recent_Sale_Price", "Total_Bedrooms", "Total_Bathrooms",
  "Total_Stories", "Total_Building_SqFt", "Total_Finished_SqFt", "Total_Living_Units",
  "Num_Buildings_Good_Condition", "Num_Residential_Buildings", "Num_Commercial_Buildings",
  "Num_Industrial_Buildings", "Num_Agricultural_Buildings", "Num_Government_Buildings",
  "Num_Utility_Buildings", "Num_Other_Buildings", "Year_Built_Mode", "Effective_Year_Built_Mode"
)

for (col_name in numeric_cols_to_convert) {
  if (col_name %in% names(full_data)) {
    if (is.character(full_data[[col_name]])) {
      full_data[[col_name]] <- as.numeric(full_data[[col_name]])
    }
  } else {
    cat("Warning: Column", col_name, "not found for numeric conversion.\n")
  }
}

# Ensure Sale_Price is numeric (it should be from the data engineering step)
if ("Sale_Price" %in% names(full_data) && !is.numeric(full_data$Sale_Price)) {
  full_data$Sale_Price <- as.numeric(full_data$Sale_Price)
}

# Create the target variable: log of Sale_Price
# Filter out non-positive Sale_Price values before log transformation
full_data <- full_data %>%
  filter(Sale_Price > 0) %>%
  mutate(Sale_Price_log = log(Sale_Price))

# Filter out properties with Total_Building_SqFt == 1 (or other problematic values)
cat("Filtering out properties with Total_Building_SqFt <= 1...\n")
initial_rows <- nrow(full_data)
full_data <- full_data %>%
  filter(Total_Building_SqFt > 1)
rows_removed <- initial_rows - nrow(full_data)
cat(rows_removed, "rows removed due to Total_Building_SqFt <= 1.\n")
cat("Data dimensions after filtering:", dim(full_data), "\n")


# Feature Engineering: Age of Property
if ("Most_Recent_Sale_Date" %in% names(full_data) && "Year_Built_Mode" %in% names(full_data)) {
  full_data <- full_data %>%
    mutate(
      Sale_Year = year(ymd(Most_Recent_Sale_Date)), # Ensure Most_Recent_Sale_Date is Date type
      Property_Age_at_Sale = Sale_Year - Year_Built_Mode
    )
  # Handle cases where Year_Built_Mode might be 0 or NA, or Sale_Year is before Year_Built_Mode
  full_data$Property_Age_at_Sale[full_data$Property_Age_at_Sale < 0 | is.na(full_data$Property_Age_at_Sale)] <- NA
} else {
  cat("Warning: Most_Recent_Sale_Date or Year_Built_Mode not found for Property_Age_at_Sale calculation.\n")
}


# --- 3. Define Roles and Select Features ---
cat("Defining roles and selecting features...\n")

# Target variable
target_variable <- "Sale_Price_log"

# ID variables (not to be used as predictors)
id_variables <- c("Parcel_Number", "Most_Recent_Sale_Date", "Sale_Price") # Sale_Price is kept for reference but not for prediction

# Potential predictor variables (start with a broad set and refine)
# Exclude the original target and ID variables
all_cols <- names(full_data)
potential_predictors <- setdiff(all_cols, c(target_variable, id_variables))

# Identify character/factor columns for potential one-hot encoding or factor handling
character_cols <- names(full_data)[sapply(full_data, function(x) is.character(x) || is.factor(x))]
character_predictors <- intersect(potential_predictors, character_cols)

# Identify numeric columns
numeric_predictors <- intersect(potential_predictors, names(full_data)[sapply(full_data, is.numeric)])

# Final predictor set (can be adjusted)
# For XGBoost, it can often handle factors directly or we can one-hot encode.
# Let's keep factors as factors for now and let XGBoost handle them.
# If issues arise, we can one-hot encode character_predictors.
selected_predictors <- c(numeric_predictors, character_predictors)
# Ensure no ID or target variables accidentally slipped into selected_predictors
selected_predictors <- setdiff(selected_predictors, c(target_variable, id_variables))

cat("Selected predictors:", paste(selected_predictors, collapse=", "), "\n")

# Subset data to only include selected predictors and the target
modeling_data <- full_data[, c(selected_predictors, target_variable), drop = FALSE]

# Convert character columns to factors for XGBoost (it handles factors well)
for (col in character_predictors) {
  if (col %in% names(modeling_data)) {
    modeling_data[[col]] <- as.factor(modeling_data[[col]])
  }
}

# --- 4. Handle Missing Values (NA Imputation) ---
cat("Handling missing values...\n")
# For numeric predictors: impute with median (or mean, or a specific value like 0 or -1)
for (col in numeric_predictors) {
  if (col %in% names(modeling_data) && any(is.na(modeling_data[[col]]))) {
    median_val <- median(modeling_data[[col]], na.rm = TRUE)
    if (is.na(median_val)) median_val <- 0 # Fallback if all are NA
    modeling_data[[col]][is.na(modeling_data[[col]])] <- median_val
    cat("Imputed NAs in numeric column:", col, "with median", median_val, "\n")
  }
}

# For factor predictors: impute with a new level "Missing" or the mode
for (col in character_predictors) { # These are now factors
  if (col %in% names(modeling_data) && any(is.na(modeling_data[[col]]))) {
    # Add a new level for "Missing"
    levels <- levels(modeling_data[[col]])
    if (!("Missing" %in% levels)) {
      levels <- c(levels, "Missing")
      modeling_data[[col]] <- factor(modeling_data[[col]], levels = levels)
    }
    modeling_data[[col]][is.na(modeling_data[[col]])] <- "Missing"
    cat("Imputed NAs in factor column:", col, "with 'Missing' level\n")
  }
}
# Verify no NAs left in predictors or target (target NAs should have been filtered earlier)
# print(sapply(modeling_data, function(x) sum(is.na(x))))


# --- 5. Split Data into Training and Testing Sets ---
cat("Splitting data into training and testing sets...\n")
set.seed(123) # for reproducibility
train_indices <- sample(1:nrow(modeling_data), 0.8 * nrow(modeling_data))
train_data <- modeling_data[train_indices, ]
test_data <- modeling_data[-train_indices, ]

cat("Training data dimensions:", dim(train_data), "\n")
cat("Testing data dimensions:", dim(test_data), "\n")

# Prepare data for XGBoost
# XGBoost requires a matrix for features and a numeric vector for the target
# It can handle factor columns directly if they are properly prepared.
# Using the formula interface is often easier.

train_labels <- train_data[[target_variable]]
train_features_df <- train_data[, !(names(train_data) %in% target_variable), drop = FALSE]

test_labels <- test_data[[target_variable]]
test_features_df <- test_data[, !(names(test_data) %in% target_variable), drop = FALSE]

# Create model matrix (handles factor conversion to numeric for XGBoost)
# Ensure all factor levels in test are present in train, or handle appropriately
# For simplicity, model.matrix will create dummy variables.
# Ensure consistent factor levels if doing this manually or for other models.
# For XGBoost, it's often better to let it handle factors internally if possible,
# or use a formula interface.

# Using formula interface for xgb.train (more robust for factors)
# Construct the formula string
formula_str <- paste(target_variable, "~ .")
formula_obj <- as.formula(formula_str)

# Create DMatrix objects using the formula
# Note: xgb.DMatrix with a formula will internally handle factor encoding.
# Ensure all factor levels in test_data are also present in train_data or are handled.
# A common way is to ensure factors are defined with all possible levels before splitting,
# or by aligning factor levels post-split if necessary.

# For simplicity and robustness with factors, let's ensure all factor columns
# in train_features_df and test_features_df have the same levels.
# This is crucial if not using a method that handles this automatically.
# However, xgb.DMatrix with a formula should handle this.

# If not using formula, you'd do:
# train_matrix <- model.matrix(~ . -1, data = train_features_df) # -1 removes intercept
# test_matrix <- model.matrix(~ . -1, data = test_features_df)
# dtrain <- xgb.DMatrix(data = train_matrix, label = train_labels, missing = NA) # XGBoost can handle NAs if specified
# dtest <- xgb.DMatrix(data = test_matrix, label = test_labels, missing = NA)

# Using formula with xgb.DMatrix (recommended for ease with factors)
# Ensure target is not in the feature set passed to data argument
dtrain <- xgb.DMatrix(data = data.matrix(train_features_df), label = train_labels, missing = NA)
dtest <- xgb.DMatrix(data = data.matrix(test_features_df), label = test_labels, missing = NA)


# --- 6. Train XGBoost Model ---
cat("Training XGBoost model...\n")

# Set XGBoost parameters
params <- list(
  booster = "gbtree",
  objective = "reg:squarederror", # for regression
  eta = 0.05,                     # learning rate
  max_depth = 7,                  # max depth of a tree
  min_child_weight = 1,           # min sum of instance weight needed in a child
  subsample = 0.7,                # subsample ratio of the training instance
  colsample_bytree = 0.7,         # subsample ratio of columns when constructing each tree
  gamma = 0.1,                    # minimum loss reduction required to make a further partition
  nthread = 16                    # Number of CPU cores to use
)

# Set number of rounds (iterations)
nrounds <- 500 # Start with a moderate number, can be tuned with xgb.cv

# Watchlist for monitoring performance on test set during training
watchlist <- list(train = dtrain, test = dtest)

# Train the model
xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = nrounds,
  watchlist = watchlist,
  early_stopping_rounds = 50, # Stop if performance doesn't improve for 50 rounds on the test set
  verbose = 1 # Print evaluation metric at each round (0 for silent, 1 for progress, 2 for detailed)
  # eval_metric = "rmse" # or "mae" - will use objective's default if not set (rmse for reg:squarederror)
)

cat("XGBoost model training complete.\n")
cat("Best iteration:", xgb_model$best_iteration, "\n")
cat("Best RMSE on test set:", xgb_model$evaluation_log$test_rmse[xgb_model$best_iteration], "\n")


# --- 7. Make Predictions ---
cat("Making predictions on the test set...\n")
best_iter <- as.integer(xgb_model$best_iteration)
predictions_log <- predict(xgb_model, dtest, iterationrange = c(1, best_iter))

# Transform predictions back to original scale (since we predicted log(Sale_Price))
predictions_original_scale <- exp(predictions_log)
test_labels_original_scale <- exp(test_labels) # Original Sale_Price from test set

# --- 8. Evaluate Model ---
cat("Evaluating model performance...\n")

# Ensure no NAs or Infs in predictions or actuals before calculating metrics
valid_preds <- is.finite(predictions_original_scale) & is.finite(test_labels_original_scale)
predictions_original_scale_clean <- predictions_original_scale[valid_preds]
test_labels_original_scale_clean <- test_labels_original_scale[valid_preds]

if(length(predictions_original_scale_clean) == 0) {
  cat("Error: No valid predictions to evaluate after cleaning NAs/Infs.\n")
} else {
  mae_value <- mae(actual = test_labels_original_scale_clean, predicted = predictions_original_scale_clean)
  rmse_value <- rmse(actual = test_labels_original_scale_clean, predicted = predictions_original_scale_clean)
  
  # Calculate R-squared
  rss <- sum((predictions_original_scale_clean - test_labels_original_scale_clean)^2)
  tss <- sum((test_labels_original_scale_clean - mean(test_labels_original_scale_clean))^2)
  r_squared <- 1 - (rss / tss)
  
  # Calculate Mean Absolute Percentage Error (MAPE) - be cautious if actuals can be zero
  # Filter out zero actuals for MAPE calculation
  non_zero_actuals_mask <- test_labels_original_scale_clean != 0
  mape_value <- if(sum(non_zero_actuals_mask) > 0) {
    mean(abs((test_labels_original_scale_clean[non_zero_actuals_mask] - predictions_original_scale_clean[non_zero_actuals_mask]) / test_labels_original_scale_clean[non_zero_actuals_mask])) * 100
  } else {
    NA # Cannot calculate MAPE if all actuals are zero or no non-zero actuals
  }

  cat("Model Performance (Original Scale):\n")
  cat("MAE: $", format(round(mae_value, 2), nsmall = 2, big.mark = ","), "\n")
  cat("RMSE: $", format(round(rmse_value, 2), nsmall = 2, big.mark = ","), "\n")
  cat("R-squared:", round(r_squared, 4), "\n")
  if (!is.na(mape_value)) {
    cat("MAPE:", round(mape_value, 2), "%\n")
  } else {
    cat("MAPE: Not calculable (e.g., due to zero actual values or no valid non-zero actuals)\n")
  }
}

# --- 9. Feature Importance ---
cat("Calculating feature importance...\n")
importance_matrix <- xgb.importance(model = xgb_model)
print(importance_matrix)

# Plot feature importance
if (nrow(importance_matrix) > 0) {
  xgb.plot.importance(importance_matrix = importance_matrix, top_n = 20)
  title("XGBoost Feature Importance (Top 20)")
} else {
  cat("No feature importance data to plot.\n")
}


# --- 10. Residual Analysis (Optional) ---
cat("Performing residual analysis (optional)...\n")
residuals_log_scale <- test_labels - predictions_log # Residuals on the log scale
residuals_original_scale <- test_labels_original_scale_clean - predictions_original_scale_clean # If using cleaned data

# Plot residuals vs. predicted values (log scale)
if (length(predictions_log) == length(residuals_log_scale)) {
  residual_plot_data_log <- data.frame(Predicted = predictions_log, Residuals = residuals_log_scale)
  p_res_log <- ggplot(residual_plot_data_log, aes(x = Predicted, y = Residuals)) +
    geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(title = "Residuals vs. Predicted Values (Log Scale)",
         x = "Predicted Sale_Price_log",
         y = "Residuals (log scale)") +
    theme_minimal()
  print(p_res_log)
}


# Plot residuals vs. predicted values (original scale)
if (length(predictions_original_scale_clean) == length(residuals_original_scale)) {
  residual_plot_data_orig <- data.frame(Predicted = predictions_original_scale_clean, Residuals = residuals_original_scale)
  p_res_orig <- ggplot(residual_plot_data_orig, aes(x = Predicted, y = Residuals)) +
    geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    scale_x_continuous(labels = scales::comma) + # Format x-axis with commas
    scale_y_continuous(labels = scales::comma) + # Format y-axis with commas
    labs(title = "Residuals vs. Predicted Values (Original Scale)",
         x = "Predicted Sale_Price (Original Scale)",
         y = "Residuals (Original Scale)") +
    theme_minimal()
  print(p_res_orig)
}

# --- 11. Actual vs. Predicted Plot ---
cat("Generating Actual vs. Predicted plot...\n")
if (length(test_labels_original_scale_clean) == length(predictions_original_scale_clean)) {
  actual_vs_predicted_data <- data.frame(
    Actual = test_labels_original_scale_clean,
    Predicted = predictions_original_scale_clean
  )
  
  p_avp <- ggplot(actual_vs_predicted_data, aes(x = Actual, y = Predicted)) +
    geom_point(alpha = 0.3) +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    scale_x_continuous(labels = scales::comma) +
    scale_y_continuous(labels = scales::comma) +
    labs(title = "Actual vs. Predicted Sale Price (Original Scale)",
         x = "Actual Sale Price",
         y = "Predicted Sale Price") +
    coord_fixed(ratio = 1, xlim = range(actual_vs_predicted_data$Actual, na.rm = TRUE), ylim = range(actual_vs_predicted_data$Predicted, na.rm = TRUE)) + # Maintain 1:1 aspect ratio
    theme_minimal()
  print(p_avp)
}

# --- 12. Partial Dependence Plots (Example for top features) ---
if (nrow(importance_matrix) > 0 && requireNamespace("pdp", quietly = TRUE)) {
  cat("Generating Partial Dependence Plots (PDP)...\n")
  
  # Select top N features for PDP
  top_n_features_for_pdp <- min(5, nrow(importance_matrix)) # e.g., top 5
  features_for_pdp <- importance_matrix$Feature[1:top_n_features_for_pdp]
  
  # Ensure these features are in train_features_df
  features_for_pdp <- intersect(features_for_pdp, names(train_features_df))

  if(length(features_for_pdp) > 0) {
    for(feature_name in features_for_pdp) {
      cat("Generating PDP for feature:", feature_name, "\n")
      tryCatch({
        # For pdp::partial, the 'object' is the xgb_model, 'pred.var' is the feature name,
        # 'train' is the training data frame used to fit the model (features only).
        # 'pred.fun' is a function that takes the model and newdata and returns predictions.
        pdp_plot <- pdp::partial(
          object = xgb_model,
          pred.var = feature_name,
          train = train_features_df, # Use the feature dataframe used for training
          pred.fun = function(object, newdata) predict(object, data.matrix(newdata)), # Custom predict for XGBoost
          # For classification, you might need to specify prob = TRUE or type = "prob"
          # and select the probability of the class of interest. For regression, it's simpler.
          plot = TRUE,
          rug = TRUE,
          plot.engine = "ggplot2"
        )
        print(pdp_plot + labs(title = paste("Partial Dependence Plot for", feature_name)))
      }, error = function(e) {
        cat("Could not generate PDP for", feature_name, ":", e$message, "\n")
      })
    }
  } else {
    cat("No valid features found for PDP after intersection with training data names.\n")
  }
} else {
  cat("Skipping PDPs: pdp package not available or no feature importance.\n")
}

cat("Script finished.\n")
```



```{r}
# --- 0. Load Libraries ---
# Ensure all necessary packages are installed:
# install.packages(c("arrow", "dplyr", "lubridate", "xgboost", "Metrics", "ggplot2", "stringr", "forcats", "fastDummies", "pdp"))

library(arrow)
library(dplyr)
library(lubridate)
library(xgboost)
library(Metrics) # For MAE, RMSE
library(ggplot2)
library(stringr) # For string manipulations if needed
library(forcats) # For factor manipulation
# library(fastDummies) # Not explicitly used if XGBoost handles factors or model.matrix is used
library(pdp)     # For partial dependence plots

# --- 1. Load Data ---
cat("Loading data...\n")
full_data_path <- "../data/stage/full_data_for_modeling.parquet" # Define path for clarity
full_data <- tryCatch({
  read_parquet(full_data_path)
}, error = function(e) {
  cat("Error loading", full_data_path, ":", e$message, "\n")
  cat("Please ensure the data engineering script has been run successfully and the file exists in the working directory.\n")
  return(NULL)
})

if (is.null(full_data)) {
  stop("Halting script due to data loading error.")
}

cat("Data loaded successfully. Initial dimensions:", dim(full_data), "\n")

# --- 1a. Filter for Specific Housing Types ---
cat("Filtering for specific housing types (Residential and Condominium)...\n")
housing_property_types_to_keep <- c("Residential", "Condominium")

initial_rows_before_housing_filter <- nrow(full_data)
full_data <- full_data %>%
  filter(Property_Type %in% housing_property_types_to_keep)

rows_removed_housing_filter <- initial_rows_before_housing_filter - nrow(full_data)
cat(rows_removed_housing_filter, "rows removed by housing type filter.\n")
cat("Data dimensions after housing type filter:", dim(full_data), "\n")

if (nrow(full_data) == 0) {
  stop("No data remaining after filtering for housing types. Please check your 'housing_property_types_to_keep' list and data.")
}


# --- 2. Initial Data Cleaning & Feature Engineering ---
cat("Starting initial data cleaning and feature engineering...\n")

# Convert character columns that should be numeric (handle NAs introduced by coercion)
numeric_cols_to_convert <- c(
  "Assessed_Value_Current_Year", "Taxable_Value_Current_Year", "Market_Value_Improvement_Current_Year",
  "Market_Value_Land_Current_Year", "Most_Recent_Sale_Price", "Total_Bedrooms", "Total_Bathrooms",
  "Total_Stories", "Total_Building_SqFt", "Total_Finished_SqFt", "Total_Living_Units",
  "Num_Buildings_Good_Condition", "Num_Residential_Buildings", "Num_Commercial_Buildings",
  "Num_Industrial_Buildings", "Num_Agricultural_Buildings", "Num_Government_Buildings",
  "Num_Utility_Buildings", "Num_Other_Buildings", "Year_Built_Mode", "Effective_Year_Built_Mode"
  # Note: Max_Land_Net_Acres should already be numeric from data engineering
)
if ("Max_Land_Net_Acres" %in% names(full_data) && is.character(full_data$Max_Land_Net_Acres)) {
    full_data$Max_Land_Net_Acres <- as.numeric(full_data$Max_Land_Net_Acres)
}


for (col_name in numeric_cols_to_convert) {
  if (col_name %in% names(full_data)) {
    if (is.character(full_data[[col_name]])) {
      full_data[[col_name]] <- as.numeric(full_data[[col_name]])
    }
  } else {
    cat("Warning: Column", col_name, "not found for numeric conversion during cleaning.\n")
  }
}

# Ensure Sale_Price is numeric
if ("Sale_Price" %in% names(full_data) && !is.numeric(full_data$Sale_Price)) {
  full_data$Sale_Price <- as.numeric(full_data$Sale_Price)
}

# Create the target variable: log of Sale_Price
# Filter out non-positive Sale_Price values before log transformation
full_data <- full_data %>%
  filter(Sale_Price > 0) %>%
  mutate(Sale_Price_log = log(Sale_Price))

# Filter out properties with Total_Building_SqFt == 1 (or other problematic values)
cat("Filtering out properties with Total_Building_SqFt <= 1...\n")
initial_rows_sqft_filter <- nrow(full_data)
full_data <- full_data %>%
  filter(Total_Building_SqFt > 1)
rows_removed_sqft_filter <- initial_rows_sqft_filter - nrow(full_data)
cat(rows_removed_sqft_filter, "rows removed due to Total_Building_SqFt <= 1.\n")
cat("Data dimensions after Total_Building_SqFt filter:", dim(full_data), "\n")

if (nrow(full_data) == 0) {
  stop("No data remaining after Total_Building_SqFt filter.")
}

# Feature Engineering: Age of Property
if ("Most_Recent_Sale_Date" %in% names(full_data) && "Year_Built_Mode" %in% names(full_data)) {
  full_data <- full_data %>%
    mutate(
      Sale_Year = year(ymd(Most_Recent_Sale_Date)),
      Property_Age_at_Sale = Sale_Year - Year_Built_Mode
    )
  full_data$Property_Age_at_Sale[full_data$Property_Age_at_Sale < 0 | is.na(full_data$Property_Age_at_Sale)] <- NA
} else {
  cat("Warning: Most_Recent_Sale_Date or Year_Built_Mode not found for Property_Age_at_Sale calculation.\n")
}


# --- 3. Define Roles and Select Features ---
cat("Defining roles and selecting features...\n")

# Target variable
target_variable <- "Sale_Price_log"

# ID variables (not to be used as predictors)
id_variables <- c("Parcel_Number", "Most_Recent_Sale_Date", "Sale_Price")

# Redundant variable to remove (as per exploration)
redundant_variables <- c("Appraisal_Account_Type_Summary")

# Potential predictor variables
all_cols <- names(full_data)
potential_predictors <- setdiff(all_cols, c(target_variable, id_variables, redundant_variables))

# Identify character/factor columns for potential one-hot encoding or factor handling
character_cols <- names(full_data)[sapply(full_data, function(x) is.character(x) || is.factor(x))]
character_predictors <- intersect(potential_predictors, character_cols)

# Identify numeric columns
numeric_predictors <- intersect(potential_predictors, names(full_data)[sapply(full_data, is.numeric)])

selected_predictors <- c(numeric_predictors, character_predictors)
selected_predictors <- setdiff(selected_predictors, c(target_variable, id_variables, redundant_variables)) # Ensure again

cat("Selected predictors (" , length(selected_predictors), "):\n", paste(selected_predictors, collapse=", "), "\n")

# Subset data to only include selected predictors and the target
modeling_data <- full_data[, c(selected_predictors, target_variable), drop = FALSE]

# Convert character columns to factors for XGBoost
for (col in character_predictors) {
  if (col %in% names(modeling_data)) {
    modeling_data[[col]] <- as.factor(modeling_data[[col]])
  }
}

# --- 4. Handle Missing Values (NA Imputation) ---
cat("Handling missing values...\n")
for (col in numeric_predictors) {
  if (col %in% names(modeling_data) && any(is.na(modeling_data[[col]]))) {
    median_val <- median(modeling_data[[col]], na.rm = TRUE)
    if (is.na(median_val) || !is.finite(median_val)) median_val <- 0 # Fallback if all are NA or median is Inf/-Inf
    modeling_data[[col]][is.na(modeling_data[[col]])] <- median_val
    # cat("Imputed NAs in numeric column:", col, "with median", median_val, "\n") # Can be verbose
  }
}

for (col in character_predictors) { # These are now factors
  if (col %in% names(modeling_data) && any(is.na(modeling_data[[col]]))) {
    levels <- levels(modeling_data[[col]])
    if (!("Missing" %in% levels)) {
      levels <- c(levels, "Missing")
      modeling_data[[col]] <- factor(modeling_data[[col]], levels = levels)
    }
    modeling_data[[col]][is.na(modeling_data[[col]])] <- "Missing"
    # cat("Imputed NAs in factor column:", col, "with 'Missing' level\n") # Can be verbose
  }
}
cat("Missing value imputation complete.\n")

# --- 5. Split Data into Training and Testing Sets ---
cat("Splitting data into training and testing sets...\n")
set.seed(123) # for reproducibility
train_indices <- sample(1:nrow(modeling_data), 0.8 * nrow(modeling_data))
train_data <- modeling_data[train_indices, ]
test_data <- modeling_data[-train_indices, ]

cat("Training data dimensions:", dim(train_data), "\n")
cat("Testing data dimensions:", dim(test_data), "\n")

train_labels <- train_data[[target_variable]]
train_features_df <- train_data[, !(names(train_data) %in% target_variable), drop = FALSE]

test_labels <- test_data[[target_variable]]
test_features_df <- test_data[, !(names(test_data) %in% target_variable), drop = FALSE]

# Create DMatrix objects
# XGBoost can often handle factor columns directly when data.matrix() is used,
# as it converts them to their numeric factor level representations.
dtrain <- xgb.DMatrix(data = data.matrix(train_features_df), label = train_labels, missing = NA)
dtest <- xgb.DMatrix(data = data.matrix(test_features_df), label = test_labels, missing = NA)
cat("DMatrix objects created.\n")

# --- 6. Train XGBoost Model ---
cat("Training XGBoost model...\n")

params <- list(
  booster = "gbtree",
  objective = "reg:squarederror",
  eta = 0.05,
  max_depth = 7,
  min_child_weight = 1,
  subsample = 0.7,
  colsample_bytree = 0.7,
  gamma = 0.1,
  nthread = 16
)

nrounds <- 500
watchlist <- list(train = dtrain, test = dtest)

xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = nrounds,
  watchlist = watchlist,
  early_stopping_rounds = 50,
  verbose = 1
  # eval_metric = "rmse" # Default for reg:squarederror
)

cat("XGBoost model training complete.\n")
best_iter_val <- xgb_model$best_iteration
cat("Best iteration:", best_iter_val, "\n")
if (!is.null(xgb_model$evaluation_log) && "test_rmse" %in% names(xgb_model$evaluation_log)) {
    cat("Best RMSE on test set (log scale):", xgb_model$evaluation_log$test_rmse[best_iter_val], "\n")
} else if (!is.null(xgb_model$evaluation_log) && "test_error" %in% names(xgb_model$evaluation_log)) { # Older XGBoost might use 'error'
    cat("Best eval metric on test set (log scale):", xgb_model$evaluation_log$test_error[best_iter_val], "\n")
}


# --- 7. Make Predictions ---
cat("Making predictions on the test set...\n")
# Use ntreelimit for predict.xgb.Booster, which is equivalent to best_iteration
predictions_log <- predict(xgb_model, dtest, ntreelimit = best_iter_val)

predictions_original_scale <- exp(predictions_log)
test_labels_original_scale <- exp(test_labels)

# --- 8. Evaluate Model ---
cat("Evaluating model performance...\n")

valid_preds <- is.finite(predictions_original_scale) & is.finite(test_labels_original_scale)
predictions_original_scale_clean <- predictions_original_scale[valid_preds]
test_labels_original_scale_clean <- test_labels_original_scale[valid_preds]

if(length(predictions_original_scale_clean) == 0) {
  cat("Error: No valid predictions to evaluate after cleaning NAs/Infs.\n")
} else {
  mae_value <- mae(actual = test_labels_original_scale_clean, predicted = predictions_original_scale_clean)
  rmse_value <- rmse(actual = test_labels_original_scale_clean, predicted = predictions_original_scale_clean)
  
  rss <- sum((predictions_original_scale_clean - test_labels_original_scale_clean)^2)
  tss <- sum((test_labels_original_scale_clean - mean(test_labels_original_scale_clean))^2)
  r_squared <- if (tss == 0) NA else (1 - (rss / tss)) # Handle case where all actuals are the same
  
  non_zero_actuals_mask <- test_labels_original_scale_clean != 0
  mape_value <- if(sum(non_zero_actuals_mask) > 0) {
    mean(abs((test_labels_original_scale_clean[non_zero_actuals_mask] - predictions_original_scale_clean[non_zero_actuals_mask]) / test_labels_original_scale_clean[non_zero_actuals_mask])) * 100
  } else {
    NA
  }

  cat("Model Performance (Original Scale) for HOUSING (Residential & Condominium):\n")
  cat("MAE: $", format(round(mae_value, 2), nsmall = 2, big.mark = ","), "\n")
  cat("RMSE: $", format(round(rmse_value, 2), nsmall = 2, big.mark = ","), "\n")
  cat("R-squared:", round(r_squared, 4), "\n")
  if (!is.na(mape_value)) {
    cat("MAPE:", round(mape_value, 2), "%\n")
  } else {
    cat("MAPE: Not calculable\n")
  }
}

# --- 9. Feature Importance ---
cat("Calculating feature importance...\n")
importance_matrix <- xgb.importance(model = xgb_model)
print(importance_matrix)

if (nrow(importance_matrix) > 0) {
  xgb.plot.importance(importance_matrix = importance_matrix, top_n = 20)
  title("XGBoost Feature Importance (Top 20) - Housing Model")
} else {
  cat("No feature importance data to plot.\n")
}


# --- 10. Residual Analysis ---
cat("Performing residual analysis...\n")
residuals_log_scale <- test_labels - predictions_log
residuals_original_scale <- test_labels_original_scale_clean - predictions_original_scale_clean

if (length(predictions_log) == length(residuals_log_scale)) {
  residual_plot_data_log <- data.frame(Predicted = predictions_log, Residuals = residuals_log_scale)
  p_res_log <- ggplot(residual_plot_data_log, aes(x = Predicted, y = Residuals)) +
    geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(title = "Residuals vs. Predicted (Log Scale) - Housing Model",
         x = "Predicted Sale_Price_log", y = "Residuals (log scale)") +
    theme_minimal()
  print(p_res_log)
}

if (length(predictions_original_scale_clean) == length(residuals_original_scale)) {
  residual_plot_data_orig <- data.frame(Predicted = predictions_original_scale_clean, Residuals = residuals_original_scale)
  p_res_orig <- ggplot(residual_plot_data_orig, aes(x = Predicted, y = Residuals)) +
    geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    scale_x_continuous(labels = scales::comma) +
    scale_y_continuous(labels = scales::comma) +
    labs(title = "Residuals vs. Predicted (Original Scale) - Housing Model",
         x = "Predicted Sale_Price (Original Scale)", y = "Residuals (Original Scale)") +
    theme_minimal()
  print(p_res_orig)
}

# --- 11. Actual vs. Predicted Plot ---
cat("Generating Actual vs. Predicted plot...\n")
if (length(test_labels_original_scale_clean) == length(predictions_original_scale_clean)) {
  actual_vs_predicted_data <- data.frame(
    Actual = test_labels_original_scale_clean,
    Predicted = predictions_original_scale_clean
  )
  
  p_avp <- ggplot(actual_vs_predicted_data, aes(x = Actual, y = Predicted)) +
    geom_point(alpha = 0.3) +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    scale_x_continuous(labels = scales::comma) +
    scale_y_continuous(labels = scales::comma) +
    labs(title = "Actual vs. Predicted Sale Price (Original Scale) - Housing Model",
         x = "Actual Sale Price", y = "Predicted Sale Price") +
    coord_fixed(ratio = 1, 
                xlim = range(c(actual_vs_predicted_data$Actual, actual_vs_predicted_data$Predicted), na.rm = TRUE), 
                ylim = range(c(actual_vs_predicted_data$Actual, actual_vs_predicted_data$Predicted), na.rm = TRUE)) +
    theme_minimal()
  print(p_avp)
}

# --- 12. Partial Dependence Plots (Example for top features) ---
if (nrow(importance_matrix) > 0 && requireNamespace("pdp", quietly = TRUE)) {
  cat("Generating Partial Dependence Plots (PDP)...\n")
  
  top_n_features_for_pdp <- min(5, nrow(importance_matrix))
  features_for_pdp <- importance_matrix$Feature[1:top_n_features_for_pdp]
  features_for_pdp <- intersect(features_for_pdp, names(train_features_df))

  if(length(features_for_pdp) > 0) {
    for(feature_name in features_for_pdp) {
      cat("Generating PDP for feature:", feature_name, "\n")
      tryCatch({
        pdp_plot_obj <- pdp::partial(
          object = xgb_model,
          pred.var = feature_name,
          train = train_features_df, 
          pred.fun = function(object, newdata) predict(object, data.matrix(newdata), ntreelimit = object$best_iteration),
          plot = TRUE,
          rug = TRUE,
          plot.engine = "ggplot2"
        )
        print(pdp_plot_obj + labs(title = paste("PDP for", feature_name, "- Housing Model")))
      }, error = function(e) {
        cat("Could not generate PDP for", feature_name, ":", e$message, "\n")
      })
    }
  } else {
    cat("No valid features found for PDP after intersection with training data names.\n")
  }
} else {
  cat("Skipping PDPs: pdp package not available or no feature importance.\n")
}

cat("Script finished.\n")
```


```{r}
# and before creating modeling_data
cat("Distribution of Parcel_Count in filtered housing data:\n")
print(table(full_data$Parcel_Count, useNA = "ifany"))
```

```{r}
# --- 0. Load Libraries ---
# Ensure all necessary packages are installed:
# install.packages(c("arrow", "dplyr", "lubridate", "xgboost", "Metrics", "ggplot2", "stringr", "forcats", "pdp", "foreach", "doParallel"))

library(arrow)
library(dplyr)
library(lubridate)
library(xgboost)
library(Metrics) # For MAE, RMSE
library(ggplot2)
library(stringr) # For string manipulations if needed
library(forcats) # For factor manipulation
library(pdp)     # For partial dependence plots
library(foreach) # For parallel loops
library(doParallel) # For parallel backend
library(caret)
# --- 1. Load Data ---
cat("Loading data...\n")
full_data_path <- "../data/stage/full_data_for_modeling.parquet"
full_data <- tryCatch({
  read_parquet(full_data_path)
}, error = function(e) {
  cat("Error loading", full_data_path, ":", e$message, "\n")
  return(NULL)
})

if (is.null(full_data)) {
  stop("Halting script due to data loading error.")
}
cat("Data loaded successfully. Initial dimensions:", paste(dim(full_data), collapse="x"), "\n")

# --- 1a. Filter for Specific Housing Types ---
cat("Filtering for specific housing types (Residential and Condominium)...\n")
housing_property_types_to_keep <- c("Residential", "Condominium")
initial_rows_before_housing_filter <- nrow(full_data)
if ("Property_Type" %in% names(full_data)) {
    full_data <- full_data %>%
      filter(Property_Type %in% housing_property_types_to_keep)
    rows_removed_housing_filter <- initial_rows_before_housing_filter - nrow(full_data)
    cat(rows_removed_housing_filter, "rows removed by housing type filter.\n")
} else {
    cat("Warning: 'Property_Type' column not found. Skipping housing type filter.\n")
}
cat("Data dimensions after housing type filter:", paste(dim(full_data), collapse="x"), "\n")
if (nrow(full_data) == 0) stop("No data remaining after filtering for housing types.")

# --- 2. Initial Data Cleaning & Feature Engineering ---
cat("Starting initial data cleaning and feature engineering...\n")
numeric_cols_to_convert <- c(
  "Assessed_Value_Current_Year", "Taxable_Value_Current_Year", "Market_Value_Improvement_Current_Year",
  "Market_Value_Land_Current_Year", "Most_Recent_Sale_Price", "Total_Bedrooms", "Total_Bathrooms",
  "Total_Stories", "Total_Building_SqFt", "Total_Finished_SqFt", "Total_Living_Units",
  "Num_Buildings_Good_Condition", "Num_Residential_Buildings", "Num_Commercial_Buildings",
  "Num_Industrial_Buildings", "Num_Agricultural_Buildings", "Num_Government_Buildings",
  "Num_Utility_Buildings", "Num_Other_Buildings", "Year_Built_Mode", "Effective_Year_Built_Mode"
)
if ("Max_Land_Net_Acres" %in% names(full_data) && is.character(full_data$Max_Land_Net_Acres)) {
    full_data$Max_Land_Net_Acres <- as.numeric(full_data$Max_Land_Net_Acres)
}
for (col_name in numeric_cols_to_convert) {
  if (col_name %in% names(full_data)) {
    if (is.character(full_data[[col_name]])) full_data[[col_name]] <- as.numeric(full_data[[col_name]])
  } else {
    cat("Warning: Column", col_name, "not found for numeric conversion.\n")
  }
}
if ("Sale_Price" %in% names(full_data) && !is.numeric(full_data$Sale_Price)) {
  full_data$Sale_Price <- as.numeric(full_data$Sale_Price)
} else if (!("Sale_Price" %in% names(full_data))) {
    stop("Critical Error: 'Sale_Price' column not found.")
}

cat("Filtering non-positive Sale_Price and creating Sale_Price_log...\n")
initial_rows_before_log_filter <- nrow(full_data)
full_data <- full_data %>% filter(Sale_Price > 0)
if(nrow(full_data) < initial_rows_before_log_filter) cat(initial_rows_before_log_filter - nrow(full_data), "rows removed due to non-positive Sale_Price.\n")
full_data <- full_data %>% mutate(Sale_Price_log = log(Sale_Price))
cat("Sale_Price_log created. Data dimensions:", paste(dim(full_data), collapse="x"), "\n")

if ("Total_Building_SqFt" %in% names(full_data)) {
    cat("Filtering out properties with Total_Building_SqFt <= 1...\n")
    initial_rows_sqft_filter <- nrow(full_data)
    full_data <- full_data %>% filter(Total_Building_SqFt > 1)
    rows_removed_sqft_filter <- initial_rows_sqft_filter - nrow(full_data)
    cat(rows_removed_sqft_filter, "rows removed due to Total_Building_SqFt <= 1.\n")
    cat("Data dimensions after Total_Building_SqFt filter:", paste(dim(full_data), collapse="x"), "\n")
} else {
    cat("Warning: 'Total_Building_SqFt' column not found. Skipping SqFt filter.\n")
}
if (nrow(full_data) == 0) stop("No data remaining after pre-processing filters.")

if ("Most_Recent_Sale_Date" %in% names(full_data) && "Year_Built_Mode" %in% names(full_data)) {
  full_data <- full_data %>%
    mutate(
      Sale_Year = year(ymd(Most_Recent_Sale_Date)),
      Property_Age_at_Sale = Sale_Year - Year_Built_Mode
    )
  full_data$Property_Age_at_Sale[full_data$Property_Age_at_Sale < 0 | is.na(full_data$Property_Age_at_Sale)] <- NA
  cat("Property_Age_at_Sale feature engineered.\n")
} else {
  cat("Warning: Columns for Property_Age_at_Sale not found.\n")
}

# --- 3. Define Roles and Select Features ---
cat("Defining roles and selecting features...\n")
target_variable <- "Sale_Price_log"
id_variables <- c("Parcel_Number", "Most_Recent_Sale_Date", "Sale_Price", "Sale_Year")
redundant_variables <- c("Appraisal_Account_Type_Summary")
all_cols <- names(full_data)
potential_predictors <- setdiff(all_cols, c(target_variable, id_variables, redundant_variables))
character_cols <- names(full_data)[sapply(full_data[, potential_predictors, drop=FALSE], function(x) is.character(x) || is.factor(x))]
numeric_cols <- names(full_data)[sapply(full_data[, potential_predictors, drop=FALSE], is.numeric)]
selected_predictors <- unique(c(numeric_cols, character_cols))
selected_predictors <- setdiff(selected_predictors, c(target_variable, id_variables, redundant_variables))
cat("Selected predictors (" , length(selected_predictors), "):\n", paste(head(selected_predictors,15), collapse=", "), if(length(selected_predictors)>15) "...\n" else "\n")
modeling_data <- full_data[, c(selected_predictors, target_variable), drop = FALSE]
character_predictors_in_modeling_data <- intersect(selected_predictors, names(modeling_data)[sapply(modeling_data, is.character)])
for (col in character_predictors_in_modeling_data) modeling_data[[col]] <- as.factor(modeling_data[[col]])
cat("Character predictors converted to factors.\n")

# --- 4. Handle Missing Values (NA Imputation) ---
cat("Handling missing values...\n")
numeric_predictors_in_modeling_data <- intersect(selected_predictors, names(modeling_data)[sapply(modeling_data, is.numeric)])
factor_predictors_in_modeling_data <- intersect(selected_predictors, names(modeling_data)[sapply(modeling_data, is.factor)])
for (col in numeric_predictors_in_modeling_data) {
  if (any(is.na(modeling_data[[col]]))) {
    median_val <- median(modeling_data[[col]], na.rm = TRUE)
    if (is.na(median_val) || !is.finite(median_val)) median_val <- 0
    modeling_data[[col]][is.na(modeling_data[[col]])] <- median_val
  }
}
for (col in factor_predictors_in_modeling_data) {
  if (any(is.na(modeling_data[[col]]))) {
    current_levels <- levels(modeling_data[[col]])
    if (!("Missing_Val" %in% current_levels)) levels(modeling_data[[col]]) <- c(current_levels, "Missing_Val")
    modeling_data[[col]][is.na(modeling_data[[col]])] <- "Missing_Val"
  }
}
cat("Missing value imputation complete.\n")

# --- 5. Split Data into Training and Testing Sets ---
cat("Splitting data into training and testing sets...\n")
set.seed(123)
train_indices <- sample(1:nrow(modeling_data), 0.8 * nrow(modeling_data))
train_data <- modeling_data[train_indices, ]
test_data <- modeling_data[-train_indices, ]
cat("Training data dimensions:", paste(dim(train_data), collapse="x"), "\n")
cat("Testing data dimensions:", paste(dim(test_data), collapse="x"), "\n")
train_labels <- train_data[[target_variable]]
train_features_df <- train_data[, !(names(train_data) %in% target_variable), drop = FALSE]
test_labels <- test_data[[target_variable]]
test_features_df <- test_data[, !(names(test_data) %in% target_variable), drop = FALSE]
dtrain <- xgb.DMatrix(data = data.matrix(train_features_df), label = train_labels, missing = NA)
dtest <- xgb.DMatrix(data = data.matrix(test_features_df), label = test_labels, missing = NA)
cat("DMatrix objects created.\n")

# --- 5b. Hyperparameter Tuning using Parallel Random Search with xgb.cv ---
cat("\n--- Starting Parallel Hyperparameter Tuning ---\n")
param_grid <- list(
  eta = c(0.01, 0.02, 0.03, 0.05, 0.07, 0.1), # Expanded eta
  max_depth = c(4, 5, 6, 7, 8, 9, 10, 11, 12), # Expanded max_depth
  min_child_weight = c(1, 2, 3, 4, 5, 6), # Expanded min_child_weight
  subsample = c(0.6, 0.7, 0.8, 0.9, 1.0),
  colsample_bytree = c(0.5, 0.6, 0.7, 0.8, 0.9), # Expanded colsample
  gamma = c(0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3) # Expanded gamma
)

num_random_searches <- 100 # Increase for a more thorough search with your hardware
nrounds_cv <- 1500
early_stopping_rounds_cv <- 30
n_cores_to_use <- max(1, detectCores() - 2) # Leave 2 cores for OS/other tasks, or use 14-15 directly
cat(paste("Setting up parallel backend with", n_cores_to_use, "cores.\n"))
cl <- makeCluster(n_cores_to_use)
registerDoParallel(cl)
cat("Parallel backend registered.\n")

# Ensure XGBoost is available on worker nodes
clusterEvalQ(cl, {
  library(xgboost)
})

tuning_results_list <- foreach(
  i = 1:num_random_searches,
  .combine = 'rbind', # Combine results into a data frame
  .packages = c("xgboost", "dplyr"), # Packages needed by each worker
  .errorhandling = 'pass' # Continue if one iteration fails, 'remove' would discard it
) %dopar% {
  
  # Set seed for reproducibility within each parallel task if needed,
  # though random sampling of params provides variability.
  # set.seed(123 + i) # Optional: if you want identical param sampling across runs of the whole script
  
  current_cv_params <- list(
    objective = "reg:squarederror",
    eval_metric = "rmse",
    eta = sample(param_grid$eta, 1),
    max_depth = sample(param_grid$max_depth, 1),
    min_child_weight = sample(param_grid$min_child_weight, 1),
    subsample = sample(param_grid$subsample, 1),
    colsample_bytree = sample(param_grid$colsample_bytree, 1),
    gamma = sample(param_grid$gamma, 1),
    nthread = 2 # CRITICAL: Use 1 or 2 threads for xgb.cv when outer loop is parallel
    # seed = 123 + i # XGBoost's internal seed for operations like subsampling
  )
  
  # Create a temporary dtrain for this specific worker if dtrain is large and
  # passing it repeatedly is an issue. For most cases, passing dtrain is fine.
  # If dtrain is very large, consider xgb.DMatrix.saveRDS and load in worker.
  
  cv_model_tuned <- xgb.cv(
    params = current_cv_params,
    data = dtrain, # dtrain should be accessible here
    nrounds = nrounds_cv,
    nfold = 5,
    early_stopping_rounds = early_stopping_rounds_cv,
    verbose = 0,
    print_every_n = 200 # Only relevant if verbose > 0
  )
  
  best_rmse_cv <- NA
  best_iter_cv <- NA
  if (length(cv_model_tuned$evaluation_log$test_rmse_mean) > 0) {
    best_rmse_cv <- min(cv_model_tuned$evaluation_log$test_rmse_mean, na.rm = TRUE)
    best_iter_cv <- which.min(cv_model_tuned$evaluation_log$test_rmse_mean)
  }
  
  # Return a data frame row
  data.frame(
    iteration = i,
    eta = current_cv_params$eta,
    max_depth = current_cv_params$max_depth,
    min_child_weight = current_cv_params$min_child_weight,
    subsample = current_cv_params$subsample,
    colsample_bytree = current_cv_params$colsample_bytree,
    gamma = current_cv_params$gamma,
    best_cv_rmse = best_rmse_cv,
    best_cv_iter = best_iter_cv,
    stringsAsFactors = FALSE
  )
}

# Stop the parallel cluster
stopCluster(cl)
registerDoSEQ() # Unregister parallel backend
cat("\n--- Parallel Hyperparameter Tuning Complete ---\n")

# Process results
tuning_results_df <- as.data.frame(tuning_results_list)
tuning_results_df <- tuning_results_df[!is.na(tuning_results_df$best_cv_rmse), ]

if (nrow(tuning_results_df) > 0) {
  print("Top 10 tuning results (by CV RMSE):")
  print(head(tuning_results_df[order(tuning_results_df$best_cv_rmse), ], 10))
  
  best_params_row_tuned <- tuning_results_df[which.min(tuning_results_df$best_cv_rmse), ]
  cat("\nBest hyperparameters found from tuning:\n")
  print(best_params_row_tuned)
  
  best_xgb_params_tuned <- list(
    objective = "reg:squarederror", eval_metric = "rmse",
    eta = best_params_row_tuned$eta,
    max_depth = best_params_row_tuned$max_depth,
    min_child_weight = best_params_row_tuned$min_child_weight,
    subsample = best_params_row_tuned$subsample,
    colsample_bytree = best_params_row_tuned$colsample_bytree,
    gamma = best_params_row_tuned$gamma,
    seed = 456,
    nthread = max(1, detectCores() - 2) # Use more threads for final model
  )
  optimal_nrounds_tuned <- best_params_row_tuned$best_cv_iter
  cat("\nOptimal parameters for final model training:\n")
  print(best_xgb_params_tuned)
  cat(paste("Optimal nrounds from CV (for these params):", optimal_nrounds_tuned, "\n"))
} else {
  cat("No successful tuning results. Using default parameters for final model.\n")
  best_xgb_params_tuned <- list(
    booster = "gbtree", objective = "reg:squarederror", eta = 0.05, max_depth = 7,
    min_child_weight = 1, subsample = 0.7, colsample_bytree = 0.7, gamma = 0.1, seed = 456,
    nthread = max(1, detectCores() - 2)
  )
  optimal_nrounds_tuned <- 500
}

# --- 6. Train Final XGBoost Model with Tuned Parameters ---
cat("\n--- Training Final XGBoost Model with Tuned (or Default) Parameters ---\n")
final_training_nrounds <- 2500 # Max rounds for final training
final_early_stopping_rounds <- 50
watchlist <- list(train = dtrain, test = dtest)

xgb_model <- xgb.train(
  params = best_xgb_params_tuned,
  data = dtrain,
  nrounds = final_training_nrounds,
  watchlist = watchlist,
  early_stopping_rounds = final_early_stopping_rounds,
  verbose = 1, print_every_n = 50
)
cat("Final XGBoost model training complete.\n")
best_iter_val <- xgb_model$best_iteration
cat("Best iteration from final training:", best_iter_val, "\n")
if (!is.null(xgb_model$evaluation_log) && "test_rmse" %in% names(xgb_model$evaluation_log)) {
    cat("Best RMSE on test set (log scale) during final training:", xgb_model$evaluation_log$test_rmse[best_iter_val], "\n")
}

# --- 7. Make Predictions ---
cat("Making predictions on the test set using the final tuned model...\n")
predictions_log <- predict(xgb_model, dtest, iterationrange = c(1, best_iter_val))
predictions_original_scale <- exp(predictions_log)
test_labels_original_scale <- exp(test_labels)

# --- 8. Evaluate Model ---
cat("Evaluating final tuned model performance...\n")
valid_preds <- is.finite(predictions_original_scale) & is.finite(test_labels_original_scale)
predictions_original_scale_clean <- predictions_original_scale[valid_preds]
test_labels_original_scale_clean <- test_labels_original_scale[valid_preds]
if(length(predictions_original_scale_clean) == 0) {
  cat("Error: No valid predictions to evaluate after cleaning NAs/Infs.\n")
} else {
  mae_value <- mae(actual = test_labels_original_scale_clean, predicted = predictions_original_scale_clean)
  rmse_value <- rmse(actual = test_labels_original_scale_clean, predicted = predictions_original_scale_clean)
  rss <- sum((predictions_original_scale_clean - test_labels_original_scale_clean)^2)
  tss <- sum((test_labels_original_scale_clean - mean(test_labels_original_scale_clean))^2)
  r_squared <- if (tss == 0) NA else (1 - (rss / tss))
  non_zero_actuals_mask <- test_labels_original_scale_clean != 0
  mape_value <- if(sum(non_zero_actuals_mask) > 0) {
    mean(abs((test_labels_original_scale_clean[non_zero_actuals_mask] - predictions_original_scale_clean[non_zero_actuals_mask]) / test_labels_original_scale_clean[non_zero_actuals_mask])) * 100
  } else { NA }
  cat("Final Tuned Model Performance (Original Scale) for HOUSING:\n")
  cat("MAE: $", format(round(mae_value, 2), nsmall = 2, big.mark = ","), "\n")
  cat("RMSE: $", format(round(rmse_value, 2), nsmall = 2, big.mark = ","), "\n")
  cat("R-squared:", round(r_squared, 4), "\n")
  if (!is.na(mape_value)) cat("MAPE:", round(mape_value, 2), "%\n") else cat("MAPE: Not calculable\n")
}

# --- 9. Feature Importance ---
cat("Calculating feature importance for the final tuned model...\n")
importance_matrix <- xgb.importance(model = xgb_model)
print(head(importance_matrix, 20))
if (nrow(importance_matrix) > 0) {
  xgb.plot.importance(importance_matrix = importance_matrix, top_n = 20)
  title("XGBoost Feature Importance (Top 20) - Final Tuned Housing Model")
} else { cat("No feature importance data to plot.\n") }

# --- 10. Residual Analysis ---
cat("Performing residual analysis for the final tuned model...\n")
residuals_log_scale <- test_labels - predictions_log
residuals_original_scale <- test_labels_original_scale_clean - predictions_original_scale_clean
if (length(predictions_log) == length(residuals_log_scale) && length(predictions_log) > 0) {
  residual_plot_data_log <- data.frame(Predicted = predictions_log, Residuals = residuals_log_scale)
  p_res_log <- ggplot(residual_plot_data_log, aes(x = Predicted, y = Residuals)) + geom_point(alpha = 0.5) + geom_hline(yintercept = 0, linetype = "dashed", color = "red") + labs(title = "Residuals vs. Predicted (Log Scale) - Final Tuned Housing Model", x = "Predicted Sale_Price_log", y = "Residuals (log scale)") + theme_minimal()
  print(p_res_log)
}
if (length(predictions_original_scale_clean) == length(residuals_original_scale) && length(predictions_original_scale_clean) > 0) {
  residual_plot_data_orig <- data.frame(Predicted = predictions_original_scale_clean, Residuals = residuals_original_scale)
  p_res_orig <- ggplot(residual_plot_data_orig, aes(x = Predicted, y = Residuals)) + geom_point(alpha = 0.5) + geom_hline(yintercept = 0, linetype = "dashed", color = "red") + scale_x_continuous(labels = scales::comma) + scale_y_continuous(labels = scales::comma) + labs(title = "Residuals vs. Predicted (Original Scale) - Final Tuned Housing Model", x = "Predicted Sale_Price (Original Scale)", y = "Residuals (Original Scale)") + theme_minimal()
  print(p_res_orig)
}

# --- 11. Actual vs. Predicted Plot ---
cat("Generating Actual vs. Predicted plot for the final tuned model...\n")
if (length(test_labels_original_scale_clean) == length(predictions_original_scale_clean) && length(test_labels_original_scale_clean) > 0) {
  actual_vs_predicted_data <- data.frame(Actual = test_labels_original_scale_clean, Predicted = predictions_original_scale_clean)
  p_avp <- ggplot(actual_vs_predicted_data, aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.3) + geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") + scale_x_continuous(labels = scales::comma) + scale_y_continuous(labels = scales::comma) + labs(title = "Actual vs. Predicted Sale Price (Original Scale) - Final Tuned Housing Model", x = "Actual Sale Price", y = "Predicted Sale Price") + coord_fixed(ratio = 1, xlim = range(c(actual_vs_predicted_data$Actual, actual_vs_predicted_data$Predicted), na.rm = TRUE), ylim = range(c(actual_vs_predicted_data$Actual, actual_vs_predicted_data$Predicted), na.rm = TRUE)) + theme_minimal()
  print(p_avp)
}

# --- 12. Partial Dependence Plots (Example for top features) ---
if (nrow(importance_matrix) > 0 && requireNamespace("pdp", quietly = TRUE)) {
  cat("Generating Partial Dependence Plots (PDP) for the final tuned model...\n")
  top_n_features_for_pdp <- min(5, nrow(importance_matrix))
  features_for_pdp <- importance_matrix$Feature[1:top_n_features_for_pdp]
  features_for_pdp <- intersect(features_for_pdp, names(train_features_df))
  if(length(features_for_pdp) > 0) {
    for(feature_name in features_for_pdp) {
      cat("Generating PDP for feature:", feature_name, "\n")
      tryCatch({
        pdp_plot_obj <- pdp::partial(object = xgb_model, pred.var = feature_name, train = train_features_df, pred.fun = function(object, newdata) predict(object, data.matrix(newdata), iterationrange = c(1, object$best_iteration)), plot = TRUE, rug = TRUE, plot.engine = "ggplot2")
        print(pdp_plot_obj + labs(title = paste("PDP for", feature_name, "- Final Tuned Housing Model")))
      }, error = function(e) { cat("Could not generate PDP for", feature_name, ":", e$message, "\n") })
    }
  } else { cat("No valid features found for PDP.\n") }
} else { cat("Skipping PDPs: pdp package not available or no feature importance.\n") }

cat("Script finished.\n")
```


```{r}
# --- 0. Load Libraries and Setup ---
cat("Loading libraries...\n")
# Ensure all necessary packages are installed
# install.packages(c("arrow", "dplyr", "lubridate", "ggplot2", "fastDummies", "xgboost", "Metrics", "pdp", "doParallel", "scales", "caret"))

library(arrow)
library(dplyr)
library(lubridate)
library(ggplot2)
library(fastDummies)
library(xgboost)
library(Metrics) # For MAE, RMSE
library(pdp)     # For Partial Dependence Plots
library(doParallel) # For parallel processing
library(scales)     # For plot axis formatting
library(caret)      # For R2 function and other utilities

# Set a seed for reproducibility
set.seed(123)

# --- 1. Load Data ---
cat("Loading data...\n")
# !!! USER ACTION REQUIRED: Update this path to your data file !!!
full_data_path <- "../data/stage/full_data_for_modeling.parquet" 

full_data <- tryCatch({
  read_parquet(full_data_path)
}, error = function(e) {
  cat("Error loading", full_data_path, ":", e$message, "\n")
  cat("Please ensure the path is correct and the file exists.\n")
  return(NULL)
})

if (is.null(full_data) || nrow(full_data) == 0) {
  stop("Halting script: Data loading failed or data is empty.")
}
cat("Data loaded. Initial dimensions:", paste(dim(full_data), collapse="x"), "\n")

# --- 2. Initial Filtering and Data Cleaning ---
cat("Starting initial filtering and data cleaning...\n")

# 2a. Filter for Specific Housing Types
housing_property_types_to_keep <- c("Residential", "Condominium")
if ("Property_Type" %in% names(full_data)) {
  full_data <- full_data %>%
    filter(Property_Type %in% housing_property_types_to_keep)
  cat("Data dimensions after housing type filter:", paste(dim(full_data), collapse="x"), "\n")
} else {
  cat("Warning: 'Property_Type' column not found. Skipping housing type filter.\n")
}
if (nrow(full_data) == 0) stop("No data remaining after filtering for housing types.")

# 2b. Sale_Date Cleaning and Feature Engineering
date_col_name <- "Sale_Date" 
if (date_col_name %in% names(full_data)) {
  cat(paste("Processing '", date_col_name, "'...\n", sep=""))
  if (is.character(full_data[[date_col_name]])) {
    full_data[[date_col_name]] <- ymd(full_data[[date_col_name]], quiet = TRUE)
  } else if (!inherits(full_data[[date_col_name]], "Date") && !inherits(full_data[[date_col_name]], "POSIXct")) {
    # Attempt conversion if not already a Date or POSIXct type
     full_data[[date_col_name]] <- as.Date(full_data[[date_col_name]])
     cat("Converted '", date_col_name, "' to Date type.\n")
  }
  
  current_date <- Sys.Date() 
  initial_rows_date_filter <- nrow(full_data)
  full_data <- full_data %>% filter(get(date_col_name) <= current_date)
  rows_removed_date_filter <- initial_rows_date_filter - nrow(full_data)
  cat(paste("Removed", rows_removed_date_filter, "rows with future Sale_Date (after", format(current_date), ").\n"))
  cat("Data dimensions after future Sale_Date filter:", paste(dim(full_data), collapse="x"), "\n")
  if (nrow(full_data) == 0) stop("No data remaining after filtering future sale dates.")

  full_data <- full_data %>%
    mutate(
      Sale_Year = as.numeric(year(get(date_col_name))), 
      Sale_Month = factor(month(get(date_col_name), label = TRUE, abbr = FALSE))
    )
  
  if ("Year_Built" %in% names(full_data) && "Sale_Year" %in% names(full_data)) {
      year_built_median_impute <- median(full_data$Year_Built[full_data$Year_Built > 1800 & full_data$Year_Built <= full_data$Sale_Year & !is.na(full_data$Year_Built)], na.rm = TRUE)
      if(is.na(year_built_median_impute)) year_built_median_impute <- 2000 # Fallback if all are NA or invalid

      full_data$Year_Built_Mode <- ifelse(is.na(full_data$Year_Built) | full_data$Year_Built <= 1800 | full_data$Year_Built > full_data$Sale_Year, 
                                          year_built_median_impute, 
                                          full_data$Year_Built)
      full_data <- full_data %>%
        mutate(Property_Age_at_Sale = Sale_Year - Year_Built_Mode) %>%
        select(-Year_Built_Mode) 
      cat("Created 'Property_Age_at_Sale'.\n")
  } else {
      cat("Warning: 'Year_Built' or 'Sale_Year' not available for 'Property_Age_at_Sale' calculation.\n")
  }

} else {
  cat(paste("Warning: Primary date column '", date_col_name, "' not found. Time-based features will be limited.\n", sep=""))
}


# 2c. Sale_Price Outlier Treatment
if (!"Sale_Price" %in% names(full_data)) {
    stop("Critical Error: 'Sale_Price' column not found for outlier treatment.")
}
if (!is.numeric(full_data$Sale_Price)) {
  full_data$Sale_Price <- as.numeric(as.character(full_data$Sale_Price)) # Ensure conversion
}
full_data <- full_data %>% filter(!is.na(Sale_Price)) 

price_floor_threshold <- 10000 
initial_rows_price_filter <- nrow(full_data)
full_data <- full_data %>% filter(Sale_Price >= price_floor_threshold)
rows_removed_price_floor <- initial_rows_price_filter - nrow(full_data)
cat(paste("Removed", rows_removed_price_floor, "rows with Sale_Price < $", format(price_floor_threshold, big.mark=","), ".\n"))
cat("Data dimensions after low Sale_Price filter:", paste(dim(full_data), collapse="x"), "\n")
if (nrow(full_data) == 0) stop("No data remaining after filtering low sale prices.")

price_cap_threshold <- quantile(full_data$Sale_Price, 0.995, na.rm = TRUE)
cat(paste("Capping Sale_Price at 99.5th percentile: $", format(price_cap_threshold, big.mark=","), ".\n"))
num_capped <- sum(full_data$Sale_Price > price_cap_threshold, na.rm = TRUE)
full_data <- full_data %>%
  mutate(Sale_Price = ifelse(Sale_Price > price_cap_threshold, price_cap_threshold, Sale_Price))
cat(paste(num_capped, "Sale_Price values were capped.\n"))

# 2d. Create Target Variable (Log Transformed)
full_data <- full_data %>%
  mutate(Sale_Price_log = log(Sale_Price))
cat("Created 'Sale_Price_log' target variable.\n")

# --- 3. Define Roles and Select Features ---
cat("Defining roles and selecting features...\n")
target_variable <- "Sale_Price_log"

# MODIFICATION: Added "ETN" to id_variables to exclude it from predictors
id_variables <- c("Parcel_Number", "Most_Recent_Sale_Date", "Sale_Price", 
                  "Sale_Year", # Sale_Year is used to create age, but might be too direct as a predictor if Sale_Date (numeric) is also used. Consider.
                  "ParcelID", "TaxAccountNbr", "ETN") # ETN added here

# Original Sale_Date (if Date/POSIXct) will be handled as numeric by XGBoost.
# If you want to explicitly exclude the original Sale_Date object and only use Sale_Year, Sale_Month, add "Sale_Date" to id_variables.
# For now, assuming Sale_Date (as numeric epoch) is an acceptable predictor.

redundant_variables <- c(
  "Appraisal_Account_Type_Summary" 
  # Add other known redundant columns
)

all_cols <- names(full_data)
potential_predictors <- setdiff(all_cols, c(target_variable, id_variables, redundant_variables))

# Identify character/factor and numeric columns among potential predictors
# Ensure that the columns selected for type checking actually exist in full_data
valid_potential_predictors <- intersect(potential_predictors, names(full_data))

character_cols_pot <- names(full_data)[sapply(full_data[, valid_potential_predictors, drop = FALSE], function(x) is.character(x) || is.factor(x))]
numeric_cols_pot <- names(full_data)[sapply(full_data[, valid_potential_predictors, drop = FALSE], is.numeric)]
posixct_cols_pot <- names(full_data)[sapply(full_data[, valid_potential_predictors, drop = FALSE], inherits, "POSIXct")]
date_cols_pot <- names(full_data)[sapply(full_data[, valid_potential_predictors, drop = FALSE], inherits, "Date")] # XGBoost handles Date as numeric

numeric_cols_pot <- unique(c(numeric_cols_pot, posixct_cols_pot, date_cols_pot))

selected_predictors <- intersect(valid_potential_predictors, unique(c(numeric_cols_pot, character_cols_pot)))

cat("Final selected predictors (", length(selected_predictors), "):\n", paste(head(selected_predictors, 20), collapse=", "), if(length(selected_predictors)>20) "...\n" else "\n")

modeling_data <- full_data[, c(selected_predictors, target_variable), drop = FALSE]
if (nrow(modeling_data) == 0) stop("No data in modeling_data after feature selection.")

# --- 4. Split Data into Training and Testing Sets ---
cat("Splitting data into training (80%) and testing (20%) sets...\n")
train_index <- sample(seq_len(nrow(modeling_data)), size = floor(0.80 * nrow(modeling_data)))
train_data <- modeling_data[train_index, ]
test_data <- modeling_data[-train_index, ]
cat("Training set dimensions:", paste(dim(train_data), collapse="x"), "\n")
cat("Testing set dimensions:", paste(dim(test_data), collapse="x"), "\n")

if (nrow(train_data) < 10 || nrow(test_data) < 10) {
    stop("Insufficient data in training or testing set after split.")
}

# --- 5. Preprocessing ---
cat("Starting preprocessing...\n")

imputation_values <- list()
processed_train_data <- train_data
processed_test_data <- test_data

for (col in selected_predictors) {
  if (col %in% names(train_data)) { # Ensure column exists
    if (is.numeric(train_data[[col]]) || inherits(train_data[[col]], "POSIXct") || inherits(train_data[[col]], "Date")) {
      median_val <- median(train_data[[col]], na.rm = TRUE)
      imputation_values[[col]] <- ifelse(is.na(median_val) || !is.finite(median_val), 0, median_val) 
      
      processed_train_data[[col]][is.na(processed_train_data[[col]])] <- imputation_values[[col]]
      if (col %in% names(processed_test_data)) {
        processed_test_data[[col]][is.na(processed_test_data[[col]])] <- imputation_values[[col]]
      }
    } else if (is.character(train_data[[col]]) || is.factor(train_data[[col]])) {
      if (is.character(processed_train_data[[col]])) {
        processed_train_data[[col]] <- factor(processed_train_data[[col]])
      }
      if (col %in% names(processed_test_data) && is.character(processed_test_data[[col]])) {
        # Ensure levels are consistent with training before converting test to factor
        train_unique_vals <- unique(na.omit(processed_train_data[[col]]))
        processed_test_data[[col]] <- factor(processed_test_data[[col]], levels = train_unique_vals)
      }
      
      train_levels <- levels(processed_train_data[[col]])
      if (!("Missing_Val" %in% train_levels)) {
        levels(processed_train_data[[col]]) <- c(train_levels, "Missing_Val")
      }
      processed_train_data[[col]][is.na(processed_train_data[[col]])] <- "Missing_Val"
      imputation_values[[col]] <- "Missing_Val" 
      
      if (col %in% names(processed_test_data)) {
        all_train_levels <- levels(processed_train_data[[col]]) 
        if (is.factor(processed_test_data[[col]])) {
            current_test_levels <- levels(processed_test_data[[col]])
            missing_levels_in_test <- setdiff(all_train_levels, current_test_levels)
            if (length(missing_levels_in_test) > 0) {
                levels(processed_test_data[[col]]) <- c(current_test_levels, missing_levels_in_test)
            }
        } else { # If it became all NAs and wasn't converted to factor with levels yet
            processed_test_data[[col]] <- factor(processed_test_data[[col]], levels = all_train_levels)
        }
        processed_test_data[[col]][is.na(processed_test_data[[col]])] <- "Missing_Val"
      }
    }
  }
}
cat("Missing value imputation complete.\n")

# 5b. One-Hot Encode Categorical Features
potential_predictor_cols_in_processed_train <- setdiff(names(processed_train_data), target_variable)
factor_cols_to_encode <- names(processed_train_data[, intersect(potential_predictor_cols_in_processed_train, colnames(processed_train_data)), drop=FALSE])[
                            sapply(processed_train_data[, intersect(potential_predictor_cols_in_processed_train, colnames(processed_train_data)), drop=FALSE], is.factor)
                         ]

# ETN is already removed from selected_predictors. Grantor/Grantee will be excluded if present.
cols_to_exclude_from_ohe_due_to_cardinality <- c("Grantor", "Grantee") 
cat("Excluding high-cardinality columns from one-hot encoding (if present and factors):", paste(cols_to_exclude_from_ohe_due_to_cardinality, collapse=", "), "\n")
factor_cols_to_encode <- setdiff(factor_cols_to_encode, cols_to_exclude_from_ohe_due_to_cardinality)

if (length(factor_cols_to_encode) > 0) {
  cat("One-hot encoding categorical features:", paste(factor_cols_to_encode, collapse=", "), "\n")
  
  train_data_encoded <- dummy_cols(processed_train_data, 
                                   select_columns = factor_cols_to_encode,
                                   remove_first_dummy = FALSE, 
                                   remove_selected_columns = TRUE) 
  
  test_data_encoded <- dummy_cols(processed_test_data, 
                                  select_columns = factor_cols_to_encode,
                                  remove_first_dummy = FALSE,
                                  remove_selected_columns = TRUE)
  
  dummified_feature_names_from_train <- setdiff(names(train_data_encoded), target_variable)

  missing_cols_in_test <- setdiff(dummified_feature_names_from_train, names(test_data_encoded))
  for (col_add in missing_cols_in_test) {
    test_data_encoded[[col_add]] <- 0
  }
  
  extra_cols_in_test <- setdiff(names(test_data_encoded), names(train_data_encoded)) 
  if (length(extra_cols_in_test) > 0) {
    test_data_encoded <- test_data_encoded[, !(names(test_data_encoded) %in% extra_cols_in_test), drop = FALSE]
  }
  
  test_data_encoded <- test_data_encoded[, names(train_data_encoded), drop = FALSE]

  final_train_features_df <- train_data_encoded %>% select(-all_of(target_variable))
  final_test_features_df <- test_data_encoded %>% select(-all_of(target_variable))
  
  train_labels <- train_data_encoded[[target_variable]]
  test_labels <- test_data_encoded[[target_variable]]
  
} else {
  cat("No categorical features to one-hot encode.\n")
  final_train_features_df <- processed_train_data %>% select(-all_of(target_variable))
  final_test_features_df <- processed_test_data %>% select(-all_of(target_variable))
  
  train_labels <- processed_train_data[[target_variable]]
  test_labels <- processed_test_data[[target_variable]]
}
cat("One-hot encoding complete. Number of features after encoding:", ncol(final_train_features_df), "\n")

# --- 6. Prepare Data for XGBoost ---
cat("Preparing data for XGBoost (creating DMatrices)...\n")
train_features_matrix <- data.matrix(final_train_features_df)
test_features_matrix <- data.matrix(final_test_features_df)

dtrain <- xgb.DMatrix(data = train_features_matrix, label = train_labels, missing = NA) # NAs should be handled, but good practice
dtest <- xgb.DMatrix(data = test_features_matrix, label = test_labels, missing = NA)
watchlist <- list(train = dtrain, test = dtest)

# --- 7. Hyperparameter Tuning with Random Search ---
cat("Starting hyperparameter tuning with random search...\n")

param_grid <- expand.grid(
  eta = c(0.01, 0.03, 0.05, 0.1),
  max_depth = c(5, 7, 9, 11),
  min_child_weight = c(1, 3, 5),
  subsample = c(0.7, 0.8, 0.9),
  colsample_bytree = c(0.7, 0.8, 0.9),
  gamma = c(0, 0.1, 0.2)
)

num_random_searches <- 20 
nfold_cv <- 3
nrounds_cv <- 200 # Max rounds for each CV fold
early_stopping_rounds_cv <- 20

tuning_results_file <- "tuning_results_checkpoint.rds" 
tuning_results <- data.frame()
start_iteration <- 1

if (file.exists(tuning_results_file)) {
  cat("Found existing tuning results. Loading...\n")
  tuning_results_loaded <- readRDS(tuning_results_file)
  if (is.data.frame(tuning_results_loaded) && nrow(tuning_results_loaded) > 0 && "iteration" %in% names(tuning_results_loaded)) {
    tuning_results <- tuning_results_loaded
    if(is.factor(tuning_results$iteration)) tuning_results$iteration <- as.numeric(as.character(tuning_results$iteration))
    
    last_completed_iteration <- max(tuning_results$iteration, na.rm = TRUE)
    if (is.finite(last_completed_iteration)) {
        start_iteration <- last_completed_iteration + 1
    }
    cat(paste("Resuming from iteration", start_iteration, "\n"))
  } else {
    cat("Checkpoint file was empty or malformed. Starting fresh.\n")
  }
} else {
  cat("No existing tuning results found. Starting fresh.\n")
}

num_cores <- detectCores() - 1
if (num_cores < 1) num_cores <- 1
cl <- makeCluster(num_cores)
registerDoParallel(cl)
cat(paste("Using", num_cores, "cores for parallel tuning.\n"))

if (start_iteration <= num_random_searches) {
  for (i in start_iteration:num_random_searches) { 
    cat(paste("Tuning iteration", i, "of", num_random_searches, "...\n"))
    
    current_params_list <- as.list(param_grid[sample(nrow(param_grid), 1), ])
    current_params_list$objective <- "reg:squarederror"
    current_params_list$eval_metric <- "rmse"
    # xgb_cv_params$nthread <- max(1, detectCores() - 2) # nthread for xgb.cv is usually handled by parallel backend if xgb itself is parallelized, or set per core.
                                                       # For xgb.train, nthread is useful. For xgb.cv, it's often less critical if outer loop is parallel.
                                                       # Let's keep it simple and rely on doParallel for now.

    cv_model <- tryCatch({
      xgb.cv(
        params = current_params_list,
        data = dtrain,
        nrounds = nrounds_cv,
        nfold = nfold_cv,
        showsd = TRUE,
        stratified = FALSE, # Not for regression
        verbose = 0, # Suppress per-fold output
        early_stopping_rounds = early_stopping_rounds_cv,
        prediction = FALSE
      )
    }, error = function(e) {
      cat("Error in xgb.cv for params:", paste(names(current_params_list), current_params_list, collapse=", "), "\nError:", e$message, "\n")
      return(NULL)
    })
    
    if (!is.null(cv_model) && !is.null(cv_model$evaluation_log) && nrow(cv_model$evaluation_log) > 0) {
      best_rmse_cv <- min(cv_model$evaluation_log$test_rmse_mean, na.rm = TRUE)
      best_iter_cv <- cv_model$best_iteration 
      
      params_for_df <- lapply(current_params_list, function(x) if(length(x)>1) paste(x, collapse=";") else x)
      result_row <- data.frame(c(params_for_df, list(best_rmse = best_rmse_cv, best_iter = best_iter_cv, iteration = i)))
      
      if (nrow(tuning_results) == 0) {
          tuning_results <- result_row
      } else {
          # Ensure column names match before rbind, especially if schema changed
          common_cols <- intersect(names(tuning_results), names(result_row))
          tuning_results <- rbind(tuning_results[, common_cols, drop=FALSE], result_row[, common_cols, drop=FALSE])
          # A more robust merge might be needed if columns are expected to differ significantly over time.
          # For this script, direct rbind after ensuring initial creation or simple rbind is usually fine.
          # A simpler approach if columns are guaranteed to be the same after first iteration:
          # tuning_results <- rbind(tuning_results, result_row)
      }

      cat(paste("Iter", i, "- Best CV RMSE:", round(best_rmse_cv, 6), "at round", best_iter_cv, "\n"))
      # print(current_params_list) # Can be verbose
      
      saveRDS(tuning_results, tuning_results_file)
      cat(paste("Saved progress to", tuning_results_file, "\n"))
      
    } else {
      cat(paste("Iter", i, "- CV failed or no evaluation log.\n"))
    }
  }
} else {
  cat("All tuning iterations already completed based on checkpoint file.\n")
}

stopCluster(cl)
registerDoSEQ() # Unregister parallel backend

best_xgb_params <- list() # Initialize
best_nrounds_final <- 100 # Default

if (exists("tuning_results") && is.data.frame(tuning_results) && nrow(tuning_results) > 0 && "best_rmse" %in% names(tuning_results)) {
    if(is.factor(tuning_results$best_rmse)) tuning_results$best_rmse <- as.numeric(as.character(tuning_results$best_rmse))
    if(all(is.na(tuning_results$best_rmse))) {
        cat("All best_rmse values in tuning_results are NA. Using default parameters.\n")
    } else {
        best_params_row <- tuning_results[which.min(tuning_results$best_rmse), ]
        cat("\n--- Best Hyperparameters Found (from CV) ---\n")
        print(best_params_row)
        
        best_xgb_params <- as.list(best_params_row)
        # Clean up columns specific to the tuning results df
        best_xgb_params$best_rmse <- NULL
        best_nrounds_final <- best_params_row$best_iter 
        best_xgb_params$best_iter <- NULL 
        best_xgb_params$iteration <- NULL
        
        best_xgb_params$objective <- "reg:squarederror" # Ensure these are set
        best_xgb_params$eval_metric <- "rmse"
        # best_xgb_params$nthread <- max(1, detectCores() - 2) # Set nthread for final training
    }
} else {
    cat("Tuning did not yield results or no new iterations to run. Using default parameters for final model.\n")
}

# Fallback if best_xgb_params is still empty or essential params missing
if(length(best_xgb_params) == 0 || is.null(best_xgb_params$objective)) {
    cat("Using default parameters for final model training.\n")
    best_xgb_params <- list(
        objective = "reg:squarederror", eval_metric = "rmse",
        eta = 0.05, max_depth = 7, subsample = 0.8, colsample_bytree = 0.8
        # nthread = max(1, detectCores() - 2)
    )
    best_nrounds_final <- 100 # Default nrounds if tuning fails
}
if (is.null(best_nrounds_final) || best_nrounds_final < 10) best_nrounds_final <- 100 # Fallback


# --- 8. Train Final Model with Best Hyperparameters ---
cat("\nTraining final XGBoost model with best hyperparameters...\n")
cat("Using nrounds:", best_nrounds_final, "\n")
# print(best_xgb_params)

xgb_model <- xgb.train(
  params = best_xgb_params,
  data = dtrain,
  nrounds = best_nrounds_final,
  watchlist = watchlist,
  verbose = 10, # Print evaluation metrics every 10 rounds
  early_stopping_rounds = early_stopping_rounds_cv + 10 # Allow a bit more for final training, or match CV
)

cat("\n--- Final Model Performance (Log Scale) from Training ---\n")
# Use the model's best iteration if early stopping occurred
final_model_best_iter <- if (!is.null(xgb_model$best_iteration)) xgb_model$best_iteration else best_nrounds_final
print(xgb_model$evaluation_log[final_model_best_iter, ])

# --- 9. Evaluate Model on Test Set (Original Scale) ---
cat("\nEvaluating model on test set (original scale)...\n")
predictions_log <- predict(xgb_model, dtest, iterationrange = c(1, final_model_best_iter))
predictions_original <- exp(predictions_log)
actual_original <- exp(test_labels) 

# Calculate metrics
mae_val <- mae(actual_original, predictions_original)
rmse_val <- Metrics::rmse(actual_original, predictions_original) # Explicitly use Metrics::
rsq_val <- R2(pred = predictions_original, obs = actual_original, form = "traditional") # caret's R2

cat("\n--- Model Performance Metrics on Test Set (Original Scale) ---\n")
print(paste("MAE:", round(mae_val, 2)))
print(paste("RMSE:", round(rmse_val, 2)))
print(paste("R-squared:", round(rsq_val, 4)))

# --- 10. Feature Importance ---
cat("\nGenerating feature importance...\n")
importance_matrix <- NULL
tryCatch({
  importance_matrix <- xgb.importance(feature_names = colnames(train_features_matrix), model = xgb_model)
  cat("\n--- Top 20 Features ---\n")
  print(head(importance_matrix, 20))
  
  # Plotting (optional, can be slow for many features)
  # xgb.plot.importance(importance_matrix = importance_matrix, top_n = 20, main = "Top 20 Feature Importance (Final Model)")
  # To save:
  # png("feature_importance_final.png", width=800, height=600)
  # xgb.plot.importance(importance_matrix = importance_matrix, top_n = 20)
  # dev.off()
  
}, error = function(e) {
  cat("Error generating feature importance:", e$message, "\n")
})

# --- 11. Partial Dependence Plots (PDP) ---
cat("\nGenerating Partial Dependence Plots for top features (if available)...\n")
if (!is.null(importance_matrix) && nrow(importance_matrix) > 0) {
  top_features_for_pdp <- head(importance_matrix$Feature, min(5, nrow(importance_matrix)))
  
  # Ensure final_train_features_df is available and is a data.frame for pdp
  pdp_train_data <- as.data.frame(final_train_features_df)

  for (feature_name_pdp in top_features_for_pdp) {
    if (feature_name_pdp %in% colnames(pdp_train_data)) {
      cat(paste("Generating PDP for:", feature_name_pdp, "\n"))
      tryCatch({
        # For PDP with XGBoost, it's often better to use the xgb.model object directly
        # The 'train' data for pdp should be the data the model was trained on (features only)
        pdp_obj <- partial(
          xgb_model,
          pred.var = feature_name_pdp,
          train = pdp_train_data, 
          ice = FALSE, 
          center = FALSE, 
          plot = FALSE, # Generate data, plot separately for more control
          rug = TRUE,
          parallel = FALSE, 
          chull = TRUE 
        )
        
        pdp_plot_obj <- autoplot(pdp_obj, main = paste("PDP for", feature_name_pdp)) + theme_minimal()
        print(pdp_plot_obj)
        # ggsave(paste0("pdp_", gsub("[^A-Za-z0-9_]", "", feature_name_pdp), "_final.png"), plot = pdp_plot_obj, width = 7, height = 5)

      }, error = function(e) {
        cat("Error generating PDP for", feature_name_pdp, ":", e$message, "\n")
      })
    } else {
        cat("Skipping PDP for", feature_name_pdp, "as it's not in the final feature matrix.\n")
    }
  }
} else {
  cat("Skipping PDPs as importance matrix is not available or empty.\n")
}

# --- 12. Residual Plot (Original Scale) ---
cat("\nGenerating Residuals vs. Predicted plot (Original Scale)...\n")
residuals_original <- actual_original - predictions_original
residual_plot_data <- data.frame(
  Predicted_Original = predictions_original,
  Residuals_Original = residuals_original
)

p_res_orig <- ggplot(residual_plot_data, aes(x = Predicted_Original, y = Residuals_Original)) +
  geom_point(alpha = 0.3, shape = 16) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed", size = 1) +
  scale_x_continuous(labels = scales::comma) +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Residuals vs. Predicted (Original Scale) - Final Model",
       x = "Predicted Sale Price ($)", y = "Residuals ($)") +
  theme_minimal()
print(p_res_orig)
# ggsave("residuals_vs_predicted_original_final.png", plot = p_res_orig, width = 8, height = 6)


cat("\n--- Script Finished ---\n")
print(sessionInfo())
```





```{r}
# --- Load necessary libraries ---
library(pdp)
library(ggplot2)
library(lubridate) # For as_date, if needed, and general date handling
library(scales)    # For date_format

# --- Ensure your model and training data are loaded ---
# Example:
# xgb_model <- your_loaded_xgboost_model
# final_train_features_df <- your_final_training_features_dataframe (as a data.frame for pdp)

# --- 1. Generate PDP data for Sale_Date ---
# We need to ensure 'Sale_Date' is the correct column name in final_train_features_df
# that corresponds to the numeric epoch time used in the model.

cat("Generating PDP data for Sale_Date...\n")
pdp_sale_date_data <- tryCatch({
  partial(
    xgb_model,
    pred.var = "Sale_Date",  # Ensure this is the correct feature name
    train = as.data.frame(final_train_features_df), # pdp prefers data.frame
    plot = FALSE,           # We want the data, not the pdp default plot
    rug = TRUE,             # Include data for rug plot (optional but can be useful)
    parallel = FALSE,       # Set to TRUE if you have 'doParallel' set up and it's slow
    # grid.resolution = 50  # Adjust as needed for smoothness vs. computation time
    chull = TRUE            # Computes convex hull for rug plot
  )
}, error = function(e) {
  cat("Error generating PDP data for Sale_Date:", e$message, "\n")
  return(NULL)
})

if (!is.null(pdp_sale_date_data)) {
  cat("PDP data generated. Converting Sale_Date to actual dates for plotting...\n")
  
  # --- 2. Convert the 'Sale_Date' column in pdp_data to POSIXct/Date ---
  # The column name in pdp_sale_date_data corresponding to 'Sale_Date' 
  # will be 'Sale_Date' (or whatever pred.var was).
  
  # Convert from numeric epoch (seconds since 1970-01-01) to POSIXct
  pdp_sale_date_data$Sale_Date_Converted <- as.POSIXct(pdp_sale_date_data$Sale_Date, 
                                                       origin = "1970-01-01", 
                                                       tz = "UTC") # Specify TZ if known, UTC is common for epoch

  # You can convert to Date if you don't need time component:
  # pdp_sale_date_data$Sale_Date_Converted <- as_date(pdp_sale_date_data$Sale_Date_Converted)

  # --- 3. Plot using ggplot2 ---
  cat("Plotting PDP with ggplot2...\n")
  
  pdp_plot_sale_date_ggplot <- ggplot(pdp_sale_date_data, aes(x = Sale_Date_Converted, y = yhat)) +
    geom_line(color = "steelblue", size = 1) +
    labs(
      title = "Partial Dependence Plot: Sale_Date",
      subtitle = "Impact of Sale Date on Predicted log(Sale_Price)",
      x = "Sale Date",
      y = "Predicted log(Sale_Price) (yhat)"
    ) +
    theme_minimal(base_size = 12) +
    scale_x_datetime( # Use scale_x_datetime if POSIXct, scale_x_date if Date
      date_breaks = "1 year", # Show a tick every year
      date_labels = "%Y",     # Display as "YYYY"
      # For more detailed labels like "Jan 2020": date_labels = "%b %Y"
      # For quarterly: date_breaks = "3 months", date_labels = "%Y-Q%q" (requires custom quarter function)
    ) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels if they overlap

  # If you included rug = TRUE and want to show it (requires original Sale_Date values)
  # You might need to get the original Sale_Date values from your training data, convert them,
  # and add them as a geom_rug. For simplicity, I'm omitting the rug here,
  # but the 'chull' data is in pdp_sale_date_data if you want to plot the convex hull.

  print(pdp_plot_sale_date_ggplot)
  
  # To save the plot:
  # ggsave("pdp_sale_date_readable.png", plot = pdp_plot_sale_date_ggplot, width = 10, height = 6)

} else {
  cat("Could not generate PDP plot for Sale_Date due to previous errors.\n")
}

# --- Some Useful Statistics to Consider Adding/Analyzing ---

# 1. Summary of residuals
cat("\n--- Summary of Residuals (Original Scale) ---\n")
# Assuming you have 'actual_original' and 'predictions_original' from your script
if (exists("actual_original") && exists("predictions_original")) {
  residuals_original <- actual_original - predictions_original
  print(summary(residuals_original))
  
  # Quantiles of absolute errors
  abs_errors_original <- abs(residuals_original)
  cat("\nQuantiles of Absolute Errors ($):\n")
  print(quantile(abs_errors_original, probs = c(0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99)))
  
  # Percentage of predictions within certain error bounds
  cat("\nPercentage of predictions within $X error:\n")
  print(paste("Within $10,000:", round(mean(abs_errors_original <= 10000) * 100, 2), "%"))
  print(paste("Within $25,000:", round(mean(abs_errors_original <= 25000) * 100, 2), "%"))
  print(paste("Within $50,000:", round(mean(abs_errors_original <= 50000) * 100, 2), "%"))

} else {
  cat("Variables 'actual_original' and 'predictions_original' not found for residual stats.\n")
}

# 2. Correlation between predicted and actual values (already have R-squared, but good to see)
if (exists("actual_original") && exists("predictions_original")) {
  cor_pred_actual <- cor(predictions_original, actual_original)
  cat("\nCorrelation between Predicted and Actual Sale Prices (Original Scale):", round(cor_pred_actual, 4), "\n")
}

# 3. If you want to add rolling averages or actuals to the Sale_Date PDP:
# You would need to:
#   a. Take your original 'full_data' (before splitting and processing for XGBoost, but after date conversion).
#   b. Aggregate 'Sale_Price_log' by month or quarter.
#   c. Convert these aggregated dates to the same POSIXct/Date format.
#   d. Add another geom_line for these actual averages to the ggplot object.
# Example sketch for actuals:
# monthly_actuals <- full_data %>%
#   mutate(Sale_Month_Yr = floor_date(Sale_Date, "month")) %>% # Assuming Sale_Date is POSIXct
#   group_by(Sale_Month_Yr) %>%
#   summarise(Avg_Actual_Sale_Price_Log = mean(Sale_Price_log, na.rm = TRUE))
#
# pdp_plot_sale_date_ggplot +
#   geom_line(data = monthly_actuals, aes(x = Sale_Month_Yr, y = Avg_Actual_Sale_Price_Log), color = "red", linetype = "dashed")
```


```{r}
# --- Deep Dive into Large Errors ---
cat("\n--- Analyzing Properties with Large Prediction Errors ---\n")

if (exists("actual_original") && 
    exists("predictions_original") && 
    exists("test_data")) { # Using 'test_data'

  if (length(actual_original) != nrow(test_data)) {
    cat("FATAL ERROR: Row count mismatch between actual_original (", length(actual_original), ") and test_data (", nrow(test_data), ").\n")
    cat("Cannot proceed with error analysis feature merge.\n")
  } else {
    residuals_df <- data.frame(
      Actual_Price = actual_original,
      Predicted_Price = predictions_original,
      Residual = actual_original - predictions_original
    )
    residuals_df$Absolute_Residual <- abs(residuals_df$Residual)
    residuals_df$Identifier <- 1:nrow(residuals_df) # Using row index

    # Merge with test_data
    error_analysis_df <- cbind(residuals_df, test_data) 

    # --- Identify properties with largest errors ---
    top_under_predictions <- error_analysis_df %>%
      arrange(desc(Residual)) %>%
      head(10)

    cat("\n--- Top 10 Under-Predictions (Actual Price >> Predicted Price) ---\n")
    # Select columns that exist in 'test_data' and are informative
    # Grantor, Grantee might be too noisy. Focus on property characteristics.
    print(dplyr::select(top_under_predictions, Identifier, Actual_Price, Predicted_Price, Residual, 
                       Sale_Date, Total_Building_SqFt, Year_Built, Property_Age_at_Sale,
                       Total_Bathrooms, Total_Bedrooms, Main_Building_Quality, Property_Type, 
                       Max_Land_Net_SqFt, Neighborhood_Summary, View_Quality_Summary)) # Add/remove as needed

    top_over_predictions <- error_analysis_df %>%
      arrange(Residual) %>%
      head(10)

    cat("\n--- Top 10 Over-Predictions (Predicted Price >> Actual Price) ---\n")
    print(dplyr::select(top_over_predictions, Identifier, Actual_Price, Predicted_Price, Residual, 
                       Sale_Date, Total_Building_SqFt, Year_Built, Property_Age_at_Sale,
                       Total_Bathrooms, Total_Bedrooms, Main_Building_Quality, Property_Type,
                       Max_Land_Net_SqFt, Neighborhood_Summary, View_Quality_Summary))

    top_absolute_errors <- error_analysis_df %>%
      arrange(desc(Absolute_Residual)) %>%
      head(20)

    cat("\n--- Top 20 Largest Absolute Errors ---\n")
    print(dplyr::select(top_absolute_errors, Identifier, Actual_Price, Predicted_Price, Residual, Absolute_Residual,
                       Sale_Date, Total_Building_SqFt, Year_Built, Property_Age_at_Sale,
                       Total_Bathrooms, Total_Bedrooms, Main_Building_Quality, Property_Type,
                       Max_Land_Net_SqFt, Neighborhood_Summary, View_Quality_Summary))
  }
} else {
  cat("Required data (actual_original, predictions_original, or test_data) not found for error analysis.\n")
}
```

```{r}
# Identifiers for further investigation (where SqFt and Year_Built were present)
ids_to_investigate <- c(46925, 56232, 57580, 68195, 58736, 45897)

cat("\n--- Detailed View of Specific Large Error Cases (with SqFt/Year_Built) ---\n")

# Ensure error_analysis_df exists and has the necessary columns
if (exists("error_analysis_df")) {
  
  detailed_errors_df <- error_analysis_df %>%
    filter(Identifier %in% ids_to_investigate) %>%
    dplyr::select(
      Identifier, Actual_Price, Predicted_Price, Residual, Absolute_Residual,
      Sale_Date, Sale_Price_log, # Included Sale_Price_log from test_data for reference
      # Key Features
      Total_Building_SqFt, Year_Built, Property_Age_at_Sale,
      Max_Land_Net_SqFt, Max_Land_Net_Acres,
      Total_Bedrooms, Total_Bathrooms, Total_Stories,
      # Quality and Condition
      Main_Building_Quality, Num_Buildings_Good_Condition, Num_Buildings_Avg_Condition,
      # Location and View
      Neighborhood_Summary, Avg_Latitude, Avg_Longitude,
      View_Quality_Summary, View_Type, View_Score, Waterfront_Type_Summary,
      # Property Type and Sale Details
      Property_Type, Instrument_Type, Sale_Warning_Code,
      Improved_Vacant, Valid_Invalid, Confirmed_Unconfirmed,
      # Other potentially relevant fields
      Total_Attached_Garage_SqFt, Total_Fireplaces,
      Residential_Amenity, Street_Type_Summary,
      Tax_Summary_Taxable_Value, Tax_Summary_Land_Value, Tax_Summary_Improvement_Value # If available and not all NA
    ) %>%
    arrange(Identifier) # Optional: arrange by Identifier for consistent order

  print(detailed_errors_df, width = 200) # Print with a wider display

} else {
  cat("Error: 'error_analysis_df' not found. Please ensure the previous error analysis steps were run.\n")
}

# You might also want to look at one property at a time if the table is too wide
# For example, for identifier 46925:
# print(error_analysis_df %>% filter(Identifier == 46925), width = 200)
```


```{r}
# Ensure dplyr is loaded
# library(dplyr) 

# Identifiers for further investigation (row indices from test_data/residuals_df)
ids_to_investigate <- c(45897, 46925, 56232, 57580, 58736, 68195)

cat("\n--- Detailed View of Specific Large Error Cases (using row indices from test_data) ---\n")

if (exists("test_data") && exists("error_analysis_df")) {
  
  # Loop through each identifier (row index)
  for (row_idx in ids_to_investigate) {
    cat("\n--- Details for Test Data Row Index (Identifier):", row_idx, "---\n")
    
    if (row_idx <= nrow(test_data)) {
      property_source_details <- test_data %>%
        dplyr::slice(row_idx) %>%
        dplyr::select(
          # Columns from your colnames(test_data) output:
          "Parcel_Count", "Sale_Date", "Instrument_Type", "Sale_Warning_Code",                 
          "Grantor", "Grantee", "Valid_Invalid", "Confirmed_Unconfirmed",             
          "Improved_Vacant", "Property_Type", "Latest_Appraisal_Date", "Max_Land_Net_Acres",                
          "Max_Land_Net_SqFt", "Waterfront_Type_Summary", "View_Quality_Summary", "Street_Type_Summary",               
          "Avg_Latitude", "Avg_Longitude", "Tax_Summary_Tax_Year", "Tax_Summary_Taxable_Value",         
          "Tax_Summary_Land_Value", "Tax_Summary_Improvement_Value", "Num_Buildings", "Total_Building_SqFt",               
          "Total_Net_SqFt", "Total_Attached_Garage_SqFt", "Total_Fireplaces", "Num_Buildings_Good_Condition",      
          "Num_Buildings_Avg_Condition", "Total_Addon_Num_2FixtureBaths", "Total_Addon_Num_3FixtureBaths", "Total_Addon_Deck_SqFt",             
          "Parcel_Has_PoolOrSpa_Flag", "Year_Built", "Total_Bedrooms", "Total_Bathrooms",                   
          "Total_Stories", "Main_Building_Quality", "Neighborhood_Summary", "View_Type",                         
          "View_Score", "View_Quality", "Residential_Amenity", "Commercial_Zoning",                 
          "Residential_Waterfront_Description", "Num_Merge_Events", "Latest_Merge_Event_Date", "Sale_Month",                        
          "Property_Age_at_Sale", "Sale_Price_log" # Sale_Price_log is here
        )

      # error_info will provide the actual and predicted prices and residuals
      error_info <- error_analysis_df %>%
        dplyr::filter(Identifier == row_idx) %>%
        dplyr::select(Actual_Price, Predicted_Price, Residual, Absolute_Residual) # Actual_Price here is the original scale

      if (nrow(property_source_details) > 0 && nrow(error_info) > 0) {
        # Combine: error_info provides the core error metrics.
        # property_source_details provides all other features from test_data for that row.
        # We already have Sale_Price_log and Property_Age_at_Sale in property_source_details.
        
        combined_details <- dplyr::bind_cols(
          error_info, # Actual_Price, Predicted_Price, Residual, Absolute_Residual
          property_source_details
        )
        
        print(as.data.frame(t(combined_details)), quote = FALSE)
      } else {
        cat("Could not retrieve full details for row index:", row_idx, "\n")
        if(nrow(property_source_details) > 0) print(as.data.frame(t(property_source_details)), quote = FALSE)
        if(nrow(error_info) > 0) print(as.data.frame(t(error_info)), quote = FALSE)
      }

    } else {
      cat("Row index", row_idx, "is out of bounds for test_data which has", nrow(test_data), "rows.\n")
    }
    cat("\n---------------------------------------------------\n")
  }
  
} else {
  if (!exists("test_data")) cat("Error: 'test_data' (or your equivalent test set name) not found.\n")
  if (!exists("error_analysis_df")) cat("Error: 'error_analysis_df' not found.\n")
}
```



```{r}
# (Inside your existing script structure)
all_problem_details <- list() # Initialize an empty list

# ... (your existing loop setup) ...
for (row_idx in ids_to_investigate) {
  # ... (your existing code to create combined_details) ...
  
  if (nrow(property_source_details) > 0 && nrow(error_info) > 0) {
    combined_details <- dplyr::bind_cols(
      error_info, 
      property_source_details
    )
    # Store it in the list, naming it by the identifier
    all_problem_details[[as.character(row_idx)]] <- combined_details 
    
    # Optionally still print to console
    cat("\n--- Details for Test Data Row Index (Identifier):", row_idx, "---\n")
    print(as.data.frame(t(combined_details)), quote = FALSE)
    cat("\n---------------------------------------------------\n")
  }
  # ... (rest of your loop) ...
}

# After the loop, you can view them:
# View(all_problem_details[['45897']])
# View(all_problem_details[['46925']])
# etc.
```




```{r}
# Ensure dplyr is loaded
# library(dplyr) 

# Identifiers for further investigation (row indices from test_data/residuals_df)
ids_to_investigate <- c(45897, 46925, 56232, 57580, 58736, 68195)

cat("\n--- Detailed View of Specific Large Error Cases (using row indices from test_data) ---\n")

# Initialize an empty list to store the details for each problematic ID
all_problem_details <- list() # <--- NEW: Initialize list

if (exists("test_data") && exists("error_analysis_df")) {
  
  # Loop through each identifier (row index)
  for (row_idx in ids_to_investigate) {
    # cat("\n--- Processing Test Data Row Index (Identifier):", row_idx, "---\n") # Optional: less verbose console output if storing
    
    if (row_idx <= nrow(test_data)) {
      property_source_details <- test_data %>%
        dplyr::slice(row_idx) %>%
        dplyr::select(
          # Columns from your colnames(test_data) output:
          "Parcel_Count", "Sale_Date", "Instrument_Type", "Sale_Warning_Code",                 
          "Grantor", "Grantee", "Valid_Invalid", "Confirmed_Unconfirmed",             
          "Improved_Vacant", "Property_Type", "Latest_Appraisal_Date", "Max_Land_Net_Acres",                
          "Max_Land_Net_SqFt", "Waterfront_Type_Summary", "View_Quality_Summary", "Street_Type_Summary",               
          "Avg_Latitude", "Avg_Longitude", "Tax_Summary_Tax_Year", "Tax_Summary_Taxable_Value",         
          "Tax_Summary_Land_Value", "Tax_Summary_Improvement_Value", "Num_Buildings", "Total_Building_SqFt",               
          "Total_Net_SqFt", "Total_Attached_Garage_SqFt", "Total_Fireplaces", "Num_Buildings_Good_Condition",      
          "Num_Buildings_Avg_Condition", "Total_Addon_Num_2FixtureBaths", "Total_Addon_Num_3FixtureBaths", "Total_Addon_Deck_SqFt",             
          "Parcel_Has_PoolOrSpa_Flag", "Year_Built", "Total_Bedrooms", "Total_Bathrooms",                   
          "Total_Stories", "Main_Building_Quality", "Neighborhood_Summary", "View_Type",                         
          "View_Score", "View_Quality", "Residential_Amenity", "Commercial_Zoning",                 
          "Residential_Waterfront_Description", "Num_Merge_Events", "Latest_Merge_Event_Date", "Sale_Month",                        
          "Property_Age_at_Sale", "Sale_Price_log" 
        )

      error_info <- error_analysis_df %>%
        dplyr::filter(Identifier == row_idx) %>%
        dplyr::select(Actual_Price, Predicted_Price, Residual, Absolute_Residual)

      if (nrow(property_source_details) > 0 && nrow(error_info) > 0) {
        combined_details <- dplyr::bind_cols(
          error_info, 
          property_source_details
        )
        
        # Store the combined_details in the list, using row_idx as the name
        all_problem_details[[as.character(row_idx)]] <- combined_details # <--- NEW: Store in list
        
        # Optionally, still print to console if you want immediate feedback
        cat("\n--- Details for Test Data Row Index (Identifier):", row_idx, "---\n")
        print(as.data.frame(t(combined_details)), quote = FALSE)
        cat("\n---------------------------------------------------\n")

      } else {
        cat("Could not retrieve full details for row index:", row_idx, "\n")
        # ... (error printing as before) ...
      }

    } else {
      cat("Row index", row_idx, "is out of bounds for test_data which has", nrow(test_data), "rows.\n")
    }
    # cat("\n---------------------------------------------------\n") # Moved print inside the if block
  }
  
  # After the loop, you can access and View individual data frames:
  # Example:
  # if(length(all_problem_details) > 0) {
  #   View(all_problem_details[['45897']]) # View details for ID 45897
  #   View(all_problem_details[['46925']]) # View details for ID 46925
  #   # And so on for the other IDs
  # }
  # You can also inspect the whole list object:
  # str(all_problem_details)

} else {
  # ... (error messages as before) ...
}
```


```{r}
# --- 1. Load Data ---
# ... (your existing code for loading full_data) ...
cat("Data loaded. Initial dimensions:", paste(dim(full_data), collapse="x"), "\n")

# --- 2. Initial Filtering and Data Cleaning ---
cat("Starting initial filtering and data cleaning...\n")

# 2a. Filter for Specific Housing Types (Property_Type)
# ... (your existing code for Property_Type filter) ...
if (nrow(full_data) == 0) stop("No data remaining after filtering for housing types.")

# <<<< INSERT NEW DETAILED CLEANING STEPS HERE >>>>
# Let's rename 'full_data' to 'data_to_clean_further' for clarity in this block
data_to_clean_further <- full_data 
cat("\nStarting detailed cleaning based on outlier investigation...\n")
cat("Initial rows for detailed cleaning:", nrow(data_to_clean_further), "\n")

# --- New Cleaning Step 1: Filter by Sale_Warning_Code ---
# (Using the code block we developed previously)
cat("\nUnique Sale_Warning_Codes before filtering:\n")
print(unique(data_to_clean_further$Sale_Warning_Code))
codes_to_remove <- c("Com and Res sold together") # Add others if needed
data_to_clean_further <- data_to_clean_further %>%
  filter(is.na(Sale_Warning_Code) | !(Sale_Warning_Code %in% codes_to_remove))
cat("Rows after Sale_Warning_Code filter:", nrow(data_to_clean_further), "\n")

# --- New Cleaning Step 2: Filter by Parcel_Count ---
data_to_clean_further <- data_to_clean_further %>%
  filter(Parcel_Count == 1 | is.na(Parcel_Count))
cat("Rows after Parcel_Count filter:", nrow(data_to_clean_further), "\n")

# --- New Cleaning Step 3: Filter by Valid_Invalid and Confirmed_Unconfirmed ---
# (Ensure these columns exist and logic is correct for your data)
if ("Valid_Invalid" %in% names(data_to_clean_further)) {
    data_to_clean_further <- data_to_clean_further %>%
      filter(Valid_Invalid == "Valid" | is.na(Valid_Invalid))
    cat("Rows after Valid_Invalid filter:", nrow(data_to_clean_further), "\n")
}
if ("Confirmed_Unconfirmed" %in% names(data_to_clean_further)) {
    # Assuming 1 means confirmed. Adjust if necessary.
    data_to_clean_further <- data_to_clean_further %>%
      filter(Confirmed_Unconfirmed == 1 | is.na(Confirmed_Unconfirmed))
    cat("Rows after Confirmed_Unconfirmed filter:", nrow(data_to_clean_further), "\n")
}

# Assign back to full_data after these initial critical filters
full_data <- data_to_clean_further
cat("Data dimensions after Sale_Warning, Parcel_Count, Valid/Confirmed filters:", paste(dim(full_data), collapse="x"), "\n")
if (nrow(full_data) == 0) stop("No data remaining after initial detailed cleaning.")


# 2b. Sale_Date Cleaning and Feature Engineering (Property_Age_at_Sale)
# ... (your existing code for Sale_Date processing, future date removal, Sale_Year, Sale_Month) ...
# ... (your existing code for Year_Built imputation and Property_Age_at_Sale creation) ...
# NOW, apply the Property_Age_at_Sale filter
if ("Property_Age_at_Sale" %in% names(full_data)) {
    initial_rows_age_filter <- nrow(full_data)
    full_data <- full_data %>%
      filter(Property_Age_at_Sale >= 0 | is.na(Property_Age_at_Sale))
    rows_removed_age_filter <- initial_rows_age_filter - nrow(full_data)
    cat(paste("Removed", rows_removed_age_filter, "rows with negative or invalid Property_Age_at_Sale.\n"))
    cat("Data dimensions after Property_Age_at_Sale filter:", paste(dim(full_data), collapse="x"), "\n")
    if (nrow(full_data) == 0) stop("No data remaining after Property_Age_at_Sale filter.")
} else {
    cat("Warning: 'Property_Age_at_Sale' not found for filtering.\n")
}


# --- New Cleaning Step 4: Address 0 Bed/Bath properties ---
# (Using the code block we developed, applied to the current 'full_data')
cat("\nInvestigating 0 Bed/Bath properties before filtering:\n")
if (all(c("Total_Bedrooms", "Total_Bathrooms", "Improved_Vacant") %in% names(full_data))) {
    zero_bed_bath_summary <- full_data %>%
      filter(Total_Bedrooms == 0 | Total_Bathrooms == 0) %>%
      group_by(Total_Bedrooms, Total_Bathrooms, Improved_Vacant) %>%
      summarise(
        count = n(),
        # Ensure Sale_Price exists for this summary, or use Sale_Price_log if already created
        # For now, assuming Sale_Price is available for inspection before log transformation
        min_price_debug = if("Sale_Price" %in% names(.)) min(Sale_Price, na.rm = TRUE) else NA,
        max_price_debug = if("Sale_Price" %in% names(.)) max(Sale_Price, na.rm = TRUE) else NA,
        avg_sqft = mean(Total_Building_SqFt, na.rm = TRUE)
      ) %>%
      ungroup()
    print(zero_bed_bath_summary)

    # Apply filter (e.g., Option A)
    initial_rows_0bedbath <- nrow(full_data)
    full_data <- full_data %>%
      filter(!((Total_Bedrooms == 0 | Total_Bathrooms == 0) & Improved_Vacant == "Improved"))
    rows_removed_0bedbath <- initial_rows_0bedbath - nrow(full_data)
    cat(paste("Removed", rows_removed_0bedbath, "rows for 0 Bed/Bath with Improved_Vacant == 'Improved'.\n"))
    cat("Data dimensions after 0 Bed/Bath filter:", paste(dim(full_data), collapse="x"), "\n")
    if (nrow(full_data) == 0) stop("No data remaining after 0 Bed/Bath filter.")
} else {
    cat("Warning: Required columns for 0 Bed/Bath filter not found.\n")
}

# <<<< END OF NEW DETAILED CLEANING STEPS >>>>

# 2c. Sale_Price Outlier Treatment (Now applied to a cleaner dataset)
# ... (your existing code for Sale_Price floor, capping, and log transformation) ...
# ... (This section should now operate on the 'full_data' that has been cleaned by the steps above) ...

# --- 3. Define Roles and Select Features ---
# ... (your existing code) ...

# --- 4. Split Data ---
# ... (your existing code) ...

# --- 5. Preprocessing (Imputation, OHE) ---
# ... (your existing code) ...
# Note: The imputation logic for categorical "Missing_Val" and numeric medians
# will now operate on the data that has already had problematic rows removed.

# --- 6. Prepare Data for XGBoost ---
# ... (your existing code) ...
```
```{r}
full_data <- full_data %>%
  filter(!((Total_Bedrooms == 0 | Total_Bathrooms == 0) & Improved_Vacant == "Improved"))
```

```{r}
# Current script section:
# 2c. Sale_Price Outlier Treatment
if (!"Sale_Price" %in% names(full_data)) {
    stop("Critical Error: 'Sale_Price' column not found for outlier treatment.")
}
if (!is.numeric(full_data$Sale_Price)) {
  full_data$Sale_Price <- as.numeric(as.character(full_data$Sale_Price)) # Ensure conversion
}
# Ensure no NAs in Sale_Price before quantile calculation or filtering
full_data <- full_data %>% filter(!is.na(Sale_Price)) 
cat("Data dimensions after removing NA Sale_Price:", paste(dim(full_data), collapse="x"), "\n")
if (nrow(full_data) == 0) stop("No data remaining after removing NA Sale_Prices.")


price_floor_threshold <- 10000 
initial_rows_price_filter <- nrow(full_data)
full_data <- full_data %>% filter(Sale_Price >= price_floor_threshold)
rows_removed_price_floor <- initial_rows_price_filter - nrow(full_data)
cat(paste("Removed", rows_removed_price_floor, "rows with Sale_Price < $", format(price_floor_threshold, big.mark=","), ".\n"))
cat("Data dimensions after low Sale_Price filter:", paste(dim(full_data), collapse="x"), "\n")
if (nrow(full_data) == 0) stop("No data remaining after filtering low sale prices.")

# Recalculate quantile on the potentially smaller, cleaner dataset
price_cap_threshold <- quantile(full_data$Sale_Price, 0.995, na.rm = TRUE)
cat(paste("Capping Sale_Price at 99.5th percentile: $", format(price_cap_threshold, big.mark=","), ".\n"))
num_capped <- sum(full_data$Sale_Price > price_cap_threshold, na.rm = TRUE)
full_data <- full_data %>%
  mutate(Sale_Price = ifelse(Sale_Price > price_cap_threshold, price_cap_threshold, Sale_Price))
cat(paste(num_capped, "Sale_Price values were capped.\n"))

# 2d. Create Target Variable (Log Transformed)
full_data <- full_data %>%
  mutate(Sale_Price_log = log(Sale_Price))
cat("Created 'Sale_Price_log' target variable.\n")
cat("Data dimensions after Sale_Price processing and log transformation:", paste(dim(full_data), collapse="x"), "\n")
```


```{r}
# --- 0. Load Libraries and Setup ---
cat("Loading libraries...\n")
# Ensure all necessary packages are installed
# install.packages(c("arrow", "dplyr", "lubridate", "ggplot2", "fastDummies", "xgboost", "Metrics", "pdp", "doParallel", "scales", "caret"))

library(arrow)
library(dplyr)
library(lubridate)
library(ggplot2)
library(fastDummies)
library(xgboost)
library(Metrics) # For MAE, RMSE
library(pdp)     # For Partial Dependence Plots
library(doParallel) # For parallel processing
library(scales)     # For plot axis formatting
library(caret)      # For R2 function and other utilities

# Set a seed for reproducibility
set.seed(123)

# --- 1. Load Data ---
cat("Loading data...\n")
# !!! USER ACTION REQUIRED: Update this path to your data file !!!
full_data_path <- "../data/stage/full_data_for_modeling.parquet" 

full_data <- tryCatch({
  read_parquet(full_data_path)
}, error = function(e) {
  cat("Error loading", full_data_path, ":", e$message, "\n")
  cat("Please ensure the path is correct and the file exists.\n")
  return(NULL)
})

if (is.null(full_data) || nrow(full_data) == 0) {
  stop("Halting script: Data loading failed or data is empty.")
}
cat("Data loaded. Initial dimensions:", paste(dim(full_data), collapse="x"), "\n")

# --- 2. Initial Filtering and Data Cleaning ---
cat("Starting initial filtering and data cleaning...\n")

# 2a. Filter for Specific Housing Types
housing_property_types_to_keep <- c("Residential", "Condominium")
if ("Property_Type" %in% names(full_data)) {
  full_data <- full_data %>%
    filter(Property_Type %in% housing_property_types_to_keep)
  cat("Data dimensions after housing type filter:", paste(dim(full_data), collapse="x"), "\n")
} else {
  cat("Warning: 'Property_Type' column not found. Skipping housing type filter.\n")
}
if (nrow(full_data) == 0) stop("No data remaining after filtering for housing types.")

# <<<< START OF NEW DETAILED CLEANING STEPS >>>>
data_to_clean_further <- full_data 
cat("\nStarting detailed cleaning based on outlier investigation...\n")
cat("Initial rows for detailed cleaning:", nrow(data_to_clean_further), "\n")

# --- New Cleaning Step 1: Filter by Sale_Warning_Code ---
if ("Sale_Warning_Code" %in% names(data_to_clean_further)) {
    cat("\nUnique Sale_Warning_Codes before filtering:\n")
    print(unique(data_to_clean_further$Sale_Warning_Code))
    codes_to_remove <- c(
      "Com and Res sold together",
      "CU Open Space & Ag RCW 84.34",
      "MH Acct Type included in sale",
      "MH & Land Sep ETN",
      "Foreclosure Sale",
      "Desig Forest Land RCW 84.33",
      "Improved after sale",
      "Change in Highest/best use", 
      "Short Sale",
      "Forced Sale Trans in Lieu Frcl",
      "Family different last names",    
      "Improvement on Leased Land",
      "Auction Sale",                   
      "Exempt for taxatn Gov nonprof",
      "Friends Not listed on market",   
      "Current Segregation not apprsd", 
      "Bldg removed after sale",
      "Valid Land Sale",                
      "Misc-Documentation Required",    
      "Subsidized Housing",
      "Gift love & affection"
    )    
    initial_rows_swc <- nrow(data_to_clean_further)
    data_to_clean_further <- data_to_clean_further %>%
      filter(is.na(Sale_Warning_Code) | !(Sale_Warning_Code %in% codes_to_remove))
    cat("Rows after Sale_Warning_Code filter:", nrow(data_to_clean_further), "\n")
    cat("Rows removed by Sale_Warning_Code filter:", initial_rows_swc - nrow(data_to_clean_further), "\n")
} else {
    cat("Warning: 'Sale_Warning_Code' column not found. Skipping this filter.\n")
}

# --- New Cleaning Step 2: Filter by Parcel_Count ---
if ("Parcel_Count" %in% names(data_to_clean_further)) {
    initial_rows_pc <- nrow(data_to_clean_further)
    data_to_clean_further <- data_to_clean_further %>%
      filter(Parcel_Count == 1 | is.na(Parcel_Count)) # Keep NA Parcel_Count for now, or decide to remove
    cat("Rows after Parcel_Count filter (keeping only 1 or NA):", nrow(data_to_clean_further), "\n")
    cat("Rows removed by Parcel_Count filter:", initial_rows_pc - nrow(data_to_clean_further), "\n")
} else {
    cat("Warning: 'Parcel_Count' column not found. Skipping this filter.\n")
}

# --- New Cleaning Step 3: Filter by Valid_Invalid and Confirmed_Unconfirmed ---
if ("Valid_Invalid" %in% names(data_to_clean_further)) {
    cat("\nUnique Valid_Invalid values:", paste(unique(data_to_clean_further$Valid_Invalid), collapse=", "), "\n")
    initial_rows_vi <- nrow(data_to_clean_further)
    data_to_clean_further <- data_to_clean_further %>%
      filter(Valid_Invalid == "Valid" | is.na(Valid_Invalid)) # Adjust if NAs should be removed
    cat("Rows after Valid_Invalid filter:", nrow(data_to_clean_further), "\n")
    cat("Rows removed by Valid_Invalid filter:", initial_rows_vi - nrow(data_to_clean_further), "\n")
} else {
    cat("Warning: 'Valid_Invalid' column not found. Skipping this filter.\n")
}

if ("Confirmed_Unconfirmed" %in% names(data_to_clean_further)) {
    cat("Unique Confirmed_Unconfirmed values:", paste(unique(data_to_clean_further$Confirmed_Unconfirmed), collapse=", "), "\n")
    initial_rows_cu <- nrow(data_to_clean_further)
    # Assuming 1 means confirmed. If 0 means confirmed, adjust.
    # If Confirmed_Unconfirmed can be NA and that's acceptable, modify the filter.
    data_to_clean_further <- data_to_clean_further %>%
      filter(Confirmed_Unconfirmed == 1 | is.na(Confirmed_Unconfirmed)) 
    cat("Rows after Confirmed_Unconfirmed filter:", nrow(data_to_clean_further), "\n")
    cat("Rows removed by Confirmed_Unconfirmed filter:", initial_rows_cu - nrow(data_to_clean_further), "\n")
} else {
    cat("Warning: 'Confirmed_Unconfirmed' column not found. Skipping this filter.\n")
}

# Assign back to full_data after these initial critical filters
full_data <- data_to_clean_further
cat("\nData dimensions after Sale_Warning, Parcel_Count, Valid/Confirmed filters:", paste(dim(full_data), collapse="x"), "\n")
if (nrow(full_data) == 0) stop("No data remaining after initial detailed cleaning.")


# 2b. Sale_Date Cleaning and Feature Engineering
date_col_name <- "Sale_Date" 
if (date_col_name %in% names(full_data)) {
  cat(paste("\nProcessing '", date_col_name, "'...\n", sep=""))
  if (is.character(full_data[[date_col_name]])) {
    full_data[[date_col_name]] <- ymd(full_data[[date_col_name]], quiet = TRUE)
  } else if (!inherits(full_data[[date_col_name]], "Date") && !inherits(full_data[[date_col_name]], "POSIXct")) {
     full_data[[date_col_name]] <- as.Date(full_data[[date_col_name]]) # Attempt conversion
     cat("Converted '", date_col_name, "' to Date type.\n")
  }
  
  current_date <- Sys.Date() 
  initial_rows_date_filter <- nrow(full_data)
  full_data <- full_data %>% filter(get(date_col_name) <= current_date)
  rows_removed_date_filter <- initial_rows_date_filter - nrow(full_data)
  cat(paste("Removed", rows_removed_date_filter, "rows with future Sale_Date (after", format(current_date), ").\n"))
  cat("Data dimensions after future Sale_Date filter:", paste(dim(full_data), collapse="x"), "\n")
  if (nrow(full_data) == 0) stop("No data remaining after filtering future sale dates.")

  full_data <- full_data %>%
    mutate(
      Sale_Year = as.numeric(year(get(date_col_name))), 
      Sale_Month = factor(month(get(date_col_name), label = TRUE, abbr = FALSE))
    )
  
  if ("Year_Built" %in% names(full_data) && "Sale_Year" %in% names(full_data)) {
      year_built_median_impute <- median(full_data$Year_Built[full_data$Year_Built > 1800 & full_data$Year_Built <= full_data$Sale_Year & !is.na(full_data$Year_Built)], na.rm = TRUE)
      if(is.na(year_built_median_impute) || !is.finite(year_built_median_impute)) year_built_median_impute <- 2000 # Fallback

      full_data$Year_Built_Mode <- ifelse(is.na(full_data$Year_Built) | full_data$Year_Built <= 1800 | full_data$Year_Built > full_data$Sale_Year, 
                                          year_built_median_impute, 
                                          full_data$Year_Built)
      full_data <- full_data %>%
        mutate(Property_Age_at_Sale = Sale_Year - Year_Built_Mode) %>%
        select(-Year_Built_Mode) 
      cat("Created 'Property_Age_at_Sale'.\n")

      # Filter by Property_Age_at_Sale
      initial_rows_age_filter <- nrow(full_data)
      full_data <- full_data %>%
        filter(Property_Age_at_Sale >= 0 | is.na(Property_Age_at_Sale)) # Keep NAs or decide to remove/impute
      rows_removed_age_filter <- initial_rows_age_filter - nrow(full_data)
      cat(paste("Removed", rows_removed_age_filter, "rows with negative or invalid Property_Age_at_Sale (or kept NAs).\n"))
      cat("Data dimensions after Property_Age_at_Sale filter:", paste(dim(full_data), collapse="x"), "\n")
      if (nrow(full_data) == 0) stop("No data remaining after Property_Age_at_Sale filter.")

  } else {
      cat("Warning: 'Year_Built' or 'Sale_Year' not available for 'Property_Age_at_Sale' calculation and filtering.\n")
  }

} else {
  cat(paste("Warning: Primary date column '", date_col_name, "' not found. Time-based features will be limited.\n", sep=""))
}


# --- New Cleaning Step 4: Address 0 Bed/Bath properties ---
cat("\nInvestigating 0 Bed/Bath properties before filtering:\n")
required_cols_0bedbath <- c("Total_Bedrooms", "Total_Bathrooms", "Improved_Vacant", "Sale_Price", "Total_Building_SqFt")
if (all(required_cols_0bedbath %in% names(full_data))) {
    zero_bed_bath_summary <- full_data %>%
      filter(Total_Bedrooms == 0 | Total_Bathrooms == 0) %>%
      group_by(Total_Bedrooms, Total_Bathrooms, Improved_Vacant) %>%
      summarise(
        count = n(),
        min_price_debug = min(Sale_Price, na.rm = TRUE),
        max_price_debug = max(Sale_Price, na.rm = TRUE),
        avg_sqft = mean(Total_Building_SqFt, na.rm = TRUE),
        .groups = 'drop' 
      )
    print(zero_bed_bath_summary)

    initial_rows_0bedbath <- nrow(full_data)
    full_data <- full_data %>%
      filter(!((Total_Bedrooms == 0 | Total_Bathrooms == 0) & Improved_Vacant == "Improved"))
    rows_removed_0bedbath <- initial_rows_0bedbath - nrow(full_data)
    cat(paste("Removed", rows_removed_0bedbath, "rows for 0 Bed/Bath with Improved_Vacant == 'Improved'.\n"))
    cat("Data dimensions after 0 Bed/Bath filter:", paste(dim(full_data), collapse="x"), "\n")
    if (nrow(full_data) == 0) stop("No data remaining after 0 Bed/Bath filter.")
} else {
    cat("Warning: One or more required columns for 0 Bed/Bath filter (", paste(required_cols_0bedbath, collapse=", "), ") not found. Skipping this filter.\n")
}
# <<<< END OF NEW DETAILED CLEANING STEPS >>>>


# 2c. Sale_Price Outlier Treatment
cat("\nStarting Sale_Price outlier treatment...\n")
if (!"Sale_Price" %in% names(full_data)) {
    stop("Critical Error: 'Sale_Price' column not found for outlier treatment.")
}
if (!is.numeric(full_data$Sale_Price)) {
  full_data$Sale_Price <- as.numeric(as.character(full_data$Sale_Price)) # Ensure conversion
}
# Ensure no NAs in Sale_Price before quantile calculation or filtering
initial_rows_na_price <- nrow(full_data)
full_data <- full_data %>% filter(!is.na(Sale_Price)) 
rows_removed_na_price <- initial_rows_na_price - nrow(full_data)
cat(paste("Removed", rows_removed_na_price, "rows with NA Sale_Price.\n"))
cat("Data dimensions after removing NA Sale_Price:", paste(dim(full_data), collapse="x"), "\n")
if (nrow(full_data) == 0) stop("No data remaining after removing NA Sale_Prices.")


price_floor_threshold <- 10000 
initial_rows_price_filter <- nrow(full_data)
full_data <- full_data %>% filter(Sale_Price >= price_floor_threshold)
rows_removed_price_floor <- initial_rows_price_filter - nrow(full_data)
cat(paste("Removed", rows_removed_price_floor, "rows with Sale_Price < $", format(price_floor_threshold, big.mark=","), ".\n"))
cat("Data dimensions after low Sale_Price filter:", paste(dim(full_data), collapse="x"), "\n")
if (nrow(full_data) == 0) stop("No data remaining after filtering low sale prices.")

price_cap_threshold <- quantile(full_data$Sale_Price, 0.995, na.rm = TRUE)
cat(paste("Capping Sale_Price at 99.5th percentile: $", format(price_cap_threshold, big.mark=","), ".\n"))
num_capped <- sum(full_data$Sale_Price > price_cap_threshold, na.rm = TRUE)
full_data <- full_data %>%
  mutate(Sale_Price = ifelse(Sale_Price > price_cap_threshold, price_cap_threshold, Sale_Price))
cat(paste(num_capped, "Sale_Price values were capped.\n"))

# 2d. Create Target Variable (Log Transformed)
full_data <- full_data %>%
  mutate(Sale_Price_log = log(Sale_Price))
cat("Created 'Sale_Price_log' target variable.\n")
cat("Data dimensions after Sale_Price processing and log transformation:", paste(dim(full_data), collapse="x"), "\n")


cat("\nDefining roles and selecting features...\n")
target_variable <- "Sale_Price_log"

id_variables <- c(
  # Existing ID variables (keep them)
  "Parcel_Number", "Most_Recent_Sale_Date", "Sale_Price", 
  "Sale_Year", 
  "ParcelID", "TaxAccountNbr", "ETN",

  # NEW additions based on our discussion
  "Grantor", "Grantee",                                      
  "Parcel_Count", "Sale_Warning_Code", "Valid_Invalid", "Confirmed_Unconfirmed" 
) 

redundant_variables <- c(
  "Appraisal_Account_Type_Summary" 
  # Add other known redundant columns
)

all_cols <- names(full_data)
potential_predictors <- setdiff(all_cols, c(target_variable, id_variables, redundant_variables))
valid_potential_predictors <- intersect(potential_predictors, names(full_data))

character_cols_pot <- names(full_data)[sapply(full_data[, valid_potential_predictors, drop = FALSE], function(x) is.character(x) || is.factor(x))]
numeric_cols_pot <- names(full_data)[sapply(full_data[, valid_potential_predictors, drop = FALSE], is.numeric)]
posixct_cols_pot <- names(full_data)[sapply(full_data[, valid_potential_predictors, drop = FALSE], inherits, "POSIXct")]
date_cols_pot <- names(full_data)[sapply(full_data[, valid_potential_predictors, drop = FALSE], inherits, "Date")]

numeric_cols_pot <- unique(c(numeric_cols_pot, posixct_cols_pot, date_cols_pot))
selected_predictors <- intersect(valid_potential_predictors, unique(c(numeric_cols_pot, character_cols_pot)))

cat("Final selected predictors (", length(selected_predictors), "):\n", paste(head(selected_predictors, 20), collapse=", "), if(length(selected_predictors)>20) "...\n" else "\n")



modeling_data <- full_data[, c(selected_predictors, target_variable), drop = FALSE]
if (nrow(modeling_data) == 0) stop("No data in modeling_data after feature selection.")
cat("Dimensions of modeling_data:", paste(dim(modeling_data), collapse="x"), "\n")


# --- 4. Split Data into Training and Testing Sets ---
cat("\nSplitting data into training (80%) and testing (20%) sets...\n")
train_index <- sample(seq_len(nrow(modeling_data)), size = floor(0.80 * nrow(modeling_data)))
train_data <- modeling_data[train_index, ]
test_data <- modeling_data[-train_index, ]
cat("Training set dimensions:", paste(dim(train_data), collapse="x"), "\n")
cat("Testing set dimensions:", paste(dim(test_data), collapse="x"), "\n")

if (nrow(train_data) < 10 || nrow(test_data) < 10) {
    stop("Insufficient data in training or testing set after split.")
}

# --- 5. Preprocessing ---
cat("\nStarting preprocessing...\n")

imputation_values <- list()
processed_train_data <- train_data
processed_test_data <- test_data

# Ensure target variable is not in predictors list for imputation loop
predictors_for_imputation <- setdiff(selected_predictors, target_variable)

for (col in predictors_for_imputation) {
  if (col %in% names(train_data)) { 
    if (is.numeric(train_data[[col]]) || inherits(train_data[[col]], "POSIXct") || inherits(train_data[[col]], "Date")) {
      median_val <- median(train_data[[col]], na.rm = TRUE)
      imputation_values[[col]] <- ifelse(is.na(median_val) || !is.finite(median_val), 0, median_val) 
      
      processed_train_data[[col]][is.na(processed_train_data[[col]])] <- imputation_values[[col]]
      if (col %in% names(processed_test_data)) {
        processed_test_data[[col]][is.na(processed_test_data[[col]])] <- imputation_values[[col]]
      }
    } else if (is.character(train_data[[col]]) || is.factor(train_data[[col]])) {
      # Convert character to factor first if necessary
      if (is.character(processed_train_data[[col]])) {
        processed_train_data[[col]] <- factor(processed_train_data[[col]])
      }
      if (col %in% names(processed_test_data) && is.character(processed_test_data[[col]])) {
        # Ensure levels are consistent with training before converting test to factor
        # Use levels from the processed_train_data factor
        processed_test_data[[col]] <- factor(processed_test_data[[col]], levels = levels(processed_train_data[[col]]))
      }
      
      # Add "Missing_Val" level if it doesn't exist and impute NAs
      train_levels <- levels(processed_train_data[[col]])
      if (!("Missing_Val" %in% train_levels)) {
        levels(processed_train_data[[col]]) <- c(train_levels, "Missing_Val")
      }
      processed_train_data[[col]][is.na(processed_train_data[[col]])] <- "Missing_Val"
      imputation_values[[col]] <- "Missing_Val" 
      
      if (col %in% names(processed_test_data)) {
        # Ensure test data factor has all levels from train, including "Missing_Val"
        if (is.factor(processed_test_data[[col]])) {
            all_train_levels_for_col <- levels(processed_train_data[[col]])
            current_test_levels_for_col <- levels(processed_test_data[[col]])
            missing_levels_in_test_for_col <- setdiff(all_train_levels_for_col, current_test_levels_for_col)
            if (length(missing_levels_in_test_for_col) > 0) {
                levels(processed_test_data[[col]]) <- c(current_test_levels_for_col, missing_levels_in_test_for_col)
            }
        } else { # If it became all NAs or wasn't a factor yet
            processed_test_data[[col]] <- factor(as.character(processed_test_data[[col]]), levels = levels(processed_train_data[[col]]))
        }
        processed_test_data[[col]][is.na(processed_test_data[[col]])] <- "Missing_Val"
      }
    }
  }
}
cat("Missing value imputation complete.\n")

# 5b. One-Hot Encode Categorical Features
potential_predictor_cols_in_processed_train <- setdiff(names(processed_train_data), target_variable)
factor_cols_to_encode <- names(processed_train_data)[sapply(processed_train_data[, potential_predictor_cols_in_processed_train, drop=FALSE], is.factor)]

cols_to_exclude_from_ohe_due_to_cardinality <- c("Grantor", "Grantee") 
cat("Excluding high-cardinality columns from one-hot encoding (if present and factors):", paste(cols_to_exclude_from_ohe_due_to_cardinality, collapse=", "), "\n")
factor_cols_to_encode <- setdiff(factor_cols_to_encode, cols_to_exclude_from_ohe_due_to_cardinality)

if (length(factor_cols_to_encode) > 0) {
  cat("One-hot encoding categorical features:", paste(factor_cols_to_encode, collapse=", "), "\n")
  
  train_data_encoded <- dummy_cols(processed_train_data, 
                                   select_columns = factor_cols_to_encode,
                                   remove_first_dummy = FALSE, 
                                   remove_selected_columns = TRUE,
                                   ignore_na = TRUE) # Important for dummy_cols with NAs handled as "Missing_Val"
  
  test_data_encoded <- dummy_cols(processed_test_data, 
                                  select_columns = factor_cols_to_encode,
                                  remove_first_dummy = FALSE,
                                  remove_selected_columns = TRUE,
                                  ignore_na = TRUE)
  
  # Align columns: Ensure test has same columns as train, in same order
  dummified_feature_names_from_train <- setdiff(names(train_data_encoded), target_variable)

  # Add missing columns to test_data_encoded (those present in train but not test after OHE)
  missing_cols_in_test <- setdiff(dummified_feature_names_from_train, names(test_data_encoded))
  for (col_add in missing_cols_in_test) {
    test_data_encoded[[col_add]] <- 0 # Add as 0, assuming it means the category wasn't present
  }
  
  # Remove extra columns from test_data_encoded (those present in test but not train after OHE)
  # These would typically be levels present in test but not in train for the OHE'd columns
  extra_cols_in_test <- setdiff(names(test_data_encoded), names(train_data_encoded)) 
  if (length(extra_cols_in_test) > 0) {
    test_data_encoded <- test_data_encoded[, !(names(test_data_encoded) %in% extra_cols_in_test), drop = FALSE]
  }
  
  # Ensure test_data_encoded has columns in the same order as train_data_encoded
  # And only includes columns that are in train_data_encoded (excluding target from feature set)
  final_train_features_df <- train_data_encoded %>% select(all_of(dummified_feature_names_from_train))
  final_test_features_df <- test_data_encoded %>% select(all_of(dummified_feature_names_from_train))
  
  train_labels <- train_data_encoded[[target_variable]]
  test_labels <- test_data_encoded[[target_variable]]
  
} else {
  cat("No categorical features to one-hot encode.\n")
  final_train_features_df <- processed_train_data %>% select(-all_of(target_variable))
  final_test_features_df <- processed_test_data %>% select(-all_of(target_variable))
  
  train_labels <- processed_train_data[[target_variable]]
  test_labels <- processed_test_data[[target_variable]]
}
cat("One-hot encoding complete. Number of features after encoding:", ncol(final_train_features_df), "\n")
cat("Dimensions of final_train_features_df:", paste(dim(final_train_features_df), collapse="x"), "\n")
cat("Dimensions of final_test_features_df:", paste(dim(final_test_features_df), collapse="x"), "\n")


# --- 6. Prepare Data for XGBoost ---
cat("\nPreparing data for XGBoost (creating DMatrices)...\n")

# Ensure all columns are numeric before converting to matrix
# This is a safeguard; OHE should produce numeric, and numerics are already numeric.
# Factors that weren't OHE'd (like Grantor/Grantee if kept as predictors) would cause issues.
# However, Grantor/Grantee are excluded from OHE and should also be excluded from selected_predictors if not numeric.
# The current selected_predictors logic should handle this.

# Convert to matrix, ensuring all data is numeric
# Dates and POSIXct are handled as numeric by data.matrix()
train_features_matrix <- data.matrix(final_train_features_df)
test_features_matrix <- data.matrix(final_test_features_df)

# Check for non-numeric columns that might have slipped through (should not happen with current logic)
# if(any(sapply(final_train_features_df, function(x) !is.numeric(x)))) {
#   stop("Non-numeric columns found in final_train_features_df before matrix conversion!")
# }

dtrain <- xgb.DMatrix(data = train_features_matrix, label = train_labels, missing = NA) 
dtest <- xgb.DMatrix(data = test_features_matrix, label = test_labels, missing = NA)
watchlist <- list(train = dtrain, test = dtest)

cat("XGBoost DMatrices created successfully.\n")
cat("Script Part 1 (Data Prep) Complete. Ready for model training.\n")

# --- END OF DATA PREPARATION SCRIPT ---

# The rest of your script (model training, evaluation, PDPs, etc.) would follow from here,
# using dtrain, dtest, final_test_features_df, test_labels, etc.
```

```{r}
# --- End of Part 1: Data Preparation ---
# XGBoost DMatrices created successfully.
# Script Part 1 (Data Prep) Complete. Ready for model training.

# --- Script Part 2: Model Training, Prediction, and Evaluation ---
cat("\n\n--- Starting Script Part 2: Model Training and Evaluation ---\n")

# --- 6. Define XGBoost Parameters and Train Model ---
cat("\nDefining XGBoost parameters...\n")

# Basic XGBoost parameters - these can be tuned further using xgb.cv
params <- list(
  objective = "reg:squarederror", # For regression, predicts the value itself
  eval_metric = "rmse",           # Root Mean Squared Error for evaluation
  eta = 0.05,                     # Learning rate (typically 0.01-0.2)
  max_depth = 6,                  # Maximum depth of a tree (typically 3-10)
  subsample = 0.8,                # Subsample ratio of the training instance
  colsample_bytree = 0.8,         # Subsample ratio of columns when constructing each tree
  min_child_weight = 1,           # Minimum sum of instance weight needed in a child
  gamma = 0                       # Minimum loss reduction required to make a further partition
  # seed = 123                    # Already set globally, but can be set here too
)

# Number of training rounds
# Consider using early stopping to find the optimal number of rounds
nrounds <- 500 # Initial number of rounds, can be adjusted

cat("Starting XGBoost model training...\n")
cat("Parameters:\n")
print(params)
cat("Number of rounds:", nrounds, "\n")

# Set up a watchlist for monitoring performance on training and test sets
watchlist <- list(train = dtrain, test = dtest)

# Optional: Enable parallel processing for faster training (if your machine has multiple cores)
# n_cores <- detectCores() - 1 # Leave one core free
# if (n_cores < 1) n_cores <- 1
# cat("Using", n_cores, "cores for training if supported by XGBoost version.\n")
# params$nthread <- n_cores # Some XGBoost versions use this, others detect automatically

xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = nrounds,
  watchlist = watchlist,
  early_stopping_rounds = 50, # Stop if performance on test set doesn't improve for 50 rounds
  print_every_n = 50,         # Print progress every 50 rounds
  verbose = 1                 # 1 for printing evaluation output, 0 for silent
)

cat("\nModel training complete.\n")
cat("Best iteration (if early stopping occurred):", xgb_model$best_iteration, "\n")
cat("Best RMSE on test set during training:", xgb_model$best_score, "\n")

# --- 7. Make Predictions on the Test Set ---
cat("\nMaking predictions on the test set...\n")

final_test_features_matrix <- data.matrix(final_test_features_df) 
test_predictions_log <- predict(xgb_model, final_test_features_matrix)

# Retrieve the actual log-transformed target values for the test set
# processed_test_data should still be in the environment from Part 1
# target_variable is also defined (e.g., "Sale_Price_log")
if (!exists("processed_test_data") || !target_variable %in% names(processed_test_data)) {
  if (exists("test_data") && target_variable %in% names(test_data)) {
      cat("Warning: 'processed_test_data' not found, attempting to use 'test_data' for target.\n")
      # Fallback if processed_test_data isn't there but test_data (pre-imputation) is
      # This assumes target column itself wasn't modified by imputation, which is typical
      test_actual_log <- test_data[[target_variable]] 
  } else {
      stop("Error: Neither 'processed_test_data' nor 'test_data' (with target) found. Ensure Part 1 ran correctly.")
  }
} else {
  test_actual_log <- processed_test_data[[target_variable]] # Primary way to get it
}

# Inverse transform predictions and actuals
test_predictions_original <- exp(test_predictions_log)
test_actual_original <- exp(test_actual_log) 

cat("Predictions made.\n")

# --- 8. Evaluate Model Performance ---
cat("\nEvaluating model performance...\n")

# Performance on the original Sale_Price scale
mae_original <- mae(test_actual_original, test_predictions_original)
rmse_original <- rmse(test_actual_original, test_predictions_original)
r_squared_original <- R2(test_actual_original, test_predictions_original, form = "traditional") # Using caret's R2

cat("Performance on Original Sale_Price Scale:\n")
cat("  Mean Absolute Error (MAE): $", format(round(mae_original, 2), big.mark=","), "\n")
cat("  Root Mean Squared Error (RMSE): $", format(round(rmse_original, 2), big.mark=","), "\n")
cat("  R-squared: ", round(r_squared_original, 4), "\n")

# Performance on the log-transformed Sale_Price_log scale (model's direct target)
mae_log <- mae(test_actual_log, test_predictions_log)
rmse_log <- rmse(test_actual_log, test_predictions_log) # Should be close to xgb_model$best_score
r_squared_log <- R2(test_actual_log, test_predictions_log, form = "traditional")

cat("\nPerformance on Log-Transformed Sale_Price_log Scale (model's optimization target):\n")
cat("  Mean Absolute Error (MAE_log): ", round(mae_log, 4), "\n")
cat("  Root Mean Squared Error (RMSE_log): ", round(rmse_log, 4), "\n")
cat("  R-squared (log): ", round(r_squared_log, 4), "\n")


# --- 9. Basic Diagnostic Plots ---
cat("\nGenerating diagnostic plots...\n")

# Create a data frame for plotting
plot_df <- data.frame(
  Actual_Original = test_actual_original,
  Predicted_Original = test_predictions_original,
  Residuals_Original = test_actual_original - test_predictions_original,
  Actual_Log = test_actual_log,
  Predicted_Log = test_predictions_log,
  Residuals_Log = test_actual_log - test_predictions_log
)

# Plot 1: Actual vs. Predicted (Original Scale)
png("Rplot_ActualVsPredicted_Original.png", width=800, height=600)
ggplot(plot_df, aes(x = Actual_Original, y = Predicted_Original)) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed", size = 1) +
  scale_x_continuous(labels = scales::dollar_format()) +
  scale_y_continuous(labels = scales::dollar_format()) +
  labs(
    title = "Actual vs. Predicted Sale Price (Original Scale)",
    x = "Actual Sale Price",
    y = "Predicted Sale Price"
  ) +
  theme_minimal()
dev.off()
cat("Saved Rplot_ActualVsPredicted_Original.png\n")

# Plot 2: Residuals vs. Predicted (Original Scale)
png("Rplot_ResidualsVsPredicted_Original.png", width=800, height=600)
ggplot(plot_df, aes(x = Predicted_Original, y = Residuals_Original)) +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed", size = 1) +
  scale_x_continuous(labels = scales::dollar_format()) +
  scale_y_continuous(labels = scales::dollar_format()) +
  labs(
    title = "Residuals vs. Predicted Sale Price (Original Scale)",
    x = "Predicted Sale Price",
    y = "Residuals (Actual - Predicted)"
  ) +
  theme_minimal()
dev.off()
cat("Saved Rplot_ResidualsVsPredicted_Original.png\n")

# Plot 3: Actual vs. Predicted (Log Scale) - Good for seeing model fit on transformed scale
png("Rplot_ActualVsPredicted_Log.png", width=800, height=600)
ggplot(plot_df, aes(x = Actual_Log, y = Predicted_Log)) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed", size = 1) +
  labs(
    title = "Actual vs. Predicted Sale Price (Log Scale)",
    x = "Actual Sale_Price_log",
    y = "Predicted Sale_Price_log"
  ) +
  theme_minimal()
dev.off()
cat("Saved Rplot_ActualVsPredicted_Log.png\n")

cat("Diagnostic plots generated.\n")

# --- 10. Feature Importance ---
cat("\nCalculating and plotting feature importance...\n")

importance_matrix <- xgb.importance(
  feature_names = colnames(final_train_features_df), # Use column names from the feature matrix
  model = xgb_model
)

cat("Top 20 most important features:\n")
print(head(importance_matrix, 20))

# Plot feature importance (Top 20)
png("Rplot_FeatureImportance.png", width=1000, height=800)
xgb.plot.importance(importance_matrix = importance_matrix, top_n = 20, measure = "Gain")
dev.off()
cat("Saved Rplot_FeatureImportance.png\n")

cat("Feature importance analysis complete.\n")

# --- 11. Further Analysis (Placeholder) ---
cat("\nFurther analysis could include:\n")
cat("- Detailed residual analysis (identifying properties with largest errors).\n")
cat("- Partial Dependence Plots (PDPs) for key features to understand their impact.\n")
cat("- Cross-validation (xgb.cv) for more robust hyperparameter tuning if not done already.\n")
cat("- Trying different model parameters or feature engineering techniques.\n")

cat("\n--- Script Part 2: Model Training and Evaluation Complete ---\n")
```


```{r}
# --- (Previous code from Part 2: Model Training, Evaluation, Plots, Feature Importance) ---

# --- 11. Detailed Residual Analysis ---
cat("\n\n--- Starting Script Part 2: Section 11 - Detailed Residual Analysis ---\n")

# Ensure necessary objects are available from previous parts
if (!exists("plot_df")) {
  stop("Error: plot_df not found. Please ensure model evaluation (Section 9 of Part 2) has run.")
}
if (!exists("processed_test_data")) {
  stop("Error: processed_test_data not found. Please ensure Part 1 (data prep) has run and objects are in scope.")
}
if (!exists("target_variable")) { # Should be "Sale_Price_log"
  stop("Error: target_variable (e.g., 'Sale_Price_log') not found.")
}

# Add absolute error and an original row identifier to plot_df
# The rows in plot_df correspond 1:1 with rows in processed_test_data
if (nrow(plot_df) != nrow(processed_test_data)) {
    stop("Runtime Error: Mismatch in row counts between plot_df and processed_test_data. This should not happen if script ran sequentially.")
}
plot_df$Absolute_Error_Original <- abs(plot_df$Residuals_Original)
# This index helps retrieve corresponding features from processed_test_data
plot_df$Internal_Test_Row_Index <- 1:nrow(plot_df) 

# Sort by absolute error to find largest discrepancies
residuals_df_sorted <- plot_df %>%
  arrange(desc(Absolute_Error_Original))

cat("\nTop properties with largest prediction errors (Original Scale):\n")
n_top_errors <- 10 # You can change this to see more or fewer properties

# --- Define a list of key features to display for these properties ---
# These should be columns present in 'processed_test_data' (i.e., selected_predictors before OHE)
# Adjust this list based on what you find most informative
key_features_to_display <- c(
  "Total_Building_SqFt", "Year_Built", "Property_Age_at_Sale",
  "Total_Bathrooms", "Bedrooms", "Stories",
  "Main_Building_Quality_Average", "Condition",
  "Land_Net_Acres", "Total_Land_SqFt", # Or Max_Land_Net_SqFt if that's the one used
  "Avg_Latitude", "Avg_Longitude", "Sale_Date",
  "Waterfront_Type_Summary", "View_Quality_Summary", "Traffic_Noise_Level_Summary",
  "Instrument_Type", "Improved_Vacant", "Property_Type" # Some of the original categoricals
)
# Filter this list to ensure all chosen features actually exist in processed_test_data
# and are not the target variable itself.
key_features_to_display <- intersect(key_features_to_display, names(processed_test_data))
key_features_to_display <- setdiff(key_features_to_display, target_variable) # target_variable is Sale_Price_log

# Helper function to print property details
print_property_details <- function(rank_label, row_data, ptd_data, features_list) {
  cat("\nRank:", rank_label, "\n")
  internal_idx <- row_data$Internal_Test_Row_Index
  cat("  Internal Test Row Index:", internal_idx, "\n") # Helps locate in processed_test_data if needed
  # If you manage to carry ETN or Major_Minor_Sale_ID into processed_test_data, you could print it here:
  # if ("ETN" %in% names(ptd_data)) { cat("  ETN:", ptd_data[internal_idx, "ETN"], "\n") }

  cat("  Actual Sale Price (Model Target): $", format(round(row_data$Actual_Original, 0), big.mark=","), "\n")
  cat("  Predicted Sale Price: $", format(round(row_data$Predicted_Original, 0), big.mark=","), "\n")
  cat("  Residual (Actual - Predicted): $", format(round(row_data$Residuals_Original, 0), big.mark=","), "\n")
  if ("Absolute_Error_Original" %in% names(row_data)) {
    cat("  Absolute Error: $", format(round(row_data$Absolute_Error_Original, 0), big.mark=","), "\n")
  }
  
  if (internal_idx <= nrow(ptd_data) && length(features_list) > 0) {
    original_features <- ptd_data[internal_idx, features_list, drop = FALSE]
    cat("  Key Features:\n")
    for (col_name in names(original_features)) {
      val <- original_features[[col_name]]
      val_to_print <- if (inherits(val, "POSIXct")) {
        format(val, "%Y-%m-%d")
      } else if (is.numeric(val) && !is.integer(val) && !is.logical(val)) { # Avoid rounding logicals if they sneak in
        round(val, 2)
      } else {
        as.character(val) # Ensure factors/characters are printed
      }
      cat("    ", col_name, ": ", val_to_print, "\n")
    }
  } else if (length(features_list) == 0) {
    cat("  No key features selected for display or none available in processed_test_data.\n")
  } else {
    cat("  Could not retrieve original features for internal index:", internal_idx, "\n")
  }
}

# --- Properties with Largest Overall Absolute Errors ---
cat(paste0("\n--- Top ", n_top_errors, " Largest Absolute Errors ---\n"))
top_n_overall_errors_df <- head(residuals_df_sorted, n_top_errors)
for (i in 1:nrow(top_n_overall_errors_df)) {
  print_property_details(i, top_n_overall_errors_df[i, ], processed_test_data, key_features_to_display)
}

# --- Properties Most Under-Predicted (Actual Price >> Predicted Price) ---
cat(paste0("\n\n--- Top ", n_top_errors, " Most Under-Predicted Properties (Actual >> Predicted) ---\n"))
# Residuals_Original = Actual - Predicted. Large positive residual means under-predicted.
under_predicted_df <- plot_df %>%
  arrange(desc(Residuals_Original)) %>%
  head(n_top_errors)
for (i in 1:nrow(under_predicted_df)) {
  print_property_details(paste(i, "(Under-prediction)"), under_predicted_df[i, ], processed_test_data, key_features_to_display)
}

# --- Properties Most Over-Predicted (Predicted Price >> Actual Price) ---
cat(paste0("\n\n--- Top ", n_top_errors, " Most Over-Predicted Properties (Predicted >> Actual) ---\n"))
# Residuals_Original = Actual - Predicted. Large negative (smallest) residual means over-predicted.
over_predicted_df <- plot_df %>%
  arrange(Residuals_Original) %>% 
  head(n_top_errors)
for (i in 1:nrow(over_predicted_df)) {
  print_property_details(paste(i, "(Over-prediction)"), over_predicted_df[i, ], processed_test_data, key_features_to_display)
}

cat("\n--- Detailed Residual Analysis Complete ---\n")
```































```{r}
# --- New Cleaning Step 1: Filter by Sale_Warning_Code ---
if ("Sale_Warning_Code" %in% names(data_to_clean_further)) {
    cat("\nUnique Sale_Warning_Codes before filtering:\n")
    print(unique(data_to_clean_further$Sale_Warning_Code))
    
    # !!! THIS IS THE LINE TO UPDATE !!!
    codes_to_remove <- c(
      "Com and Res sold together", "CU Open Space & Ag RCW 84.34", "MH Acct Type included in sale",
      "MH & Land Sep ETN", "Foreclosure Sale", "Desig Forest Land RCW 84.33",
      "Improved after sale", "Change in Highest/best use", "Short Sale",
      "Forced Sale Trans in Lieu Frcl", "Family different last names", "Improvement on Leased Land",
      "Auction Sale", "Exempt for taxatn Gov nonprof", "Friends Not listed on market",
      "Current Segregation not apprsd", "Bldg removed after sale", "Valid Land Sale",
      "Misc-Documentation Required", "Subsidized Housing", "Gift love & affection"
      # Add/remove codes here based on your review
    ) 
    
    initial_rows_swc <- nrow(data_to_clean_further)
    data_to_clean_further <- data_to_clean_further %>%
      filter(is.na(Sale_Warning_Code) | !(Sale_Warning_Code %in% codes_to_remove))
    cat("Rows after Sale_Warning_Code filter:", nrow(data_to_clean_further), "\n")
    cat("Rows removed by Sale_Warning_Code filter:", initial_rows_swc - nrow(data_to_clean_further), "\n")
} else {
    cat("Warning: 'Sale_Warning_Code' column not found. Skipping this filter.\n")
}
```




```{r}
# --- (Previous code from Part 2: Model Training, Evaluation, Plots, Feature Importance) ---

# --- 11. Detailed Residual Analysis (Revised Completeness Check) ---
cat("\n\n--- Starting Script Part 2: Section 11 - Detailed Residual Analysis (Revised Completeness Check) ---\n")

# Ensure necessary objects are available from Part 1
if (!exists("plot_df")) stop("Error: plot_df (from model predictions on test set) not found.")
if (!exists("full_data_for_section_11_analysis")) stop("Error: 'full_data_for_section_11_analysis' (snapshot from Part 1) not found.")
if (!exists("train_index")) stop("Error: train_index (row indices for training set within modeling_data) not found.")
if (!exists("processed_test_data")) stop("Error: processed_test_data (test data after recipe) not found.")
if (!exists("target_variable")) stop("Error: target_variable name not found.")
if (!exists("selected_predictors")) stop("Error: selected_predictors list not found.")
if (!exists("modeling_data")) stop("Error: modeling_data (data used for training/testing split, after na.omit) not found.")


data_to_analyze_residuals <- full_data_for_section_11_analysis
cat("Using 'full_data_for_section_11_analysis' for residual detail. Dimensions:", paste(dim(data_to_analyze_residuals), collapse="x"), "\n")
# print(names(data_to_analyze_residuals)) # Already confirmed this is good by previous output

# 1. Define the set of columns that na.omit() acted upon in Part 1.
# These are the target variable and the selected predictors, as they exist in data_to_analyze_residuals.
cols_for_na_omit_check_direct <- c(target_variable, selected_predictors)
cols_for_na_omit_check_direct <- unique(cols_for_na_omit_check_direct)

# Check if all these columns exist in data_to_analyze_residuals
missing_cols_for_check <- cols_for_na_omit_check_direct[!cols_for_na_omit_check_direct %in% names(data_to_analyze_residuals)]
if (length(missing_cols_for_check) > 0) {
  stop(paste("Error: Columns intended for NA check are missing from data_to_analyze_residuals:",
             paste(missing_cols_for_check, collapse=", "),
             "\nThis implies a mismatch between selected_predictors and the snapshot contents, or an issue with target_variable name."))
}
cat("All columns for NA check (", length(cols_for_na_omit_check_direct), "columns) found directly in data_to_analyze_residuals.\n")
# cat("Columns being checked for NAs:", paste(cols_for_na_omit_check_direct, collapse=", "), "\n")


# --- DEBUGGING STEP: Check NA counts in data_to_analyze_residuals for these specific columns ---
cat("\n--- DEBUG: NA counts in data_to_analyze_residuals for columns in cols_for_na_omit_check_direct ---\n")
na_counts_debug_direct <- sapply(data_to_analyze_residuals[, cols_for_na_omit_check_direct, drop = FALSE], function(x) sum(is.na(x)))
print(sort(na_counts_debug_direct, decreasing = TRUE)) # Sort to see worst offenders first

if(any(na_counts_debug_direct > 0)) {
    cat("\nColumns with NAs (count > 0):\n")
    print(na_counts_debug_direct[na_counts_debug_direct > 0])
} else {
    cat("\nNo NAs found in any columns listed in cols_for_na_omit_check_direct within data_to_analyze_residuals.\n")
}
cat("Total rows in data_to_analyze_residuals (before na.omit replication):", nrow(data_to_analyze_residuals), "\n")
cat("--- END DEBUG ---\n")


# 2. Identify rows in data_to_analyze_residuals that are complete for these columns.
# This directly mimics what na.omit(modeling_data_subsetted_by_predictors_and_target) did in Part 1.
complete_case_indices_in_snapshot <- which(complete.cases(data_to_analyze_residuals[, cols_for_na_omit_check_direct, drop = FALSE]))
cat("Number of rows found to be complete (complete_case_indices_in_snapshot):", length(complete_case_indices_in_snapshot), "\n")


# 3. Create data_filtered_complete_cases by subsetting data_to_analyze_residuals using these indices.
if (length(complete_case_indices_in_snapshot) > 0) {
    data_filtered_complete_cases <- data_to_analyze_residuals[complete_case_indices_in_snapshot, , drop = FALSE]
} else {
    # Create empty df with correct columns if no complete cases found
    data_filtered_complete_cases <- data_to_analyze_residuals[0, , drop = FALSE]
}
cat("Number of rows in data_filtered_complete_cases (representing data after na.omit):", nrow(data_filtered_complete_cases), "\n")

if (nrow(data_filtered_complete_cases) == 0 && nrow(data_to_analyze_residuals) > 0) {
    cat("CRITICAL WARNING: No complete cases found. This means every row in data_to_analyze_residuals has at least one NA in the columns listed for checking.\n")
    cat("This would imply modeling_data (after na.omit) in Part 1 would also have been empty, which is unlikely if the model trained.\n")
    cat("Please carefully review the NA counts printed above. Check how NAs were handled for these columns in Part 1 before 'modeling_data' was created and na.omit was applied.\n")
    cat("The columns checked were:", paste(cols_for_na_omit_check_direct, collapse=", "), "\n")
    # Consider stopping if this is unexpected: stop("Halting due to no complete cases found for residual analysis.")
}


# 4. Sanity Check: Compare with modeling_data row count from Part 1
# modeling_data in Part 1 is the result AFTER na.omit.
cat("\nRow count of 'modeling_data' (from Part 1, after na.omit):", nrow(modeling_data), "\n")
if (nrow(data_filtered_complete_cases) != nrow(modeling_data)) {
    cat("WARNING: Row count of data_filtered_complete_cases (", nrow(data_filtered_complete_cases),
        ") does NOT match modeling_data (", nrow(modeling_data), "). This indicates an issue in replicating the na.omit step from Part 1.\n")
} else {
    cat("SUCCESS: Row count of data_filtered_complete_cases matches modeling_data. The na.omit replication seems correct.\n")
}

if (nrow(data_filtered_complete_cases) == 0) {
    stop("Cannot proceed with residual analysis as data_filtered_complete_cases is empty. Review NA counts and Part 1 logic.")
}

# 5. Identify the original indices of the test set within these complete cases
# train_index contains the row numbers from modeling_data (which corresponds to data_filtered_complete_cases) that went into the training set.
test_indices_in_complete_cases <- setdiff(1:nrow(data_filtered_complete_cases), train_index)
data_filtered_test_set_rows <- data_filtered_complete_cases[test_indices_in_complete_cases, , drop = FALSE]
cat("\nNumber of rows in data_filtered_test_set_rows (original features for test set):", nrow(data_filtered_test_set_rows), "\n")


# 6. Sanity Check: Compare with processed_test_data row count
# processed_test_data is the test portion of modeling_data, after the recipe was applied.
# Its row count should match data_filtered_test_set_rows.
cat("Row count of 'processed_test_data' (from recipe in Part 1, test split):", nrow(processed_test_data), "\n")
if (nrow(data_filtered_test_set_rows) != nrow(processed_test_data)) {
    cat("CRITICAL WARNING: Row count of data_filtered_test_set_rows (", nrow(data_filtered_test_set_rows),
        ") does NOT match processed_test_data (", nrow(processed_test_data),
        "). This indicates a major issue in identifying the correct test set rows for detailed analysis.\n")
} else {
    cat("SUCCESS: Row count of data_filtered_test_set_rows matches processed_test_data.\n")
}


# 7. Final Check before merging for error analysis
# plot_df contains predictions and actuals for the test set. It should align row-wise with data_filtered_test_set_rows.
if (nrow(plot_df) != nrow(data_filtered_test_set_rows)) {
    stop(paste("Row mismatch before final merge: plot_df has", nrow(plot_df),
               "rows, but data_filtered_test_set_rows has", nrow(data_filtered_test_set_rows),
               "rows. Cannot merge for detailed error analysis."))
}
cat("Row counts of plot_df and data_filtered_test_set_rows match. Ready for merging.\n")


# 8. Combine original features with prediction results
error_analysis_df <- bind_cols(
  data_filtered_test_set_rows, # Original features for the test set
  plot_df                      # Contains actual_original, predicted_original, residual_original, etc.
)

cat("\nSuccessfully created 'error_analysis_df' with", ncol(error_analysis_df), "columns and", nrow(error_analysis_df), "rows (test set observations).\n")
cat("Head of error_analysis_df:\n")
print(head(error_analysis_df))

# --- The rest of Section 11 (sorting by error, displaying top errors, etc.) ---
# This part should be the same as the version you had that was working after the "CRITICAL Error: Row count mismatch after correction" was fixed.
# It starts with adding Absolute_Error_Original to plot_df (though it's now in error_analysis_df)
# and then the print_property_details_v2 function and the loops for top errors.

# Ensure Absolute_Error_Original is in error_analysis_df (it should come from plot_df)
if (!"Absolute_Error_Original" %in% names(error_analysis_df)) {
    if ("Residuals_Original" %in% names(error_analysis_df)) {
        error_analysis_df$Absolute_Error_Original <- abs(error_analysis_df$Residuals_Original)
        cat("Added 'Absolute_Error_Original' to error_analysis_df.\n")
    } else {
        stop("Cannot calculate Absolute_Error_Original as Residuals_Original is missing from error_analysis_df.")
    }
}

# Ensure Internal_Test_Row_Index is correctly 1 to N for mapping if needed,
# though direct row correspondence with data_filtered_test_set_rows is now primary.
# The error_analysis_df is already aligned.

# Sort by absolute error to find largest discrepancies
residuals_df_sorted_final <- error_analysis_df %>%
  arrange(desc(Absolute_Error_Original))

cat("\nTop properties with largest prediction errors (Original Scale), showing more features:\n")
n_top_errors <- 10 # You can change this

# --- Define an updated list of key features to display ---
# These are columns from data_filtered_test_set_rows (which is now part of error_analysis_df)
key_features_to_display_updated <- c(
  "ETN", #"Major_Minor_Sale_ID", # Assuming Major_Minor_Sale_ID is not present
  "Sale_Price", # This is the original Sale_Price from before log transform and capping, from the snapshot
  "Sale_Date",
  "Sale_Warning_Code",
  "Total_Building_SqFt", "Year_Built", "Property_Age_at_Sale",
  # "Condition", # Assuming 'Condition' is not a direct column name, check your actual columns
  "Main_Building_Quality", # Changed from Main_Building_Quality_Average
  "Total_Bathrooms", "Total_Bedrooms", "Total_Stories", # Changed 'Bedrooms' to 'Total_Bedrooms'
  "Max_Land_Net_Acres", # Changed from Land_Net_Acres
  # "Total_Land_SqFt", # This might be Max_Land_Net_SqFt or similar, check your columns
  "Max_Land_Net_SqFt",
  "Avg_Latitude", "Avg_Longitude",
  "Waterfront_Type_Summary", "View_Quality_Summary", #"Traffic_Noise_Level_Summary", # Assuming not present
  "Instrument_Type", "Improved_Vacant", "Property_Type"
)

# Filter this list to only include columns actually present in error_analysis_df
key_features_to_display_updated <- intersect(key_features_to_display_updated, names(error_analysis_df))
cat("Will display the following key features (if found in error_analysis_df):\n")
print(key_features_to_display_updated)


# Helper function to print property details (modified to use error_analysis_df directly)
print_property_details_v3 <- function(rank_label, single_error_row_data, features_list) {
  cat("\nRank:", rank_label, "\n")
  # single_error_row_data is one row from the sorted error_analysis_df

  cat("  Actual Sale Price (from plot_df, Capped if applicable): $", format(round(single_error_row_data$Actual_Original, 0), big.mark=","), "\n")
  cat("  Predicted Sale Price (from plot_df): $", format(round(single_error_row_data$Predicted_Original, 0), big.mark=","), "\n")
  cat("  Residual (Actual - Predicted, from plot_df): $", format(round(single_error_row_data$Residuals_Original, 0), big.mark=","), "\n")
  cat("  Absolute Error (from plot_df): $", format(round(single_error_row_data$Absolute_Error_Original, 0), big.mark=","), "\n")
  
  if (length(features_list) > 0) {
    cat("  Key Original Features:\n")
    for (col_name in features_list) { # Iterate through the filtered list of features to display
      if (col_name %in% names(single_error_row_data)) {
        val <- single_error_row_data[[col_name]]
        val_to_print <- if (inherits(val, "POSIXct") || inherits(val, "Date")) {
          format(val, "%Y-%m-%d")
        } else if (is.numeric(val) && !is.integer(val) && !is.logical(val)) {
          tryCatch({round(val, 2)}, error = function(e) {as.character(val)})
        } else {
          as.character(val)
        }
        cat("    ", col_name, ": ", val_to_print, "\n")
      }
    }
  } else {
    cat("  No key features selected for display or none available in error_analysis_df.\n")
  }
}

# --- Properties with Largest Overall Absolute Errors ---
cat(paste0("\n--- Top ", n_top_errors, " Largest Absolute Errors (using error_analysis_df) ---\n"))
top_n_overall_errors_df_final <- head(residuals_df_sorted_final, n_top_errors)
for (i in 1:nrow(top_n_overall_errors_df_final)) {
  print_property_details_v3(i, top_n_overall_errors_df_final[i, ], key_features_to_display_updated)
}

# --- Properties Most Under-Predicted ---
cat(paste0("\n\n--- Top ", n_top_errors, " Most Under-Predicted Properties (using error_analysis_df) ---\n"))
under_predicted_df_final <- error_analysis_df %>%
  arrange(desc(Residuals_Original)) %>% # Residuals_Original is positive for under-predictions
  head(n_top_errors)
for (i in 1:nrow(under_predicted_df_final)) {
  print_property_details_v3(paste(i, "(Under-prediction)"), under_predicted_df_final[i, ], key_features_to_display_updated)
}

# --- Properties Most Over-Predicted ---
cat(paste0("\n\n--- Top ", n_top_errors, " Most Over-Predicted Properties (using error_analysis_df) ---\n"))
over_predicted_df_final <- error_analysis_df %>%
  arrange(Residuals_Original) %>% # Residuals_Original is negative for over-predictions
  head(n_top_errors)
for (i in 1:nrow(over_predicted_df_final)) {
  print_property_details_v3(paste(i, "(Over-prediction)"), over_predicted_df_final[i, ], key_features_to_display_updated)
}

cat("\n--- Detailed Residual Analysis Complete ---\n")
```


```{r}
# --- 0. Load Libraries and Setup ---
cat("Loading libraries...\n")
# Ensure all necessary packages are installed
# install.packages(c("arrow", "dplyr", "lubridate", "ggplot2", "fastDummies", "xgboost", "Metrics", "pdp", "doParallel", "scales", "caret"))

library(arrow)
library(dplyr)
library(lubridate)
library(ggplot2)
library(fastDummies)
library(xgboost)
library(Metrics) # For MAE, RMSE
library(pdp)     # For Partial Dependence Plots
library(doParallel) # For parallel processing
library(scales)     # For plot axis formatting
library(caret)      # For R2 function and other utilities
library(Ckmeans.1d.dp) # Load it as well

# Set a seed for reproducibility
set.seed(123)

# --- Script Part 1: Data Preparation ---
cat("\n\n--- Starting Script Part 1: Data Preparation ---\n")

# --- 1. Load Data ---
cat("Loading data...\n")
full_data_path <- "../data/stage/full_data_for_modeling.parquet" 

full_data <- tryCatch({
  read_parquet(full_data_path)
}, error = function(e) {
  cat("Error loading", full_data_path, ":", e$message, "\n")
  cat("Please ensure the path is correct and the file exists.\n")
  return(NULL)
})

if (is.null(full_data) || nrow(full_data) == 0) {
  stop("Halting script: Data loading failed or data is empty.")
}
cat("Data loaded. Initial dimensions:", paste(dim(full_data), collapse="x"), "\n")

# --- 2. Initial Filtering and Data Cleaning ---
cat("Starting initial filtering and data cleaning...\n")

# 2a. Filter for Specific Housing Types
housing_property_types_to_keep <- c("Residential", "Condominium")
if ("Property_Type" %in% names(full_data)) {
  full_data <- full_data %>%
    filter(Property_Type %in% housing_property_types_to_keep)
  cat("Data dimensions after housing type filter:", paste(dim(full_data), collapse="x"), "\n")
} else {
  cat("Warning: 'Property_Type' column not found. Skipping housing type filter.\n")
}
if (nrow(full_data) == 0) stop("No data remaining after filtering for housing types.")

data_to_clean_further <- full_data 
cat("\nStarting detailed cleaning based on outlier investigation...\n")
cat("Initial rows for detailed cleaning:", nrow(data_to_clean_further), "\n")

# --- New Cleaning Step 1: Filter by Sale_Warning_Code ---
if ("Sale_Warning_Code" %in% names(data_to_clean_further)) {
    codes_to_remove <- c(
      "Com and Res sold together", "CU Open Space & Ag RCW 84.34", "MH Acct Type included in sale",
      "MH & Land Sep ETN", "Foreclosure Sale", "Desig Forest Land RCW 84.33", "Improved after sale",
      "Change in Highest/best use", "Short Sale", "Forced Sale Trans in Lieu Frcl", "Family different last names",    
      "Improvement on Leased Land", "Auction Sale", "Exempt for taxatn Gov nonprof", "Friends Not listed on market",   
      "Current Segregation not apprsd", "Bldg removed after sale", "Valid Land Sale", "Misc-Documentation Required",    
      "Subsidized Housing", "Gift love & affection"
    )    
    initial_rows_swc <- nrow(data_to_clean_further)
    data_to_clean_further <- data_to_clean_further %>%
      filter(is.na(Sale_Warning_Code) | !(Sale_Warning_Code %in% codes_to_remove))
    cat("Rows after Sale_Warning_Code filter:", nrow(data_to_clean_further), "\n")
}

# --- New Cleaning Step 2: Filter by Parcel_Count ---
if ("Parcel_Count" %in% names(data_to_clean_further)) {
    initial_rows_pc <- nrow(data_to_clean_further)
    data_to_clean_further <- data_to_clean_further %>%
      filter(Parcel_Count == 1 | is.na(Parcel_Count))
    cat("Rows after Parcel_Count filter:", nrow(data_to_clean_further), "\n")
}

# --- New Cleaning Step 3: Filter by Valid_Invalid and Confirmed_Unconfirmed ---
if ("Valid_Invalid" %in% names(data_to_clean_further)) {
    initial_rows_vi <- nrow(data_to_clean_further)
    data_to_clean_further <- data_to_clean_further %>%
      filter(Valid_Invalid == "Valid" | is.na(Valid_Invalid))
    cat("Rows after Valid_Invalid filter:", nrow(data_to_clean_further), "\n")
}
if ("Confirmed_Unconfirmed" %in% names(data_to_clean_further)) {
    initial_rows_cu <- nrow(data_to_clean_further)
    data_to_clean_further <- data_to_clean_further %>%
      filter(Confirmed_Unconfirmed == 1 | is.na(Confirmed_Unconfirmed)) 
    cat("Rows after Confirmed_Unconfirmed filter:", nrow(data_to_clean_further), "\n")
}
full_data <- data_to_clean_further
cat("\nData dimensions after detailed cleaning filters:", paste(dim(full_data), collapse="x"), "\n")
if (nrow(full_data) == 0) stop("No data remaining after initial detailed cleaning.")

# --- 2b. Sale_Date Cleaning and Feature Engineering (REVISED) ---
date_col_name <- "Sale_Date" 
if (date_col_name %in% names(full_data)) {
  cat(paste("\nProcessing '", date_col_name, "'...\n", sep=""))
  if (is.character(full_data[[date_col_name]])) {
    full_data[[date_col_name]] <- ymd(full_data[[date_col_name]], quiet = TRUE)
  } else if (!inherits(full_data[[date_col_name]], "Date") && !inherits(full_data[[date_col_name]], "POSIXct")) {
     full_data[[date_col_name]] <- as.Date(full_data[[date_col_name]]) # Attempt conversion
  }
  
  # DEBUG CHECK for Sale_Date
  if (!inherits(full_data[[date_col_name]], "Date") && !inherits(full_data[[date_col_name]], "POSIXct")) {
    cat("CRITICAL WARNING: ", date_col_name, " is NOT a Date/POSIXct object after parsing attempts. Class:", class(full_data[[date_col_name]]), "\n")
  } else {
    cat("Class of ", date_col_name, " after parsing:", class(full_data[[date_col_name]]), ". NAs:", sum(is.na(full_data[[date_col_name]])), "\n")
  }

  current_date <- Sys.Date() 
  full_data <- full_data %>% filter(get(date_col_name) <= current_date | is.na(get(date_col_name))) # Keep NAs for now
  
  full_data <- full_data %>%
    mutate(
      Sale_Year = as.numeric(year(get(date_col_name))), 
      Sale_Month = factor(month(get(date_col_name), label = TRUE, abbr = FALSE))
    )
  if ("Year_Built" %in% names(full_data) && "Sale_Year" %in% names(full_data)) {
      year_built_median_impute <- median(full_data$Year_Built[full_data$Year_Built > 1800 & full_data$Year_Built <= full_data$Sale_Year & !is.na(full_data$Year_Built)], na.rm = TRUE)
      if(is.na(year_built_median_impute) || !is.finite(year_built_median_impute)) year_built_median_impute <- 2000 
      full_data$Year_Built_Mode <- ifelse(is.na(full_data$Year_Built) | full_data$Year_Built <= 1800 | full_data$Year_Built > full_data$Sale_Year, 
                                          year_built_median_impute, full_data$Year_Built)
      full_data <- full_data %>%
        mutate(Property_Age_at_Sale = Sale_Year - Year_Built_Mode) %>%
        select(-Year_Built_Mode) 
      full_data <- full_data %>% filter(Property_Age_at_Sale >= 0 | is.na(Property_Age_at_Sale))
  }
}

# Process Latest_Merge_Event_Date similarly if it exists and is a date string
merge_date_col_name <- "Latest_Merge_Event_Date"
if (merge_date_col_name %in% names(full_data) && (is.character(full_data[[merge_date_col_name]]) || !inherits(full_data[[merge_date_col_name]], "Date"))) {
  cat(paste("\nProcessing '", merge_date_col_name, "'...\n", sep=""))
  if (is.character(full_data[[merge_date_col_name]])) {
    full_data[[merge_date_col_name]] <- ymd(full_data[[merge_date_col_name]], quiet = TRUE)
  } else if (!inherits(full_data[[merge_date_col_name]], "Date") && !inherits(full_data[[merge_date_col_name]], "POSIXct")) {
     full_data[[merge_date_col_name]] <- as.Date(full_data[[merge_date_col_name]])
  }
  # DEBUG CHECK for Latest_Merge_Event_Date
  if (!inherits(full_data[[merge_date_col_name]], "Date") && !inherits(full_data[[merge_date_col_name]], "POSIXct")) {
    cat("CRITICAL WARNING: ", merge_date_col_name, " is NOT a Date/POSIXct object after parsing attempts. Class:", class(full_data[[merge_date_col_name]]), "\n")
  } else {
    cat("Class of ", merge_date_col_name, " after parsing:", class(full_data[[merge_date_col_name]]), ". NAs:", sum(is.na(full_data[[merge_date_col_name]])), "\n")
  }
}


# --- New Cleaning Step 4: Address 0 Bed/Bath properties ---
required_cols_0bedbath <- c("Total_Bedrooms", "Total_Bathrooms", "Improved_Vacant", "Sale_Price", "Total_Building_SqFt")
if (all(required_cols_0bedbath %in% names(full_data))) {
    full_data <- full_data %>%
      filter(!((Total_Bedrooms == 0 | Total_Bathrooms == 0) & Improved_Vacant == "Improved"))
}

# 2c. Sale_Price Outlier Treatment
if (!"Sale_Price" %in% names(full_data)) stop("Critical Error: 'Sale_Price' column not found.")
if (!is.numeric(full_data$Sale_Price)) full_data$Sale_Price <- as.numeric(as.character(full_data$Sale_Price))
full_data <- full_data %>% filter(!is.na(Sale_Price)) 
if (nrow(full_data) == 0) stop("No data remaining after removing NA Sale_Prices.")
price_floor_threshold <- 10000 
full_data <- full_data %>% filter(Sale_Price >= price_floor_threshold)
if (nrow(full_data) == 0) stop("No data remaining after filtering low sale prices.")
price_cap_threshold <- quantile(full_data$Sale_Price, 0.995, na.rm = TRUE)
full_data <- full_data %>%
  mutate(Sale_Price = ifelse(Sale_Price > price_cap_threshold, price_cap_threshold, Sale_Price))

# 2d. Create Target Variable (Log Transformed)
full_data <- full_data %>% mutate(Sale_Price_log = log(Sale_Price))

# --- 2e. Ensure Correct Numeric Types for Key Columns (NEW SECTION) ---
cat("\n--- Ensuring correct numeric types for key columns ---\n")
cols_to_ensure_numeric <- c(
  "Max_Land_Net_SqFt", "Total_Stories", "Total_Building_SqFt", 
  "Total_Net_SqFt", "Total_Attached_Garage_SqFt", "Total_Bedrooms", 
  "Total_Bathrooms", "Year_Built", "Property_Age_at_Sale",
  "View_Score" # If View_Score is intended to be numeric
  # Add any other columns that are conceptually numeric but might be character/factor
)

for (col_num_check in cols_to_ensure_numeric) {
  if (col_num_check %in% names(full_data)) {
    if (!is.numeric(full_data[[col_num_check]])) {
      cat("Column '", col_num_check, "' is not numeric (Class: ", class(full_data[[col_num_check]]), "). Attempting conversion to numeric.\n")
      original_na_count <- sum(is.na(full_data[[col_num_check]]))
      # Convert to character first, then to numeric, to handle factors properly
      full_data[[col_num_check]] <- suppressWarnings(as.numeric(as.character(full_data[[col_num_check]])))
      new_na_count <- sum(is.na(full_data[[col_num_check]]))
      if (new_na_count > original_na_count) {
        cat("  NAs introduced/exposed by conversion for '", col_num_check, "': ", new_na_count - original_na_count, "\n")
      }
      if (all(is.na(full_data[[col_num_check]]))) {
          cat("  WARNING: Column '", col_num_check, "' became all NAs after conversion.\n")
      }
    }
  }
}
cat("Data dimensions after all cleaning and Sale_Price processing:", paste(dim(full_data), collapse="x"), "\n")


# --- 3. Define Roles and Select Features ---
cat("\nDefining roles and selecting features...\n")
target_variable <- "Sale_Price_log"

# REVISED id_variables to include Tax_Summary_Tax_Year
id_variables <- c(
  "Parcel_Number", "Most_Recent_Sale_Date", "Sale_Price", "Sale_Year", 
  "ParcelID", "TaxAccountNbr", "ETN", "Grantor", "Grantee",                                      
  "Parcel_Count", "Sale_Warning_Code", "Valid_Invalid", "Confirmed_Unconfirmed",
  "Tax_Summary_Tax_Year"  # <<< ADDED HERE TO EXCLUDE IT FROM PREDICTORS
) 

redundant_variables <- c(
  "Appraisal_Account_Type_Summary"
  # Add other known redundant or non-predictive columns here if identified
)

all_cols <- names(full_data)
potential_predictors <- setdiff(all_cols, c(target_variable, id_variables, redundant_variables))

# Ensure selected_predictors only contains columns actually present in full_data
selected_predictors <- intersect(potential_predictors, names(full_data)) 

cat("Final selected predictors (", length(selected_predictors), "):\n", 
    paste(head(selected_predictors, 20), collapse=", "), 
    if(length(selected_predictors) > 20) "...\n" else "\n")

# --- Sanity Check: Ensure Tax_Summary_Tax_Year is NOT in selected_predictors ---
if ("Tax_Summary_Tax_Year" %in% selected_predictors) {
  cat("WARNING: Tax_Summary_Tax_Year is still in selected_predictors. Check id_variables.\n")
  selected_predictors <- setdiff(selected_predictors, "Tax_Summary_Tax_Year")
  cat("Corrected: Tax_Summary_Tax_Year removed from selected_predictors.\n")
} else {
  cat("Confirmed: Tax_Summary_Tax_Year is NOT in selected_predictors.\n")
}

# --- 4. Create Base Modeling Data & Impute (NEW PLACEMENT OF IMPUTATION) ---
cat("\nCreating base data for modeling and applying imputation universally...\n")
# Select only the target and chosen predictors for the base modeling dataset
data_for_modeling_imputed <- full_data[, c(target_variable, selected_predictors), drop = FALSE]
cat("Dimensions of data_for_modeling_imputed (before imputation):", paste(dim(data_for_modeling_imputed), collapse="x"), "\n")

imputation_values_universal <- list()
predictors_for_universal_imputation <- selected_predictors # Target is already handled (no NAs expected after filters)

for (col in predictors_for_universal_imputation) {
  if (col %in% names(data_for_modeling_imputed)) { 
    if (is.numeric(data_for_modeling_imputed[[col]]) || inherits(data_for_modeling_imputed[[col]], "Date") || inherits(data_for_modeling_imputed[[col]], "POSIXct")) { # Corrected: Date/POSIXct are numeric-like for imputation
      median_val <- median(as.numeric(data_for_modeling_imputed[[col]]), na.rm = TRUE) # Convert Date/POSIXct to numeric for median
      imputation_values_universal[[col]] <- ifelse(is.na(median_val) || !is.finite(median_val), 0, median_val) 
      # For Date/POSIXct, impute with a Date/POSIXct object if possible, or as numeric if that's the target type
      if (inherits(data_for_modeling_imputed[[col]], "Date")) {
          data_for_modeling_imputed[[col]][is.na(data_for_modeling_imputed[[col]])] <- as.Date(imputation_values_universal[[col]], origin = "1970-01-01")
      } else if (inherits(data_for_modeling_imputed[[col]], "POSIXct")) {
          data_for_modeling_imputed[[col]][is.na(data_for_modeling_imputed[[col]])] <- as.POSIXct(imputation_values_universal[[col]], origin = "1970-01-01")
      } else { # Numeric
          data_for_modeling_imputed[[col]][is.na(data_for_modeling_imputed[[col]])] <- imputation_values_universal[[col]]
      }
    } else if (is.character(data_for_modeling_imputed[[col]]) || is.factor(data_for_modeling_imputed[[col]])) {
      if (is.character(data_for_modeling_imputed[[col]])) {
        data_for_modeling_imputed[[col]] <- factor(data_for_modeling_imputed[[col]])
      }
      current_levels <- levels(data_for_modeling_imputed[[col]])
      if (!("Missing_Val" %in% current_levels)) {
        levels(data_for_modeling_imputed[[col]]) <- c(current_levels, "Missing_Val")
      }
      data_for_modeling_imputed[[col]][is.na(data_for_modeling_imputed[[col]])] <- "Missing_Val"
      imputation_values_universal[[col]] <- "Missing_Val" 
    }
  }
}
cat("Universal missing value imputation complete on data_for_modeling_imputed.\n")
cat("Dimensions of data_for_modeling_imputed (after imputation):", paste(dim(data_for_modeling_imputed), collapse="x"), "\n")

# --- Check for NAs after universal imputation ---
na_counts_after_universal_impute <- sapply(data_for_modeling_imputed, function(x) sum(is.na(x)))
if(any(na_counts_after_universal_impute > 0)) {
    cat("WARNING: NAs still present in data_for_modeling_imputed after universal imputation:\n")
    print(na_counts_after_universal_impute[na_counts_after_universal_impute > 0])
    stop("Halting due to NAs remaining after universal imputation. Check imputation logic for listed columns.")
} else {
    cat("Confirmed: No NAs in data_for_modeling_imputed after universal imputation.\n")
}


# --- TAKE SNAPSHOT FOR SECTION 11 ANALYSIS (CRITICAL PLACEMENT) ---
full_data_for_section_11_analysis <- data_for_modeling_imputed 
cat("Saved 'full_data_for_section_11_analysis' (snapshot of imputed data). Dimensions:", paste(dim(full_data_for_section_11_analysis), collapse="x"), "\n")

modeling_data <- data_for_modeling_imputed 
cat("Dimensions of modeling_data (imputed, ready for split):", paste(dim(modeling_data), collapse="x"), "\n")


# --- 5. Split Data into Training and Testing Sets ---
cat("\nSplitting data into training (80%) and testing (20%) sets...\n")
if(any(is.na(modeling_data[[target_variable]]))) {
    stop("NAs found in target variable before splitting. This should not happen.")
}
train_index <- createDataPartition(modeling_data[[target_variable]], p = 0.8, list = FALSE, times = 1)
train_data <- modeling_data[train_index, ]
test_data <- modeling_data[-train_index, ]

cat("Training set dimensions:", paste(dim(train_data), collapse="x"), "\n")
cat("Testing set dimensions:", paste(dim(test_data), collapse="x"), "\n")
if (nrow(train_data) < 10 || nrow(test_data) < 10) stop("Insufficient data in training or testing set.")

# --- Place this BEFORE the "--- 6. One-Hot Encode Categorical Features (REVISED) ---" block ---
# Ensure Total_Stories and View_Score are numeric
cat("\nEnsuring Total_Stories and View_Score are numeric...\n")
if ("Total_Stories" %in% names(processed_train_data)) {
  processed_train_data$Total_Stories <- as.numeric(as.character(processed_train_data$Total_Stories)) # as.character first if it's a factor
  processed_test_data$Total_Stories <- as.numeric(as.character(processed_test_data$Total_Stories))
  cat("Converted Total_Stories to numeric.\n")
} else {
  cat("Warning: Total_Stories column not found in processed_train_data.\n")
}

if ("View_Score" %in% names(processed_train_data)) {
  processed_train_data$View_Score <- as.numeric(as.character(processed_train_data$View_Score)) # as.character first if it's a factor
  processed_test_data$View_Score <- as.numeric(as.character(processed_test_data$View_Score))
  cat("Converted View_Score to numeric.\n")
} else {
  cat("Warning: View_Score column not found in processed_train_data.\n")
}
# --- End of new numeric conversion block ---

# --- 6. One-Hot Encode Categorical Features (REVISED) ---
cat("\nStarting One-Hot Encoding...\n")
processed_train_data <- train_data
processed_test_data <- test_data

potential_predictor_cols_in_processed_train <- setdiff(names(processed_train_data), target_variable)
# Identify factors, EXCLUDING Date/POSIXct which should be numeric by now for XGBoost
factor_cols_to_encode <- names(processed_train_data)[sapply(processed_train_data[, potential_predictor_cols_in_processed_train, drop=FALSE], function(x) is.factor(x) && !inherits(x, "Date") && !inherits(x, "POSIXct"))]


# --- DEBUG: Inspecting columns selected for One-Hot Encoding ---
cat("\n--- DEBUG: Inspecting columns initially identified as factors for OHE ---\n")
if (length(factor_cols_to_encode) > 0) {
  for (col_name_debug in factor_cols_to_encode) {
    if (col_name_debug %in% names(processed_train_data)) {
      cat("Column:", col_name_debug, "\n")
      cat("  Class:", class(processed_train_data[[col_name_debug]]), "\n")
      num_unique_debug <- length(unique(processed_train_data[[col_name_debug]]))
      cat("  Number of unique values:", num_unique_debug, "\n")
      if (is.factor(processed_train_data[[col_name_debug]])) {
        cat("  Number of levels:", nlevels(processed_train_data[[col_name_debug]]), "\n")
      }
      cat("  First 5 unique values:", paste(head(unique(as.character(processed_train_data[[col_name_debug]])), 5), collapse=", "), "\n\n")
    }
  }
} else {
  cat("No columns initially identified as factors for OHE.\n")
}
cat("--- END DEBUG ---\n")

# Exclude known high-cardinality columns and columns that should be numeric
cols_to_manually_exclude_from_ohe <- c(
  "Neighborhood_Summary",      # Known high cardinality
  "Sale_Date",                 # Should be numeric (e.g., Sale_Date_numeric)
  "Max_Land_Net_SqFt",         # Should be numeric
  "Latest_Merge_Event_Date",   # Should be numeric (e.g., Latest_Merge_Event_Date_numeric)
  "Grantor", "Grantee",        # High cardinality, already excluded
  "Total_Stories",             # Explicitly exclude, ensure it's numeric
  "View_Score"                 # Explicitly exclude, ensure it's numeric
  # Add any other columns that are definitely numeric and should NOT be OHE'd
)
cols_to_exclude_from_ohe_due_to_cardinality <- c("Grantor", "Grantee") 

# Refine factor_cols_to_encode
factor_cols_to_encode <- setdiff(factor_cols_to_encode, cols_to_exclude_from_ohe_due_to_cardinality)
factor_cols_to_encode <- setdiff(factor_cols_to_encode, cols_to_manually_exclude_from_ohe)

cat("Final columns selected for One-Hot Encoding after refinement:", paste(factor_cols_to_encode, collapse=", "), "\n")

# ... (rest of your OHE logic using dummy_cols) ...


if (length(factor_cols_to_encode) > 0) {
  cat("One-hot encoding categorical features:", paste(factor_cols_to_encode, collapse=", "), "\n")
  train_data_encoded <- dummy_cols(processed_train_data, select_columns = factor_cols_to_encode,
                                   remove_first_dummy = FALSE, remove_selected_columns = TRUE, ignore_na = FALSE) 
  test_data_encoded <- dummy_cols(processed_test_data, select_columns = factor_cols_to_encode,
                                  remove_first_dummy = FALSE, remove_selected_columns = TRUE, ignore_na = FALSE)
  
  dummified_feature_names_from_train <- setdiff(names(train_data_encoded), target_variable)
  missing_cols_in_test <- setdiff(dummified_feature_names_from_train, names(test_data_encoded))
  for (col_add in missing_cols_in_test) { test_data_encoded[[col_add]] <- 0 }
  extra_cols_in_test <- setdiff(names(test_data_encoded), names(train_data_encoded)) 
  if (length(extra_cols_in_test) > 0) {
    test_data_encoded <- test_data_encoded[, !(names(test_data_encoded) %in% extra_cols_in_test), drop = FALSE]
  }
  final_train_features_df <- train_data_encoded %>% select(all_of(dummified_feature_names_from_train))
  final_test_features_df <- test_data_encoded %>% select(all_of(dummified_feature_names_from_train))
  train_labels <- train_data_encoded[[target_variable]]
  test_labels <- test_data_encoded[[target_variable]]
} else {
  cat("No categorical features to one-hot encode.\n")
  final_train_features_df <- processed_train_data %>% select(-all_of(target_variable))
  final_test_features_df <- processed_test_data %>% select(-all_of(target_variable))
  train_labels <- processed_train_data[[target_variable]]
  test_labels <- processed_test_data[[target_variable]]
}
cat("One-hot encoding complete. Number of features after encoding:", ncol(final_train_features_df), "\n")

# --- 7. Prepare Data for XGBoost ---
cat("\nPreparing data for XGBoost (creating DMatrices)...\n")
# Convert Date/POSIXct columns to numeric before creating matrix for XGBoost
# XGBoost expects a purely numeric matrix.
convert_datetime_to_numeric <- function(df) {
  for (col_name in names(df)) {
    if (inherits(df[[col_name]], "Date") || inherits(df[[col_name]], "POSIXct")) {
      df[[col_name]] <- as.numeric(df[[col_name]])
    }
  }
  return(df)
}
final_train_features_df <- convert_datetime_to_numeric(final_train_features_df)
final_test_features_df <- convert_datetime_to_numeric(final_test_features_df)

train_features_matrix <- data.matrix(final_train_features_df)
test_features_matrix <- data.matrix(final_test_features_df)
dtrain <- xgb.DMatrix(data = train_features_matrix, label = train_labels, missing = NA) 
dtest <- xgb.DMatrix(data = test_features_matrix, label = test_labels, missing = NA)
watchlist <- list(train = dtrain, test = dtest)
cat("XGBoost DMatrices created successfully.\n")
cat("Script Part 1 (Data Prep) Complete. Ready for model training.\n")


# --- Script Part 2: Model Training and Evaluation ---
cat("\n\n--- Starting Script Part 2: Model Training and Evaluation ---\n")

# --- 8. XGBoost Model Training ---
cat("Starting XGBoost model training...\n")
best_xgb_params_tuned <- list(
  objective = "reg:squarederror", eval_metric = "rmse", eta = 0.01, max_depth = 10,
  subsample = 0.8, colsample_bytree = 0.8, min_child_weight = 1, gamma = 0.1,
  lambda = 1, alpha = 0 
)
final_training_nrounds <- 2000 
final_early_stopping_rounds <- 50

xgb_model <- xgb.train(
  params = best_xgb_params_tuned, data = dtrain, nrounds = final_training_nrounds,
  watchlist = watchlist, early_stopping_rounds = final_early_stopping_rounds,
  print_every_n = 50, verbose = 1
)
cat("XGBoost model training complete. Best iteration:", xgb_model$best_iteration, "\n")
best_iter_final_model <- xgb_model$best_iteration

# --- 9. Make Predictions and Evaluate ---
cat("\nMaking predictions on the test set...\n")
predictions_log <- predict(xgb_model, dtest, iterationrange = c(1, best_iter_final_model))
predictions_original <- exp(predictions_log) 
test_labels_original_scale <- exp(test_labels) 
predictions_original[predictions_original <= 0] <- min(predictions_original[predictions_original > 0], 1, na.rm = TRUE) 

mae_log <- mae(test_labels, predictions_log)
rmse_log <- rmse(test_labels, predictions_log)
r_squared_log <- R2(predictions_log, test_labels) 
mae_original <- mae(test_labels_original_scale, predictions_original)
rmse_original <- rmse(test_labels_original_scale, predictions_original)
r_squared_original <- R2(predictions_original, test_labels_original_scale)

cat("\nPerformance Metrics (Log Scale):\n")
cat("  MAE (Log): ", round(mae_log, 4), "\n")
cat("  RMSE (Log): ", round(rmse_log, 4), "\n")
cat("  R-squared (Log): ", round(r_squared_log, 4), "\n")
cat("\nPerformance Metrics (Original Scale):\n")
cat("  MAE (Original): $", format(round(mae_original, 0), big.mark=","), "\n")
cat("  RMSE (Original): $", format(round(rmse_original, 0), big.mark=","), "\n")
cat("  R-squared (Original): ", round(r_squared_original, 4), "\n")

# --- 10. Visualizations ---
cat("\nGenerating visualizations...\n")
plot_df <- data.frame(
  Actual_Original = test_labels_original_scale, Predicted_Original = predictions_original,
  Residuals_Original = test_labels_original_scale - predictions_original,
  Actual_Log = test_labels, Predicted_Log = predictions_log,
  Residuals_Log = test_labels - predictions_log
)
plot_df$Internal_Test_Row_Index <- 1:nrow(plot_df)
plot_df$Absolute_Error_Original <- abs(plot_df$Residuals_Original) # Ensure this is present

p_avp_orig <- ggplot(plot_df, aes(x = Actual_Original, y = Predicted_Original)) +
  geom_point(alpha = 0.5) + geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Actual vs. Predicted Sale Price (Original Scale)", x = "Actual Sale Price ($)", y = "Predicted Sale Price ($)") +
  scale_x_continuous(labels = scales::dollar_format()) + scale_y_continuous(labels = scales::dollar_format()) + theme_minimal()
print(p_avp_orig)
ggsave("Rplot_ActualVsPredicted_Original.png", plot = p_avp_orig, width = 8, height = 6)

p_res_vs_pred_orig <- ggplot(plot_df, aes(x = Predicted_Original, y = Residuals_Original)) +
  geom_point(alpha = 0.5) + geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs. Predicted Sale Price (Original Scale)", x = "Predicted Sale Price ($)", y = "Residuals ($)") +
  scale_x_continuous(labels = scales::dollar_format()) + scale_y_continuous(labels = scales::dollar_format()) + theme_minimal()
print(p_res_vs_pred_orig)
ggsave("Rplot_ResidualsVsPredicted_Original.png", plot = p_res_vs_pred_orig, width = 8, height = 6)

importance_matrix <- xgb.importance(model = xgb_model)
p_importance <- xgb.ggplot.importance(importance_matrix, top_n = 20) +
  labs(title = "Top 20 Feature Importance") + theme_minimal()
print(p_importance)
ggsave("Rplot_FeatureImportance.png", plot = p_importance, width = 10, height = 8)


# --- 11. Detailed Residual Analysis (Revised Completeness Check) ---
cat("\n\n--- Starting Script Part 2: Section 11 - Detailed Residual Analysis (Revised Completeness Check) ---\n")
if (!exists("plot_df")) stop("Error: plot_df not found.")
if (!exists("full_data_for_section_11_analysis")) stop("Error: 'full_data_for_section_11_analysis' not found.")
if (!exists("train_index")) stop("Error: train_index not found.")
if (!exists("final_test_features_df")) stop("Error: final_test_features_df not found.")
if (!exists("target_variable")) stop("Error: target_variable name not found.")
if (!exists("selected_predictors")) stop("Error: selected_predictors list not found.")
if (!exists("modeling_data")) stop("Error: modeling_data (imputed, before split) not found.")

data_to_analyze_residuals <- full_data_for_section_11_analysis
cat("Using 'full_data_for_section_11_analysis' for residual detail. Dimensions:", paste(dim(data_to_analyze_residuals), collapse="x"), "\n")
cols_for_completeness_check <- unique(c(target_variable, selected_predictors))
missing_cols_for_check <- cols_for_completeness_check[!cols_for_completeness_check %in% names(data_to_analyze_residuals)]
if (length(missing_cols_for_check) > 0) stop(paste("Error: Columns for completeness check missing:", paste(missing_cols_for_check, collapse=", ")))
cat("All columns for completeness check found.\n")

cat("\n--- DEBUG: NA counts in data_to_analyze_residuals ---\n")
na_counts_debug_snapshot <- sapply(data_to_analyze_residuals[, cols_for_completeness_check, drop = FALSE], function(x) sum(is.na(x)))
if(any(na_counts_debug_snapshot > 0)) {
    cat("CRITICAL WARNING: NAs found in snapshot:\n"); print(na_counts_debug_snapshot[na_counts_debug_snapshot > 0])
    stop("Halting: Snapshot data for residual analysis contains NAs.")
} else { cat("SUCCESS: No NAs found in snapshot for modeling columns.\n") }
cat("--- END DEBUG ---\n")

data_imputed_complete_for_model <- data_to_analyze_residuals
cat("Rows in data_imputed_complete_for_model:", nrow(data_imputed_complete_for_model), "\n")
cat("Row count of 'modeling_data':", nrow(modeling_data), "\n")
if (nrow(data_imputed_complete_for_model) != nrow(modeling_data)) stop("Halting due to row count mismatch (snapshot vs modeling_data).")
cat("SUCCESS: Row count of snapshot matches modeling_data.\n")

test_indices_in_imputed_complete <- setdiff(1:nrow(data_imputed_complete_for_model), train_index)
original_features_test_set_imputed <- data_imputed_complete_for_model[test_indices_in_imputed_complete, , drop = FALSE]
cat("\nRows in original_features_test_set_imputed:", nrow(original_features_test_set_imputed), "\n")
cat("Row count of 'final_test_features_df':", nrow(final_test_features_df), "\n")
if (nrow(original_features_test_set_imputed) != nrow(final_test_features_df)) stop("Halting due to row count mismatch (imputed test vs OHE'd test).")
cat("SUCCESS: Row count of imputed test features matches OHE'd test features.\n")

if (nrow(plot_df) != nrow(original_features_test_set_imputed)) stop("Row mismatch: plot_df vs original_features_test_set_imputed.")
cat("Row counts of plot_df and original_features_test_set_imputed match.\n")

error_analysis_df <- bind_cols(
    original_features_test_set_imputed %>% select(-all_of(target_variable)), # Remove target to avoid clash
    plot_df
)
cat("\nCreated 'error_analysis_df'. Dimensions:", paste(dim(error_analysis_df), collapse="x"), "\n")
cat("Head of error_analysis_df (first few columns):\n")
print(head(error_analysis_df[, 1:min(6, ncol(error_analysis_df))]))

if (!"Absolute_Error_Original" %in% names(error_analysis_df)) {
    if ("Residuals_Original" %in% names(error_analysis_df)) {
        error_analysis_df$Absolute_Error_Original <- abs(error_analysis_df$Residuals_Original)
        cat("Added 'Absolute_Error_Original' to error_analysis_df.\n")
    } else { stop("Cannot calculate Absolute_Error_Original.") }
}

residuals_df_sorted_final <- error_analysis_df %>% arrange(desc(Absolute_Error_Original))
cat("\nTop properties with largest prediction errors (Original Scale):\n")
n_top_errors <- 10 
key_features_to_display_from_snapshot <- intersect(selected_predictors, names(error_analysis_df))
cat("Will display the following key features:\n"); print(key_features_to_display_from_snapshot)

print_property_details_v4 <- function(rank_label, single_error_row_data, features_list) {
  cat("\nRank:", rank_label, "\n")
  cat("  Actual Sale Price: $", format(round(single_error_row_data$Actual_Original, 0), big.mark=","), "\n")
  cat("  Predicted Sale Price: $", format(round(single_error_row_data$Predicted_Original, 0), big.mark=","), "\n")
  cat("  Residual: $", format(round(single_error_row_data$Residuals_Original, 0), big.mark=","), "\n")
  cat("  Absolute Error: $", format(round(single_error_row_data$Absolute_Error_Original, 0), big.mark=","), "\n")
  if (length(features_list) > 0) {
    cat("  Key Original (Imputed, Pre-OHE) Features:\n")
    for (col_name in features_list) { 
      if (col_name %in% names(single_error_row_data)) {
        val <- single_error_row_data[[col_name]]
        val_to_print <- if (inherits(val, "POSIXct") || inherits(val, "Date")) { format(val, "%Y-%m-%d") }
                        else if (is.factor(val)) { as.character(val) }
                        else if (is.numeric(val) && !is.integer(val) && !is.logical(val)) { tryCatch({round(val, 2)}, error = function(e) {as.character(val)}) }
                        else { as.character(val) }
        cat("    ", col_name, ": ", val_to_print, "\n")
      }
    }
  }
}

cat(paste0("\n--- Top ", n_top_errors, " Largest Absolute Errors ---\n"))
top_n_overall_errors_df_final <- head(residuals_df_sorted_final, n_top_errors)
for (i in 1:nrow(top_n_overall_errors_df_final)) { print_property_details_v4(i, top_n_overall_errors_df_final[i, ], key_features_to_display_from_snapshot) }

cat(paste0("\n\n--- Top ", n_top_errors, " Most Under-Predicted Properties ---\n"))
under_predicted_df_final <- error_analysis_df %>% arrange(desc(Residuals_Original)) %>% head(n_top_errors)
for (i in 1:nrow(under_predicted_df_final)) { print_property_details_v4(paste(i, "(Under-prediction)"), under_predicted_df_final[i, ], key_features_to_display_from_snapshot) }

cat(paste0("\n\n--- Top ", n_top_errors, " Most Over-Predicted Properties ---\n"))
over_predicted_df_final <- error_analysis_df %>% arrange(Residuals_Original) %>% head(n_top_errors)
for (i in 1:nrow(over_predicted_df_final)) { print_property_details_v4(paste(i, "(Over-prediction)"), over_predicted_df_final[i, ], key_features_to_display_from_snapshot) }

cat("\n--- Detailed Residual Analysis Complete ---\n")
cat("\n--- Full Script Run Complete ---\n")
```


```{r}
#| label: inspect-parquet-tax-columns
#| echo: true

# Load the arrow library if you haven't already
# install.packages("arrow") # Run this once if not installed
library(arrow)

# Define the path to the parquet file
# Adjust if your working directory or file location is different
parquet_file_path <- "../data/stage/full_data_for_modeling.parquet"

# Check if the parquet file exists
if (file.exists(parquet_file_path)) {
  
  cat(paste0("Loading parquet file: ", parquet_file_path, "\n"))
  # Read the parquet file into an R dataframe
  # Using as.data.frame() to make it easier to work with standard R functions immediately
  data_from_parquet <- arrow::read_parquet(parquet_file_path, as_data_frame = TRUE) 
  
  cat("Parquet file loaded successfully.\n")
  cat("Dimensions of data_from_parquet: ", paste(dim(data_from_parquet), collapse = " rows, "), " columns\n")
  
  # Define the tax columns we expect to see (based on previous discussions)
  # These are the names *after* the parcel_tax_summary aggregation in DuckDB
  tax_cols_in_parquet <- c(
    "Tax_Summary_Tax_Year", 
    "Tax_Summary_Taxable_Value", 
    "Tax_Summary_Land_Value", 
    "Tax_Summary_Improvement_Value"
    # Add any other tax columns you expect to be in this file
  )
  
  # Check which of these columns actually exist in the loaded dataframe
  existing_tax_cols <- intersect(tax_cols_in_parquet, names(data_from_parquet))
  
  if (length(existing_tax_cols) > 0) {
    cat("\n### Summary of Tax Columns in data_from_parquet (from full_data_for_modeling.parquet):\n")
    for (col_name in existing_tax_cols) {
      cat(paste0("\n#### Summary for: ", col_name, "\n"))
      print(summary(data_from_parquet[[col_name]]))
      
      cat(paste0("\n#### Number of NAs in ", col_name, ": "))
      print(sum(is.na(data_from_parquet[[col_name]])))
      
      cat(paste0("\n#### Number of Zeros in ", col_name, " (if numeric): "))
      if (is.numeric(data_from_parquet[[col_name]])) {
        print(sum(data_from_parquet[[col_name]] == 0, na.rm = TRUE))
      } else {
        print("Not a numeric column.")
      }
      
      cat(paste0("\n#### First 5 values of ", col_name, ":\n"))
      print(head(data_from_parquet[[col_name]], 5))
    }
  } else {
    cat("\nNone of the specified tax columns were found in data_from_parquet.\n")
    cat("Available columns in data_from_parquet: ", paste(names(data_from_parquet), collapse=", "), "\n")
  }
  
} else {
  cat(paste0("Error: Parquet file not found at '", parquet_file_path, "'. Please check the path.\n"))
}
```



```{r}
# --- Section 12: Analysis of Assessed Value vs. Actual Sale Price ---

cat("\n--- Starting Script Part 2: Section 12 - Analysis of Assessed Value vs. Actual Sale Price ---\n")

# Ensure error_analysis_df is available and contains the necessary columns
# It should have:
# - Actual_Sale_Price_Original (Actual sale price on original scale)
# - Tax_Summary_Taxable_Value
# - Residual_Original_Scale (Model's residual: Actual - Predicted on original scale)
# - Other key features for displaying details

if (!exists("error_analysis_df")) {
  stop("error_analysis_df not found. Please ensure Section 11 (Detailed Residual Analysis) has been run.")
}

required_cols_s12 <- c("Actual_Sale_Price_Original", "Tax_Summary_Taxable_Value", "Residual_Original_Scale")
if (!all(required_cols_s12 %in% names(error_analysis_df))) {
  stop(paste("One or more required columns for Section 12 are missing from error_analysis_df:", 
             paste(required_cols_s12[!required_cols_s12 %in% names(error_analysis_df)], collapse=", ")))
}

# 1. Calculate Difference and Ratio
cat("\nCalculating Assessed Value vs. Actual Sale Price metrics...\n")
error_analysis_df$Assessed_vs_Actual_Diff <- error_analysis_df$Actual_Sale_Price_Original - error_analysis_df$Tax_Summary_Taxable_Value

# For ratio, handle cases where Tax_Summary_Taxable_Value might be 0 or very small to avoid Inf or extreme values
# Using NA if Tax_Summary_Taxable_Value is 0. Consider a threshold if very small non-zero values are an issue.
error_analysis_df$Assessed_vs_Actual_Ratio <- ifelse(
  error_analysis_df$Tax_Summary_Taxable_Value != 0,
  error_analysis_df$Actual_Sale_Price_Original / error_analysis_df$Tax_Summary_Taxable_Value,
  NA 
)

cat("Metrics calculated: Assessed_vs_Actual_Diff, Assessed_vs_Actual_Ratio\n")

# 2. Descriptive Statistics and Visualizations
cat("\n--- Descriptive Statistics for Assessed vs. Actual Metrics ---\n")
cat("Summary of Assessed_vs_Actual_Diff (Actual Sale Price - Taxable Value):\n")
print(summary(error_analysis_df$Assessed_vs_Actual_Diff))

cat("\nSummary of Assessed_vs_Actual_Ratio (Actual Sale Price / Taxable Value):\n")
print(summary(error_analysis_df$Assessed_vs_Actual_Ratio))

# Histograms
# You might want to save these plots to files as well, similar to other plots
# For example:
# png("Rplot_Hist_AssessedVsActualDiff.png", width=800, height=600)
# hist(error_analysis_df$Assessed_vs_Actual_Diff, 
#      main="Histogram of (Actual Sale Price - Taxable Value)", 
#      xlab="Difference ($)", breaks=50, col="lightblue")
# dev.off()

# png("Rplot_Hist_AssessedVsActualRatio.png", width=800, height=600)
# hist(error_analysis_df$Assessed_vs_Actual_Ratio, 
#      main="Histogram of (Actual Sale Price / Taxable Value)", 
#      xlab="Ratio", breaks=50, col="lightgreen", xlim=c(0, 5)) # Adjust xlim as needed
# dev.off()

cat("\nDisplaying Histograms (check plot pane or saved files if enabled):\n")
hist(error_analysis_df$Assessed_vs_Actual_Diff, 
     main="Histogram of (Actual Sale Price - Taxable Value)", 
     xlab="Difference ($)", breaks=50, col="lightblue")
readline(prompt="Press [enter] to continue to next plot...") # Pauses for interactive viewing

# For the ratio, the distribution can be very skewed, especially if some taxable values are small.
# It might be useful to look at a specific range or log-transform the ratio for visualization if it's too skewed.
# For now, a simple histogram with a reasonable xlim.
hist(na.omit(error_analysis_df$Assessed_vs_Actual_Ratio), # na.omit for cases where Taxable Value was 0
     main="Histogram of (Actual Sale Price / Taxable Value)", 
     xlab="Ratio", breaks=100, col="lightgreen", xlim=c(quantile(error_analysis_df$Assessed_vs_Actual_Ratio, 0.01, na.rm=TRUE), quantile(error_analysis_df$Assessed_vs_Actual_Ratio, 0.99, na.rm=TRUE))) # Show 1st to 99th percentile
readline(prompt="Press [enter] to continue...")


# 3. Identifying Extreme Cases
cat("\n--- Extreme Cases: Assessed Value vs. Actual Sale Price ---\n")
N_extreme_cases <- 10 # Number of top cases to display


# --- MODIFICATION START: Explicitly define features for Section 12 ---
key_features_to_display_s11 <- c(
  "Sale_Date", 
  "Property_Type", 
  "Total_Building_SqFt", 
  "Year_Built", 
  "Neighborhood_Summary",
  "Waterfront_Type_Summary",
  "View_Quality_Summary",
  "Main_Building_Quality" 
  # Add any other columns from error_analysis_df you want to see
)
# Optional: Add a debug print to confirm it's set right before use
cat("DEBUG: In Section 12, key_features_to_display_s11 is NOW set to:\n")
print(key_features_to_display_s11)

# The original 'if (!exists(...))' block should be REMOVED or COMMENTED OUT:
# if (!exists("key_features_to_display_s11")) {
#     # Define a default list or pull from how it was defined in Section 11
#     # This is just an example, adjust to your actual list from Section 11
#     key_features_to_display_s11 <- names(error_analysis_df)[1:min(20, ncol(error_analysis_df))] 
#     cat("Warning: 'key_features_to_display_s11' not found, using a default list of features.\n")
# }
# --- MODIFICATION END ---

# Cases: Actual Sale Price MUCH HIGHER than Taxable Value (Large Positive Difference)
cat(paste0("\n--- Top ", N_extreme_cases, " Properties: Actual Sale Price >> Taxable Value (Largest Positive Difference) ---\n"))
sold_much_higher_than_assessed <- error_analysis_df %>%
  dplyr::arrange(desc(Assessed_vs_Actual_Diff)) %>%
  head(N_extreme_cases)

for(i in 1:nrow(sold_much_higher_than_assessed)) {
  cat(paste("\nRank:", i, "(Sold Higher vs Assessed)\n"))
  cat(paste("  Actual Sale Price: $", format(round(sold_much_higher_than_assessed$Actual_Sale_Price_Original[i]), big.mark=","), "\n"))
  cat(paste("  Taxable Value: $", format(round(sold_much_higher_than_assessed$Tax_Summary_Taxable_Value[i]), big.mark=","), "\n"))
  cat(paste("  Difference (Actual - Taxable): $", format(round(sold_much_higher_than_assessed$Assessed_vs_Actual_Diff[i]), big.mark=","), "\n"))
  cat(paste("  Ratio (Actual / Taxable):", round(sold_much_higher_than_assessed$Assessed_vs_Actual_Ratio[i], 2), "\n"))
  cat(paste("  Model Predicted Sale Price: $", format(round(sold_much_higher_than_assessed$Predicted_Sale_Price_Original[i]), big.mark=","), "\n"))
  cat(paste("  Model Residual: $", format(round(sold_much_higher_than_assessed$Residual_Original_Scale[i]), big.mark=","), "\n"))
  cat("  Key Original Features:\n")
  
  # --- MODIFIED PRINTING ---
  # Convert to a base R data.frame explicitly for printing
  temp_df_to_print <- as.data.frame(sold_much_higher_than_assessed[i, key_features_to_display_s11])
  # Use the default print method for data.frames, ensuring row names are off
  print.data.frame(temp_df_to_print, row.names = FALSE)
  # --- END OF MODIFIED PRINTING ---
}
# Do the same for 'sold_much_lower_than_assessed' loop:
# ...
# print(as.data.frame(sold_much_lower_than_assessed[i, key_features_to_display_s11]), row.names = FALSE)
# Cases: Actual Sale Price MUCH LOWER than Taxable Value (Large Negative Difference)
cat(paste0("\n--- Top ", N_extreme_cases, " Properties: Actual Sale Price << Taxable Value (Largest Negative Difference) ---\n"))
sold_much_lower_than_assessed <- error_analysis_df %>%
  dplyr::arrange(Assessed_vs_Actual_Diff) %>% # Arrange by ascending difference
  head(N_extreme_cases)

for(i in 1:nrow(sold_much_lower_than_assessed)) {
  cat(paste("\nRank:", i, "(Sold Lower vs Assessed)\n"))
  cat(paste("  Actual Sale Price: $", format(round(sold_much_lower_than_assessed$Actual_Sale_Price_Original[i]), big.mark=","), "\n"))
  cat(paste("  Taxable Value: $", format(round(sold_much_lower_than_assessed$Tax_Summary_Taxable_Value[i]), big.mark=","), "\n"))
  cat(paste("  Difference (Actual - Taxable): $", format(round(sold_much_lower_than_assessed$Assessed_vs_Actual_Diff[i]), big.mark=","), "\n"))
  cat(paste("  Ratio (Actual / Taxable):", round(sold_much_lower_than_assessed$Assessed_vs_Actual_Ratio[i], 2), "\n"))
  cat(paste("  Model Predicted Sale Price: $", format(round(sold_much_lower_than_assessed$Predicted_Sale_Price_Original[i]), big.mark=","), "\n"))
  cat(paste("  Model Residual: $", format(round(sold_much_lower_than_assessed$Residual_Original_Scale[i]), big.mark=","), "\n"))
  cat("  Key Original Features:\n")
  
  # --- MODIFIED PRINTING ---
  temp_df_to_print_lower <- as.data.frame(sold_much_lower_than_assessed[i, key_features_to_display_s11])
  print.data.frame(temp_df_to_print_lower, row.names = FALSE)
  # --- END OF MODIFIED PRINTING ---
}

# 4. Relationship with Model Residuals
cat("\n--- Relationship between Assessed-Actual Difference and Model Residuals ---\n")

# Scatter plot: Assessed_vs_Actual_Diff vs. Model_Residual
# Again, consider saving this plot
# png("Rplot_AssessedDiff_vs_ModelResidual.png", width=800, height=600)
# plot(error_analysis_df$Assessed_vs_Actual_Diff, error_analysis_df$Residual_Original_Scale,
#      xlab = "Actual Sale Price - Taxable Value ($)",
#      ylab = "Model Residual (Actual - Predicted) ($)",
#      main = "Assessed-Actual Difference vs. Model Residual",
#      pch = 16, col = rgb(0,0,1,0.5)) # Semi-transparent blue points
# abline(h = 0, col = "red", lty = 2)
# abline(v = 0, col = "darkgray", lty = 2)
# dev.off()

plot(error_analysis_df$Assessed_vs_Actual_Diff, error_analysis_df$Residual_Original_Scale,
     xlab = "Actual Sale Price - Taxable Value ($)",
     ylab = "Model Residual (Actual - Predicted) ($)",
     main = "Assessed-Actual Difference vs. Model Residual",
     pch = 16, col = rgb(0,0,1,0.5)) 
abline(h = 0, col = "red", lty = 2)
abline(v = 0, col = "darkgray", lty = 2)
readline(prompt="Press [enter] to continue to next plot...")

# Scatter plot: Assessed_vs_Actual_Ratio vs. Model_Residual
# Be cautious with the ratio plot if there are extreme values.
# You might want to filter error_analysis_df for the plot, e.g., for ratios between 0.1 and 3, or use log scale for ratio.
plot_df_ratio_vs_residual <- error_analysis_df[!is.na(error_analysis_df$Assessed_vs_Actual_Ratio) & 
                                               is.finite(error_analysis_df$Assessed_vs_Actual_Ratio), ]
# Further filter extreme ratios for better visualization if needed
ratio_q01 <- quantile(plot_df_ratio_vs_residual$Assessed_vs_Actual_Ratio, 0.01, na.rm = TRUE)
ratio_q99 <- quantile(plot_df_ratio_vs_residual$Assessed_vs_Actual_Ratio, 0.99, na.rm = TRUE)
plot_df_ratio_vs_residual_filtered <- plot_df_ratio_vs_residual[
    plot_df_ratio_vs_residual$Assessed_vs_Actual_Ratio >= ratio_q01 &
    plot_df_ratio_vs_residual$Assessed_vs_Actual_Ratio <= ratio_q99, ]


plot(plot_df_ratio_vs_residual_filtered$Assessed_vs_Actual_Ratio, plot_df_ratio_vs_residual_filtered$Residual_Original_Scale,
     xlab = "Actual Sale Price / Taxable Value (Ratio)",
     ylab = "Model Residual (Actual - Predicted) ($)",
     main = "Assessed-Actual Ratio vs. Model Residual (1st-99th Pctl Ratio)",
     pch = 16, col = rgb(0,0.5,0,0.5)) # Semi-transparent green points
abline(h = 0, col = "red", lty = 2)
# abline(v = 1, col = "darkgray", lty = 2) # Ratio = 1 means Actual = Assessed
readline(prompt="Press [enter] to continue...")

# Correlation
correlation_diff_residual <- cor(error_analysis_df$Assessed_vs_Actual_Diff, 
                                 error_analysis_df$Residual_Original_Scale, 
                                 use = "complete.obs")
cat(paste("\nCorrelation between (Actual Sale Price - Taxable Value) and Model Residual: ", 
          round(correlation_diff_residual, 3), "\n"))

correlation_ratio_residual <- cor(error_analysis_df$Assessed_vs_Actual_Ratio, 
                                  error_analysis_df$Residual_Original_Scale, 
                                  use = "complete.obs")
cat(paste("Correlation between (Actual Sale Price / Taxable Value) and Model Residual: ", 
          round(correlation_ratio_residual, 3), "\n"))


cat("\n--- Section 12: Analysis of Assessed Value vs. Actual Sale Price Complete ---\n")
```




```{r}
if (exists("predictions_original_scale_clean")) {
  cat("Length of 'predictions_original_scale_clean':", length(predictions_original_scale_clean), "\n")
} else {
  cat("'predictions_original_scale_clean' does not exist.\n")
}
```

```{r}
# Check 1: Length of predictions_log
if (exists("predictions_log")) {
  cat("Length of 'predictions_log':", length(predictions_log), "\n")
} else {
  cat("'predictions_log' does not exist.\n")
}

# Check 2: Length of valid_preds
if (exists("valid_preds")) {
  cat("Length of 'valid_preds':", length(valid_preds), "\n")
  # If it exists and has the right length, let's also see a few values to guess its scale
  if (length(valid_preds) == 35263) { 
    cat("First few values of 'valid_preds':\n")
    print(head(valid_preds))
  }
} else {
  cat("'valid_preds' does not exist.\n")
}
```

```{r}
# --- Step 1: Confirm 'predictions_log' exists and prepare original scale predictions ---
predicted_prices_for_test_set_original_scale <- NULL # Initialize

if (exists("predictions_log") && length(predictions_log) == 35263) {
    cat("Found 'predictions_log' with the correct length (35263).\n")
    cat("Assuming these are log-transformed predictions. Converting to original scale using exp().\n")
    
    # Exponentiate to get back to original scale
    predicted_prices_for_test_set_original_scale <- exp(predictions_log)
    
    cat("Created 'predicted_prices_for_test_set_original_scale'.\n")
    cat("First few values:\n")
    print(head(predicted_prices_for_test_set_original_scale))
    cat("Length:", length(predicted_prices_for_test_set_original_scale), "\n\n")
    
} else {
    cat("Error: 'predictions_log' not found or has incorrect length.\n")
    cat("Please ensure 'predictions_log' contains the log-scale predictions for the test set.\n")
}

# --- Step 2: Attempt to create 'test_set_with_predictions' using the new predictions ---

# Ensure the new predictions object was created successfully
if (!is.null(predicted_prices_for_test_set_original_scale)) {

    base_test_df_name <- "original_features_test_set_imputed" 
    actual_prices_vector_name <- "test_labels_original_scale"
    
    # Use the newly created original scale predictions
    predicted_prices_vector <- predicted_prices_for_test_set_original_scale

    # Check if all other necessary objects exist
    if (exists(base_test_df_name) && exists(actual_prices_vector_name)) {

        base_test_df <- get(base_test_df_name)
        actual_prices_vector <- get(actual_prices_vector_name)

        # Verify row counts/lengths match for combining
        if (nrow(base_test_df) == length(actual_prices_vector) &&
            nrow(base_test_df) == length(predicted_prices_vector)) {
            
            test_set_with_predictions <- base_test_df %>%
                dplyr::mutate(
                    # This column will hold the actual original sale prices
                    Actual_Sale_Price_Original_Temp = actual_prices_vector, 
                    # This column will hold the predicted original sale prices
                    predicted_price_original_scale = predicted_prices_vector 
                )
            
            cat("Successfully created 'test_set_with_predictions'.\n")
            cat("It should now have columns 'Actual_Sale_Price_Original_Temp' and 'predicted_price_original_scale'.\n\n")
            
            cat("You can check its structure with:\n")
            cat("print(colnames(test_set_with_predictions))\n")
            cat("print(head(test_set_with_predictions[, c('Actual_Sale_Price_Original_Temp', 'predicted_price_original_scale', if('Tax_Summary_Taxable_Value' %in% colnames(test_set_with_predictions)) 'Tax_Summary_Taxable_Value')]))\n")
            
        } else {
            cat("Error: Row/length mismatch. Cannot combine.\n")
            cat(paste("nrow(", base_test_df_name, "):", nrow(base_test_df), "\n"))
            cat(paste("length(", actual_prices_vector_name, "):", length(actual_prices_vector), "\n"))
            cat(paste("length of 'predicted_prices_for_test_set_original_scale':", length(predicted_prices_vector), "\n"))
        }
    } else {
        cat("Error: One or more key base objects are missing from your environment:\n")
        if (!exists(base_test_df_name)) cat(paste("- ", base_test_df_name, "\n"))
        if (!exists(actual_prices_vector_name)) cat(paste("- ", actual_prices_vector_name, "\n"))
    }
} else {
    cat("\n'predicted_prices_for_test_set_original_scale' was not created. Cannot proceed to build 'test_set_with_predictions'.\n")
}

# --- Step 3: Now, try to create error_analysis_df ---
# This code assumes 'test_set_with_predictions' was successfully created above
    
if (exists("test_set_with_predictions")) {
    error_analysis_df <- test_set_with_predictions %>%
      dplyr::mutate(
        Actual_Sale_Price_Original = Actual_Sale_Price_Original_Temp, 
        Predicted_Sale_Price_Original = predicted_price_original_scale, 
        Residual_Original_Scale = Actual_Sale_Price_Original - Predicted_Sale_Price_Original,
        Absolute_Error_Original_Scale = abs(Residual_Original_Scale)
      ) %>%
      # Make sure Tax_Summary_Taxable_Value is also present
      dplyr::select(
        Actual_Sale_Price_Original, 
        Predicted_Sale_Price_Original,
        Residual_Original_Scale, 
        Absolute_Error_Original_Scale,
        # Ensure Tax_Summary_Taxable_Value is selected if it exists
        dplyr::matches("Tax_Summary_Taxable_Value"), 
        dplyr::everything() 
      )
    cat("\nSuccessfully created 'error_analysis_df'.\n")
    cat("Columns in error_analysis_df: ", paste(colnames(error_analysis_df), collapse=", "), "\n")
    # print(head(error_analysis_df))
} else {
    cat("\n'test_set_with_predictions' was not created. Cannot create 'error_analysis_df'.\n")
}
```


```{r}
# --- Section 13: Investigating Taxable Value vs. Assessed-to-Actual Ratio ---
cat("\n\n--- Section 13: Investigating Taxable Value vs. Assessed-to-Actual Ratio ---\n")

# Ensure error_analysis_df exists and has the required columns
if (!exists("error_analysis_df") || 
    !all(c("Tax_Summary_Taxable_Value", "Assessed_vs_Actual_Ratio", "Property_Type") %in% names(error_analysis_df))) {
  cat("Error: 'error_analysis_df' not found or missing required columns for Section 13.\n")
  cat("Please ensure Sections 11 and 12 have been run successfully.\n")
} else {
  
  cat("\nGenerating scatter plot: Taxable Value vs. Assessed-to-Actual Ratio...\n")
  
  # Load ggplot2 if not already loaded
  if (!require(ggplot2)) {
    install.packages("ggplot2")
    library(ggplot2)
  }
  if (!require(scales)) {
    install.packages("scales")
    library(scales)
  }

  # Create the scatter plot
  # We might have some extreme outliers in Assessed_vs_Actual_Ratio, 
  # so let's consider filtering or transforming the y-axis for better visualization.
  # For now, let's plot it directly and see.
  
  # Remove rows with NA in Assessed_vs_Actual_Ratio for plotting
  plot_data_s13 <- error_analysis_df[!is.na(error_analysis_df$Assessed_vs_Actual_Ratio), ]

  # Cap the Assessed_vs_Actual_Ratio for visualization to avoid extreme outliers dominating the scale
  # For example, cap at a ratio of 3 (meaning actual is 3x assessed) for the main plot view.
  # We can adjust this cap as needed.
  ratio_upper_cap <- 3 
  plot_data_s13$Assessed_vs_Actual_Ratio_Capped <- pmin(plot_data_s13$Assessed_vs_Actual_Ratio, ratio_upper_cap)
  # Also, let's consider a lower cap if there are extremely small ratios, e.g., 0.01
  ratio_lower_cap <- 0.01
  plot_data_s13$Assessed_vs_Actual_Ratio_Capped <- pmax(plot_data_s13$Assessed_vs_Actual_Ratio_Capped, ratio_lower_cap)


  scatter_plot_taxval_ratio <- ggplot(plot_data_s13, 
                                      aes(x = Tax_Summary_Taxable_Value, 
                                          y = Assessed_vs_Actual_Ratio_Capped)) +
    geom_point(aes(color = Property_Type), alpha = 0.5, size = 1.5) +
    geom_hline(yintercept = 1, linetype = "dashed", color = "red") + # Line where Actual = Assessed
    scale_x_continuous(labels = scales::comma) + # Format x-axis labels nicely
    scale_y_continuous(labels = scales::percent_format(accuracy = 1), 
                       breaks = seq(min(plot_data_s13$Assessed_vs_Actual_Ratio_Capped, na.rm=TRUE), 
                                    max(plot_data_s13$Assessed_vs_Actual_Ratio_Capped, na.rm=TRUE), 
                                    by = 0.25)) + # Y-axis as percentage
    labs(
      title = "Taxable Value vs. Ratio of Actual Sale Price to Taxable Value",
      subtitle = paste0("Ratio capped between ", ratio_lower_cap*100, "% and ", ratio_upper_cap*100, "% for visualization. Red line at 100% (Actual = Assessed)."),
      x = "Taxable Value ($)",
      y = "Actual Sale Price / Taxable Value (Ratio %)",
      color = "Property Type"
    ) +
    theme_minimal() +
    theme(legend.position = "top")

  # Display the plot
  print(scatter_plot_taxval_ratio)
  cat("Scatter plot generated. Check the Plots pane.\n")

  # Optional: Save the plot
  # ggsave("scatter_taxable_value_vs_assessed_ratio.png", plot = scatter_plot_taxval_ratio, width = 10, height = 7)
  # cat("Plot saved as scatter_taxable_value_vs_assessed_ratio.png\n")

  # Further analysis:
  # We could also look at correlation or fit a smoothed line
  correlation_taxval_ratio <- cor(plot_data_s13$Tax_Summary_Taxable_Value, 
                                  plot_data_s13$Assessed_vs_Actual_Ratio, 
                                  use = "complete.obs") # Use original ratio for correlation
  cat(paste("\nCorrelation between Taxable Value and Assessed-to-Actual Ratio:", 
            round(correlation_taxval_ratio, 3), "\n"))
  
  # Add a smoothed line (loess) to see the trend
  scatter_plot_taxval_ratio_with_smooth <- scatter_plot_taxval_ratio +
    geom_smooth(method = "loess", se = FALSE, color = "blue")
  
  print(scatter_plot_taxval_ratio_with_smooth)
  cat("Scatter plot with smoothed trend line generated. Check the Plots pane.\n")
  # ggsave("scatter_taxable_value_vs_assessed_ratio_smooth.png", plot = scatter_plot_taxval_ratio_with_smooth, width = 10, height = 7)

}
cat("\n--- Section 13: Analysis Complete ---\n")
```

```{r}
# --- Section 14: Analysis by Neighborhood Summary ---
cat("\n\n--- Section 14: Analysis by Neighborhood Summary ---\n")

# Ensure error_analysis_df exists and has the required columns
if (!exists("error_analysis_df") || 
    !all(c("Neighborhood_Summary", "Assessed_vs_Actual_Ratio", "Tax_Summary_Taxable_Value") %in% names(error_analysis_df))) {
  cat("Error: 'error_analysis_df' not found or missing required columns for Section 14.\n")
  cat("Please ensure Sections 11, 12, and 13 have been run successfully.\n")
} else {
  
  # Load dplyr if not already loaded (it should be from previous sections)
  if (!require(dplyr)) {
    install.packages("dplyr")
    library(dplyr)
  }
  if (!require(knitr)) { # For kable table formatting
    install.packages("knitr")
    library(knitr)
  }


  cat("\nCalculating average metrics by Neighborhood_Summary...\n")
  
  # Group by Neighborhood_Summary and calculate average ratio, count, and avg taxable value
  # Filter out NA ratios and NA Neighborhood_Summary before grouping
  neighborhood_analysis <- error_analysis_df %>%
    filter(!is.na(Assessed_vs_Actual_Ratio) & !is.na(Neighborhood_Summary) & Neighborhood_Summary != "Missing_Val") %>%
    group_by(Neighborhood_Summary) %>%
    summarise(
      Avg_Assessed_vs_Actual_Ratio = mean(Assessed_vs_Actual_Ratio, na.rm = TRUE),
      Avg_Taxable_Value = mean(Tax_Summary_Taxable_Value, na.rm = TRUE),
      Num_Sales = n(),
      .groups = 'drop' # drop grouping for further operations
    ) %>%
    arrange(desc(Avg_Assessed_vs_Actual_Ratio)) # Arrange by highest ratio first

  # Filter out neighborhoods with too few sales for more reliable averages
  min_sales_threshold <- 5 # Define a minimum number of sales for a neighborhood to be included
  neighborhood_analysis_filtered <- neighborhood_analysis %>%
    filter(Num_Sales >= min_sales_threshold)
  
  cat(paste0("Analysis complete. Considered neighborhoods with at least ", min_sales_threshold, " sales.\n"))
  cat(paste0("Total neighborhoods meeting threshold: ", nrow(neighborhood_analysis_filtered), "\n"))

  if(nrow(neighborhood_analysis_filtered) > 0) {
    # Display Top N neighborhoods with the HIGHEST average ratio (Actual >> Assessed)
    N_top_neighborhoods <- 10
    cat(paste0("\n--- Top ", N_top_neighborhoods, " Neighborhoods: Highest Avg. Actual Sale Price / Taxable Value Ratio ---\n"))
    cat("(Properties tend to sell for much MORE than assessed value)\n")
    
    top_high_ratio_neighborhoods <- neighborhood_analysis_filtered %>%
      arrange(desc(Avg_Assessed_vs_Actual_Ratio)) %>%
      head(N_top_neighborhoods) %>%
      mutate(Avg_Assessed_vs_Actual_Ratio = scales::percent(Avg_Assessed_vs_Actual_Ratio, accuracy = 0.1),
             Avg_Taxable_Value = scales::dollar(Avg_Taxable_Value, accuracy = 1))

    print(kable(top_high_ratio_neighborhoods, 
                col.names = c("Neighborhood", "Avg. Ratio (Actual/Assessed)", "Avg. Taxable Value", "Num. Sales"),
                caption = "Top Neighborhoods: Highest Avg. Sale Price to Taxable Value Ratio"))
    
    # Display Top N neighborhoods with the LOWEST average ratio (Actual closer to or << Assessed)
    cat(paste0("\n\n--- Top ", N_top_neighborhoods, " Neighborhoods: Lowest Avg. Actual Sale Price / Taxable Value Ratio ---\n"))
    cat("(Properties tend to sell for CLOSER TO or LESS than assessed value)\n")
    
    top_low_ratio_neighborhoods <- neighborhood_analysis_filtered %>%
      arrange(Avg_Assessed_vs_Actual_Ratio) %>% # Ascending order for lowest ratio
      head(N_top_neighborhoods) %>%
      mutate(Avg_Assessed_vs_Actual_Ratio = scales::percent(Avg_Assessed_vs_Actual_Ratio, accuracy = 0.1),
             Avg_Taxable_Value = scales::dollar(Avg_Taxable_Value, accuracy = 1))

    print(kable(top_low_ratio_neighborhoods,
                col.names = c("Neighborhood", "Avg. Ratio (Actual/Assessed)", "Avg. Taxable Value", "Num. Sales"),
                caption = "Top Neighborhoods: Lowest Avg. Sale Price to Taxable Value Ratio"))

  } else {
    cat("\nNo neighborhoods met the minimum sales threshold for analysis.\n")
  }
}
cat("\n--- Section 14: Analysis by Neighborhood Summary Complete ---\n")
```


```{r}
# --- Section 15: Deep Dive into Neighborhood "401" ---
cat("\n\n--- Section 15: Deep Dive into Neighborhood \"401\" ---\n")

# Ensure error_analysis_df exists
if (!exists("error_analysis_df")) {
  cat("Error: 'error_analysis_df' not found. Please ensure previous sections have run.\n")
} else {
  
  # Load dplyr if not already loaded
  if (!require(dplyr)) {
    install.packages("dplyr")
    library(dplyr)
  }
  if (!require(knitr)) { # For kable table formatting
    install.packages("knitr")
    library(knitr)
  }
  if (!require(scales)) { # For formatting numbers
    install.packages("scales")
    library(scales)
  }

  neighborhood_to_inspect <- "401"
  cat(paste0("\nInspecting properties in Neighborhood_Summary: ", neighborhood_to_inspect, "\n"))
  
  properties_in_nhood_401 <- error_analysis_df %>%
    filter(Neighborhood_Summary == neighborhood_to_inspect)
  
  num_properties_401 <- nrow(properties_in_nhood_401)
  
  if (num_properties_401 > 0) {
    cat(paste0("Found ", num_properties_401, " properties in neighborhood ", neighborhood_to_inspect, ".\n"))
    
    # Summary of Property Types in Neighborhood 401
    cat("\nSummary of Property Types in Neighborhood 401:\n")
    property_type_summary_401 <- properties_in_nhood_401 %>%
      group_by(Property_Type) %>%
      summarise(Count = n(), .groups = 'drop') %>%
      arrange(desc(Count))
    print(kable(property_type_summary_401, caption = paste("Property Types in Neighborhood", neighborhood_to_inspect)))

    # Display key details for a sample of these properties
    # If there are many, just show the first N, or those with the highest ratios
    
    cat(paste0("\n\nKey details for properties in Neighborhood ", neighborhood_to_inspect, 
               " (ordered by highest Assessed_vs_Actual_Ratio):\n"))
    
    # Select and format columns for display
    properties_to_display_401 <- properties_in_nhood_401 %>%
      arrange(desc(Assessed_vs_Actual_Ratio)) %>%
      select(
        Actual_Sale_Price_Original,
        Tax_Summary_Taxable_Value,
        Assessed_vs_Actual_Ratio,
        Predicted_Sale_Price_Original, # Model's prediction
        Residual_Original_Scale,      # Model's error (Actual - Predicted)
        Property_Type,
        Year_Built,
        Total_Building_SqFt,
        Main_Building_Quality
        # Add other columns if desired, e.g., Sale_Date, View_Quality_Summary
      ) %>%
      mutate(
        Actual_Sale_Price_Original = dollar(Actual_Sale_Price_Original, accuracy = 1),
        Tax_Summary_Taxable_Value = dollar(Tax_Summary_Taxable_Value, accuracy = 1),
        Assessed_vs_Actual_Ratio = percent(Assessed_vs_Actual_Ratio, accuracy = 0.1),
        Predicted_Sale_Price_Original = dollar(Predicted_Sale_Price_Original, accuracy = 1),
        Residual_Original_Scale = dollar(Residual_Original_Scale, accuracy = 1)
      )
      
    # Display all if not too many, otherwise a sample (e.g., top 20)
    num_to_display <- min(num_properties_401, 20) 
    print(kable(head(properties_to_display_401, num_to_display), 
                caption = paste("Sample Properties in Neighborhood", neighborhood_to_inspect, "(Top", num_to_display, "by Ratio)")))

    if (num_properties_401 > num_to_display) {
      cat(paste0("\n(Showing top ", num_to_display, " of ", num_properties_401, 
                 " properties, ordered by highest Assessed_vs_Actual_Ratio)\n"))
    }

    # Average model residual for this neighborhood
    avg_model_residual_401 <- mean(properties_in_nhood_401$Residual_Original_Scale, na.rm = TRUE)
    avg_model_abs_residual_401 <- mean(abs(properties_in_nhood_401$Residual_Original_Scale), na.rm = TRUE)
    cat(paste0("\n\nAverage Model Residual (Actual - Predicted) in Neighborhood 401: ", dollar(avg_model_residual_401)))
    cat(paste0("\nAverage Model Absolute Residual in Neighborhood 401: ", dollar(avg_model_abs_residual_401)))
    
    avg_actual_price_401 <- mean(properties_in_nhood_401$Actual_Sale_Price_Original, na.rm = TRUE)
    mape_401 <- avg_model_abs_residual_401 / avg_actual_price_401
    cat(paste0("\nModel MAPE (Mean Absolute Percentage Error) for Neighborhood 401: ", percent(mape_401, accuracy=0.1), "\n"))


  } else {
    cat(paste0("No properties found for Neighborhood_Summary: ", neighborhood_to_inspect, "\n"))
  }
}
cat("\n--- Section 15: Deep Dive into Neighborhood \"401\" Complete ---\n")
```

```{r}
# --- Section 16: Deep Dive into Neighborhood "030512" (Low Ratio Example) ---
cat("\n\n--- Section 16: Deep Dive into Neighborhood \"030512\" ---\n")

# Ensure error_analysis_df exists
if (!exists("error_analysis_df")) {
  cat("Error: 'error_analysis_df' not found. Please ensure previous sections have run.\n")
} else {
  
  # Load dplyr, knitr, scales if not already loaded (should be)
  if (!require(dplyr)) { install.packages("dplyr"); library(dplyr) }
  if (!require(knitr)) { install.packages("knitr"); library(knitr) }
  if (!require(scales)) { install.packages("scales"); library(scales) }

  neighborhood_to_inspect_s16 <- "030512" # Changed to the selected neighborhood
  cat(paste0("\nInspecting properties in Neighborhood_Summary: ", neighborhood_to_inspect_s16, "\n"))
  
  properties_in_nhood_s16 <- error_analysis_df %>%
    filter(Neighborhood_Summary == neighborhood_to_inspect_s16)
  
  num_properties_s16 <- nrow(properties_in_nhood_s16)
  
  if (num_properties_s16 > 0) {
    cat(paste0("Found ", num_properties_s16, " properties in neighborhood ", neighborhood_to_inspect_s16, ".\n"))
    
    # Summary of Property Types in Neighborhood 030512
    cat("\nSummary of Property Types in Neighborhood ", neighborhood_to_inspect_s16, ":\n")
    property_type_summary_s16 <- properties_in_nhood_s16 %>%
      group_by(Property_Type) %>%
      summarise(Count = n(), .groups = 'drop') %>%
      arrange(desc(Count))
    print(kable(property_type_summary_s16, caption = paste("Property Types in Neighborhood", neighborhood_to_inspect_s16)))

    # Display key details for a sample of these properties
    cat(paste0("\n\nKey details for properties in Neighborhood ", neighborhood_to_inspect_s16, 
               " (ordered by lowest Assessed_vs_Actual_Ratio):\n")) # Order by lowest ratio
    
    properties_to_display_s16 <- properties_in_nhood_s16 %>%
      arrange(Assessed_vs_Actual_Ratio) %>% # Order by lowest ratio first
      select(
        Actual_Sale_Price_Original,
        Tax_Summary_Taxable_Value,
        Assessed_vs_Actual_Ratio,
        Predicted_Sale_Price_Original,
        Residual_Original_Scale,
        Property_Type,
        Year_Built,
        Total_Building_SqFt,
        Main_Building_Quality,
        Waterfront_Type_Summary, # Added this as it might be relevant for high-value properties
        View_Quality_Summary     # Added this as well
      ) %>%
      mutate(
        Actual_Sale_Price_Original = dollar(Actual_Sale_Price_Original, accuracy = 1),
        Tax_Summary_Taxable_Value = dollar(Tax_Summary_Taxable_Value, accuracy = 1),
        Assessed_vs_Actual_Ratio = percent(Assessed_vs_Actual_Ratio, accuracy = 0.1),
        Predicted_Sale_Price_Original = dollar(Predicted_Sale_Price_Original, accuracy = 1),
        Residual_Original_Scale = dollar(Residual_Original_Scale, accuracy = 1)
      )
      
    num_to_display_s16 <- min(num_properties_s16, 20) 
    print(kable(head(properties_to_display_s16, num_to_display_s16), 
                caption = paste("Sample Properties in Neighborhood", neighborhood_to_inspect_s16, "(Top", num_to_display_s16, "by Lowest Ratio)")))

    if (num_properties_s16 > num_to_display_s16) {
      cat(paste0("\n(Showing top ", num_to_display_s16, " of ", num_properties_s16, 
                 " properties, ordered by lowest Assessed_vs_Actual_Ratio)\n"))
    }

    # Average model residual for this neighborhood
    avg_model_residual_s16 <- mean(properties_in_nhood_s16$Residual_Original_Scale, na.rm = TRUE)
    avg_model_abs_residual_s16 <- mean(abs(properties_in_nhood_s16$Residual_Original_Scale), na.rm = TRUE)
    cat(paste0("\n\nAverage Model Residual (Actual - Predicted) in Neighborhood ", neighborhood_to_inspect_s16, ": ", dollar(avg_model_residual_s16)))
    cat(paste0("\nAverage Model Absolute Residual in Neighborhood ", neighborhood_to_inspect_s16, ": ", dollar(avg_model_abs_residual_s16)))
    
    avg_actual_price_s16 <- mean(properties_in_nhood_s16$Actual_Sale_Price_Original, na.rm = TRUE)
    mape_s16 <- avg_model_abs_residual_s16 / avg_actual_price_s16
    cat(paste0("\nModel MAPE (Mean Absolute Percentage Error) for Neighborhood ", neighborhood_to_inspect_s16, ": ", percent(mape_s16, accuracy=0.1), "\n"))

  } else {
    cat(paste0("No properties found for Neighborhood_Summary: ", neighborhood_to_inspect_s16, "\n"))
  }
}
cat("\n--- Section 16: Deep Dive into Neighborhood \"030512\" Complete ---\n")
```


```{r}
# --- Section 17: Summary of Key Findings & Actionable Insights ---
cat("\n\n--- Section 17: Summary of Key Findings & Actionable Insights ---\n")

# This section is primarily for documenting our conclusions.

# I. Executive Summary:
#    - Brief overview: Analysis of Pierce County property sales data reveals significant
#      discrepancies between assessed taxable values and actual market sale prices for
#      certain property types and neighborhoods.
#    - An XGBoost predictive model for sale prices consistently demonstrates higher accuracy
#      in reflecting market values compared to current assessments in these anomalous areas.
#    - Key areas of concern include [e.g., under-assessment of new condos, potential
#      over-assessment of specific high-value residential properties].

# II. Key Findings:
#
#    A. Overall Assessment Ratio Analysis (from Section 12 & 13):
#       1. Significant Spread: Many properties sell for considerably more or less than
#          their assessed taxable value.
#          - Recall the histograms and the `Assessed_vs_Actual_Ratio`.
#       2. Property Type Influence:
#          - Condominiums: Frequently observed selling for substantially more than
#            their assessed value (high `Assessed_vs_Actual_Ratio`).
#          - Residential: More varied, but includes instances of both over and under-assessment.
#       3. Taxable Value Trend (Correlation -0.154):
#          - A weak negative correlation exists, suggesting higher taxable value properties
#            tend to have lower sale price to assessment ratios (i.e., are less likely to be
#            "under-assessed" or more likely to be "over-assessed" on a percentage basis).
#
#    B. Anomalous Neighborhood Deep Dive - Neighborhood "401" (New Condominiums):
#       1. Extreme Under-assessment: Average sale price is ~6.8 times the assessed taxable value.
#       2. Property Profile: Exclusively "Condominium" property type, all built in 2021,
#          "Very Good" quality.
#       3. Root Cause Hypothesis: Assessments likely based on pre-construction/outdated land
#          values, failing to capture the market value of new, high-quality condo units.
#       4. Model Performance: XGBoost model predicted sale prices with high accuracy (4.3% MAPE),
#          indicating predictable market behavior despite assessment lag.
#
#    C. Anomalous Neighborhood Deep Dive - Neighborhood "030512" (Waterfront Residentials):
#       1. Significant Over-assessment (or Under-selling): Average sale price is ~42.6% of
#          the assessed taxable value.
#       2. Property Profile: Exclusively "Residential" property type, all "WF Lake" (Lake Waterfront).
#          Predominantly older homes (1970s-1990s) with variable quality. `View_Quality_Summary`
#          often "Missing_Val".
#       3. Root Cause Hypothesis: Assessments may over-value the "WF Lake" designation without
#          sufficiently accounting for property age, condition, specific lake characteristics,
#          or actual view quality. Market may not support the high assessed values for these
#          particular older/varied-condition waterfront properties.
#       4. Model Performance: XGBoost model predicted sale prices with reasonable accuracy
#          (10% MAPE), generally much closer to actual sale prices than assessed values.
#          Model sometimes predicted higher sale prices than actuals, suggesting potential
#          market resistance even beyond what the model captured.
#
#    D. General Model Utility:
#       1. The developed XGBoost model consistently provides sale price predictions that are
#          closer to actual market transactions than the currently listed assessed taxable values,
#          especially in the identified anomalous areas.
#
# III. Actionable Insights & Potential Recommendations:
#
#    A. For Assessment Authorities (e.g., Pierce County Assessor-Treasurer):
#       1. Prioritize Review of Neighborhood "401": Conduct an immediate review of assessment
#          methodologies and values for new condominium developments, particularly those from 2021.
#          Ensure assessments reflect current market values upon completion and sale.
#       2. Investigate Waterfront Assessments (Neighborhood "030512" as case study):
#          Review how "Waterfront" status (especially "WF Lake") contributes to assessed value,
#          particularly for older residential properties. Consider incorporating more granular
#          data on property condition, specific view quality, and recent comparable sales of
#          similar older waterfront homes.
#       3. Broader Review for Condominiums: Given the general trend, expand review of
#          condominium assessments beyond just "401" to ensure equitable valuation.
#       4. Leverage Predictive Modeling: Consider incorporating advanced predictive modeling
#          (like the XGBoost model developed) as a supplementary tool for:
#          - Identifying potential assessment anomalies at scale.
#          - Providing an independent check on assessed values.
#          - Improving the accuracy and equity of future assessments.
#
#    B. For Further Analysis:
#       1. Quantify Assessment Accuracy: Perform a systematic comparison of model MAPE vs.
#          assessment "MAPE" across different property types and value segments.
#       2. Explore Other Neighborhoods: Identify and analyze other neighborhoods exhibiting
#          high/low assessment ratios.
#       3. Refine Model: Incorporate additional data if available (e.g., detailed property
#          condition, renovation history, specific view codes) to further improve model accuracy,
#          especially for unique properties like those in "030512".
#
# IV. Limitations:
#    - Analysis based on available sales data; does not include all properties in the county.
#    - Model predictions are estimates and subject to error.
#    - External factors not included in the dataset can influence sale prices.

We should update Section 17 to reflect these strong quantitative findings. Specifically:

In II.A. Property Type Influence: We can now add the specific MAPE figures.
"Condominiums: Frequently observed selling for substantially more than their assessed value. The average 'MAPE' for assessments against actual sale prices for Condominiums is 85.9%, while the predictive model achieves a MAPE of 7.8% for this category."
"Residential: Also exhibit significant assessment inaccuracies, with an average assessment 'MAPE' of 114.2% against actual sale prices. The predictive model achieves a MAPE of 8.7% for Residential properties."
In II.D. General Model Utility: We can add the overall MAPE comparison.
"The developed XGBoost model (overall MAPE of 8.7%) consistently provides sale price predictions that are significantly closer to actual market transactions than the currently listed assessed taxable values (overall assessment 'MAPE' of 113.7%)."
In III.A. For Assessment Authorities:
When recommending review for Condominiums, we can cite the 85.9% vs 7.8% figures.
We need to add a stronger recommendation for a broad review of Residential property assessments, citing the 114.2% vs 8.7% figures. This is now quantitatively shown to be an even larger average discrepancy than for condos, though the nature of the discrepancies might differ (e.g., new condos being extreme under-assessments, while residential might be a mix including significant over-assessments of high-value properties).

In II. Key Findings:

Under B. Anomalous Neighborhood Deep Dive - Neighborhood "401" (New Condominiums):
We could add a sentence like: "Further analysis of Neighborhood "402" (also entirely condominiums but with mixed ages including 1968 and 2004 builds) showed similar assessment challenges, with individual properties being both significantly under-assessed (e.g., a 1968 condo selling for 733% of its assessed value) and over-assessed. The model maintained high accuracy (6.0% MAPE) in this mixed scenario, while the aggregate sale price to taxable value ratio was 77.2%."
Under C. Anomalous Neighborhood Deep Dive - Neighborhood "030512" (Waterfront Residentials):
We could add a sentence like: "This pattern of potential over-assessment for high-value waterfront residential properties was echoed in Neighborhood "121130" (entirely residential, average 1976 build, "WF Salt" designation). Here, properties sold for an average of 40.5% of their very high taxable values (avg.$2M), with the model achieving a 10.4% MAPE. This suggests a systemic issue with how certain waterfront properties are valued by assessors versus the market."
In III. Actionable Insights & Potential Recommendations:

Under A.1. Prioritize Review of Neighborhood "401": We can broaden this slightly.
"Prioritize Review of Condominium Assessments: Conduct an immediate review of assessment methodologies and values for condominium developments, particularly new ones (e.g., Neighborhood "401") but also encompassing older and mixed-age condo neighborhoods (e.g., Neighborhood "402") where significant under- and over-assessment discrepancies were observed. Ensure assessments reflect current market values."
Under A.2. Investigate Waterfront Assessments (Neighborhood "030512" as case study): We can add "121130".
"Investigate Waterfront Assessments (Neighborhoods "030512" - WF Lake, and "121130" - WF Salt as case studies): Review how "Waterfront" status contributes to assessed value, particularly for residential properties (both older and more recent builds) that may have condition issues or other factors not fully captured. Consider incorporating more granular data..."

cat("\n--- Section 17: Summary Complete (Drafted in Comments) ---\n")
```

```{r}
# --- Section 18: Quantifying Model vs. Assessment Accuracy ---
cat("\n\n--- Section 18: Quantifying Model vs. Assessment Accuracy ---\n")

# Ensure error_analysis_df exists and has the required columns
if (!exists("error_analysis_df") || 
    !all(c("Actual_Sale_Price_Original", "Predicted_Sale_Price_Original", "Tax_Summary_Taxable_Value", "Property_Type") %in% names(error_analysis_df))) {
  cat("Error: 'error_analysis_df' not found or missing required columns for Section 18.\n")
  cat("Please ensure Sections 11-16 have been run successfully.\n")
} else {
  
  # Load dplyr if not already loaded
  if (!require(dplyr)) {
    install.packages("dplyr")
    library(dplyr)
  }
  if (!require(scales)) { # For percent formatting
    install.packages("scales")
    library(scales)
  }

  # --- Helper function to calculate MAPE ---
  calculate_mape <- function(actual, predicted) {
    # Ensure no division by zero if actual is 0, and handle NAs
    # We should filter out Actual_Sale_Price_Original == 0 before calling this if they exist
    valid_indices <- !is.na(actual) & !is.na(predicted) & actual != 0
    if (sum(valid_indices) == 0) return(NA_real_) # Return NA if no valid data points
    
    mean(abs((actual[valid_indices] - predicted[valid_indices]) / actual[valid_indices]))
  }

  cat("\nCalculating Overall Model MAPE vs. Assessment MAPE...\n")

  # Filter out rows where Actual_Sale_Price_Original is 0 or NA, as MAPE is undefined
  # Also filter where Taxable_Value or Predicted_Sale_Price is NA for fair comparison
  mape_calculation_df <- error_analysis_df %>%
    filter(!is.na(Actual_Sale_Price_Original) & Actual_Sale_Price_Original > 0 &
           !is.na(Predicted_Sale_Price_Original) &
           !is.na(Tax_Summary_Taxable_Value)) # Ensure Taxable Value is also present for its MAPE

  if(nrow(mape_calculation_df) == 0) {
    cat("No valid data for MAPE calculation after filtering.\n")
  } else {
    # Overall MAPE
    overall_model_mape <- calculate_mape(mape_calculation_df$Actual_Sale_Price_Original, 
                                         mape_calculation_df$Predicted_Sale_Price_Original)
    
    overall_assessment_mape <- calculate_mape(mape_calculation_df$Actual_Sale_Price_Original, 
                                              mape_calculation_df$Tax_Summary_Taxable_Value)

    cat("\n--- Overall Accuracy Comparison ---\n")
    cat(paste0("Number of sales considered for overall MAPE: ", nrow(mape_calculation_df), "\n"))
    cat(paste0("Overall Model MAPE:                       ", scales::percent(overall_model_mape, accuracy = 0.1), "\n"))
    cat(paste0("Overall Assessment 'MAPE':                ", scales::percent(overall_assessment_mape, accuracy = 0.1), "\n"))

    # MAPE by Property Type
    cat("\n\n--- Accuracy Comparison by Property Type ---\n")
    
    mape_by_property_type <- mape_calculation_df %>%
      group_by(Property_Type) %>%
      summarise(
        Num_Sales = n(),
        Model_MAPE = calculate_mape(Actual_Sale_Price_Original, Predicted_Sale_Price_Original),
        Assessment_MAPE = calculate_mape(Actual_Sale_Price_Original, Tax_Summary_Taxable_Value),
        .groups = 'drop'
      ) %>%
      arrange(desc(Num_Sales)) %>% # Show most common property types first
      mutate(
        Model_MAPE_Formatted = scales::percent(Model_MAPE, accuracy = 0.1),
        Assessment_MAPE_Formatted = scales::percent(Assessment_MAPE, accuracy = 0.1)
      )

    # Print using kable for better formatting if available
    if (require(knitr)) {
      print(kable(mape_by_property_type %>% select(Property_Type, Num_Sales, Model_MAPE_Formatted, Assessment_MAPE_Formatted),
                  col.names = c("Property Type", "Num Sales", "Model MAPE", "Assessment 'MAPE'"),
                  caption = "Model MAPE vs. Assessment 'MAPE' by Property Type"))
    } else {
      print(mape_by_property_type %>% select(Property_Type, Num_Sales, Model_MAPE_Formatted, Assessment_MAPE_Formatted))
    }
    
    # Identify property types where assessment MAPE is particularly high or model shows big improvement
    significant_difference_threshold <- 0.10 # e.g., model is 10 percentage points better
    
    assessment_issues_highlight <- mape_by_property_type %>%
      filter(Assessment_MAPE > 0.30 | (Assessment_MAPE - Model_MAPE > significant_difference_threshold)) %>% # High assessment MAPE or big improvement by model
      arrange(desc(Assessment_MAPE))
      
    if(nrow(assessment_issues_highlight) > 0){
      cat("\n\n--- Property Types with Notable Assessment Discrepancies or Model Improvement ---\n")
      if (require(knitr)) {
        print(kable(assessment_issues_highlight %>% select(Property_Type, Num_Sales, Model_MAPE_Formatted, Assessment_MAPE_Formatted),
                  col.names = c("Property Type", "Num Sales", "Model MAPE", "Assessment 'MAPE'"),
                  caption = "Property Types: High Assessment 'MAPE' or Significant Model Improvement"))
      } else {
        print(assessment_issues_highlight %>% select(Property_Type, Num_Sales, Model_MAPE_Formatted, Assessment_MAPE_Formatted))
      }
    } else {
      cat("\nNo property types met the criteria for highlighting significant assessment discrepancies based on the current thresholds.\n")
    }
  }
}
cat("\n--- Section 18: Quantifying Accuracy Complete ---\n")
```

```{r}
# --- Section 19: Brief Analysis of Neighborhoods "402" and "121130" ---
cat("\n\n--- Section 19: Brief Analysis of Neighborhoods \"402\" and \"121130\" ---\n")

# Ensure error_analysis_df exists
if (!exists("error_analysis_df")) {
  cat("Error: 'error_analysis_df' not found. Please ensure previous sections have run.\n")
} else {
  
  # Load dplyr, knitr, scales if not already loaded
  if (!require(dplyr)) { install.packages("dplyr"); library(dplyr) }
  if (!require(knitr)) { install.packages("knitr"); library(knitr) }
  if (!require(scales)) { install.packages("scales"); library(scales) }

  # --- Helper function to calculate MAPE (if not already in environment) ---
  if (!exists("calculate_mape")) {
    calculate_mape <- function(actual, predicted) {
      valid_indices <- !is.na(actual) & !is.na(predicted) & actual != 0
      if (sum(valid_indices) == 0) return(NA_real_)
      mean(abs((actual[valid_indices] - predicted[valid_indices]) / actual[valid_indices]))
    }
  }
  
  # --- Function to perform brief analysis for a given neighborhood ---
  analyze_neighborhood_briefly <- function(nhood_code, df) {
    cat(paste0("\n\n--- Analyzing Neighborhood: ", nhood_code, " ---\n"))
    
    nhood_data <- df %>% filter(Neighborhood_Summary == nhood_code)
    num_properties <- nrow(nhood_data)
    
    if (num_properties == 0) {
      cat(paste0("No properties found for Neighborhood_Summary: ", nhood_code, "\n"))
      return()
    }
    
    cat(paste0("Found ", num_properties, " properties.\n"))
    
    # Property Type Summary
    cat("\nProperty Types:\n")
    property_type_summary <- nhood_data %>%
      group_by(Property_Type) %>%
      summarise(Count = n(), .groups = 'drop') %>%
      arrange(desc(Count))
    print(kable(property_type_summary))
    
    # Average Metrics
    avg_year_built <- mean(nhood_data$Year_Built, na.rm = TRUE)
    avg_actual_sale_price <- mean(nhood_data$Actual_Sale_Price_Original, na.rm = TRUE)
    avg_taxable_value <- mean(nhood_data$Tax_Summary_Taxable_Value, na.rm = TRUE)
    avg_predicted_price <- mean(nhood_data$Predicted_Sale_Price_Original, na.rm = TRUE)
    
    cat("\nAverage Metrics:\n")
    cat(paste0("  Avg. Year Built: ", round(avg_year_built, 0), "\n"))
    cat(paste0("  Avg. Actual Sale Price: ", dollar(avg_actual_sale_price, accuracy = 1), "\n"))
    cat(paste0("  Avg. Taxable Value:     ", dollar(avg_taxable_value, accuracy = 1), "\n"))
    cat(paste0("  Avg. Predicted Price:   ", dollar(avg_predicted_price, accuracy = 1), "\n"))
    
    # Assessment Ratio (Actual/Taxable)
    avg_assessment_ratio <- avg_actual_sale_price / avg_taxable_value
    cat(paste0("  Avg. Ratio (Actual Sale / Taxable): ", percent(avg_assessment_ratio, accuracy = 0.1), "\n"))
    
    # Model MAPE for this neighborhood
    model_mape_nhood <- calculate_mape(nhood_data$Actual_Sale_Price_Original, 
                                       nhood_data$Predicted_Sale_Price_Original)
    cat(paste0("  Model MAPE for this neighborhood:   ", percent(model_mape_nhood, accuracy = 0.1), "\n"))

    # Display a few sample properties (ordered by ratio for consistency with previous deep dives)
    # For "high ratio" neighborhoods, order desc; for "low ratio", order asc.
    # We'll determine order based on whether avg_assessment_ratio is > or < 1.
    
    cat("\nSample Properties (Top 5 by discrepancy magnitude):\n")
    sample_properties <- nhood_data %>%
      mutate(Discrepancy = abs(Actual_Sale_Price_Original - Tax_Summary_Taxable_Value)) %>%
      arrange(desc(Discrepancy)) %>% # Show largest dollar discrepancies
      head(5) %>%
      select(
        Actual_Sale_Price_Original, Tax_Summary_Taxable_Value, Assessed_vs_Actual_Ratio,
        Predicted_Sale_Price_Original, Property_Type, Year_Built, Main_Building_Quality,
        Waterfront_Type_Summary # Keep this for potential relevance
      ) %>%
      mutate(
        Actual_Sale_Price_Original = dollar(Actual_Sale_Price_Original, accuracy = 1),
        Tax_Summary_Taxable_Value = dollar(Tax_Summary_Taxable_Value, accuracy = 1),
        Assessed_vs_Actual_Ratio = percent(Assessed_vs_Actual_Ratio, accuracy = 0.1),
        Predicted_Sale_Price_Original = dollar(Predicted_Sale_Price_Original, accuracy = 1)
      )
    print(kable(sample_properties))
  }

  # --- Analyze the selected neighborhoods ---
  neighborhood_list_s19 <- c("402", "121130")
  
  # Filter error_analysis_df once for efficiency if it's large
  # and has all necessary columns for MAPE calculation
  analysis_base_df_s19 <- error_analysis_df %>%
    filter(!is.na(Actual_Sale_Price_Original) & Actual_Sale_Price_Original > 0 &
           !is.na(Predicted_Sale_Price_Original) &
           !is.na(Tax_Summary_Taxable_Value) &
           !is.na(Year_Built) & !is.na(Property_Type) & # Add other columns used in analyze_neighborhood_briefly
           !is.na(Main_Building_Quality) & !is.na(Waterfront_Type_Summary) & !is.na(Assessed_vs_Actual_Ratio)
           )


  for (nhood in neighborhood_list_s19) {
    analyze_neighborhood_briefly(nhood, analysis_base_df_s19)
  }
}
cat("\n--- Section 19: Brief Neighborhood Analysis Complete ---\n")
```


```{r}
# --- Section 20: Report Visualizations ---
cat("\n\n--- Section 20: Report Visualizations ---\n")

# Ensure necessary data frames and values exist from Section 18
if (!exists("mape_calculation_df") || !exists("overall_model_mape") || 
    !exists("overall_assessment_mape") || !exists("mape_by_property_type")) {
  cat("Error: Data from Section 18 (mape_calculation_df, overall MAPEs, mape_by_property_type) not found.\n")
  cat("Please ensure Section 18 has been run successfully.\n")
} else {

  # Load ggplot2 if not already loaded
  if (!require(ggplot2)) {
    install.packages("ggplot2")
    library(ggplot2)
  }
  if (!require(scales)) { # For percent formatting in plots
    install.packages("scales")
    library(scales)
  }
  if (!require(dplyr)) { # For data manipulation if needed
    install.packages("dplyr")
    library(dplyr)
  }

  # --- Visual 1: Overall Model MAPE vs. Assessment "MAPE" Bar Chart ---
  cat("\nGenerating Visual 1: Overall MAPE Comparison Bar Chart...\n")
  
  overall_mape_data <- data.frame(
    Source = factor(c("Predictive Model", "County Assessment"), 
                    levels = c("Predictive Model", "County Assessment")),
    MAPE = c(overall_model_mape, overall_assessment_mape)
  )
  
  plot_overall_mape <- ggplot(overall_mape_data, aes(x = Source, y = MAPE, fill = Source)) +
    geom_bar(stat = "identity", width = 0.6) +
    geom_text(aes(label = scales::percent(MAPE, accuracy = 0.1)), 
              vjust = -0.5, size = 4, color = "black") +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits = c(0, max(overall_mape_data$MAPE) * 1.1)) +
    labs(title = "Overall Predictive Accuracy Comparison",
         subtitle = paste0("Based on ", nrow(mape_calculation_df), " property sales"),
         x = "",
         y = "Mean Absolute Percentage Error (MAPE)") +
    theme_minimal(base_size = 14) +
    theme(legend.position = "none",
          plot.title = element_text(hjust = 0.5, face = "bold"),
          plot.subtitle = element_text(hjust = 0.5)) +
    scale_fill_manual(values = c("Predictive Model" = "#377EB8", "County Assessment" = "#E41A1C"))

  print(plot_overall_mape)
  ggsave("Rplot_OverallMAPE_Comparison.png", plot = plot_overall_mape, width = 7, height = 5)
  cat("Saved Rplot_OverallMAPE_Comparison.png\n")

  # --- Visual 2: Grouped Bar Chart: MAPE by Property Type ---
  cat("\nGenerating Visual 2: MAPE by Property Type Grouped Bar Chart...\n")
  
  # Reshape data for grouped bar chart
  mape_by_type_long <- mape_by_property_type %>%
    select(Property_Type, Model_MAPE, Assessment_MAPE) %>%
    rename(`Predictive Model` = Model_MAPE, `County Assessment` = Assessment_MAPE) %>%
    tidyr::pivot_longer(cols = c(`Predictive Model`, `County Assessment`), 
                        names_to = "Source", values_to = "MAPE") %>%
    mutate(Source = factor(Source, levels = c("Predictive Model", "County Assessment")))

  plot_mape_by_type <- ggplot(mape_by_type_long, aes(x = Property_Type, y = MAPE, fill = Source)) +
    geom_bar(stat = "identity", position = position_dodge(width = 0.9), width = 0.8) +
    geom_text(aes(label = scales::percent(MAPE, accuracy = 0.1)), 
              position = position_dodge(width = 0.9), vjust = -0.5, size = 3.5) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits = c(0, max(mape_by_type_long$MAPE, na.rm=TRUE) * 1.15 )) +
    labs(title = "Predictive Accuracy Comparison by Property Type",
         x = "Property Type",
         y = "Mean Absolute Percentage Error (MAPE)",
         fill = "Prediction Source") +
    theme_minimal(base_size = 14) +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          legend.position = "top") +
    scale_fill_manual(values = c("Predictive Model" = "#377EB8", "County Assessment" = "#E41A1C"))
    
  print(plot_mape_by_type)
  ggsave("Rplot_MAPE_By_PropertyType.png", plot = plot_mape_by_type, width = 8, height = 6)
  cat("Saved Rplot_MAPE_By_PropertyType.png\n")

  # --- Visual 4: Scatter Plot: Actual Sale Price vs. Taxable Value (Assessment) ---
  cat("\nGenerating Visual 4: Scatter Plot of Actual Sale Price vs. Taxable Value...\n")
  
  # Use a sample for plotting if the dataset is very large to avoid overplotting
  # Let's use mape_calculation_df as it's already filtered for valid values
  sample_size_scatter <- min(nrow(mape_calculation_df), 5000) # Plot up to 5000 points
  scatter_sample_df <- mape_calculation_df[sample(nrow(mape_calculation_df), sample_size_scatter), ]

  plot_actual_vs_taxable <- ggplot(scatter_sample_df, 
                                   aes(x = Tax_Summary_Taxable_Value, y = Actual_Sale_Price_Original)) +
    geom_point(alpha = 0.3, aes(color = Property_Type)) + # Color by property type
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red", linewidth = 1) + # y=x line
    scale_x_continuous(labels = scales::dollar_format(scale = 1e-6, suffix = "M"), limits = c(0, quantile(mape_calculation_df$Tax_Summary_Taxable_Value, 0.99, na.rm=TRUE))) +
    scale_y_continuous(labels = scales::dollar_format(scale = 1e-6, suffix = "M"), limits = c(0, quantile(mape_calculation_df$Actual_Sale_Price_Original, 0.99, na.rm=TRUE))) +
    labs(title = "Actual Sale Price vs. Assessed Taxable Value",
         subtitle = "Red dashed line represents Perfect Agreement (Price = Assessment)",
         x = "Assessed Taxable Value",
         y = "Actual Sale Price",
         color = "Property Type") +
    theme_minimal(base_size = 14) +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          plot.subtitle = element_text(hjust = 0.5),
          legend.position = "top") +
    coord_fixed(ratio = 1) # Ensure 1:1 aspect ratio for better comparison with y=x line

  print(plot_actual_vs_taxable)
  ggsave("Rplot_ActualVsTaxableValue.png", plot = plot_actual_vs_taxable, width = 8, height = 7)
  cat("Saved Rplot_ActualVsTaxableValue.png\n")

  cat("\n--- Visuals 1, 2, and 4 generated and saved. ---\n")
  
  # --- Notes on Other Visuals ---
  cat("\n--- Notes on Other Visualizations ---\n")
  
  cat("Visual 3: Scatter Plot - Actual Sale Price vs. Predicted Sale Price (Model)\n")
  cat("  - This was likely generated in an earlier section.\n")
  cat("  - Filename: Rplot_ActualVsPredicted_Original.png (or similar from Section 11).\n")
  cat("  - This plot demonstrates the model's general predictive accuracy.\n\n")
  
  cat("Visual 5: Histograms of Assessed_vs_Actual_Ratio\n")
  cat("  - These were generated in Section 12.\n")
  cat("  - Filenames: Rplot01.png (overall ratio), Rplot.png (ratio by property type).\n")
  cat("  - These show the distribution of discrepancies between sale price and assessed value.\n\n")

  cat("Visual 7: Tables for Anomalous Neighborhoods\n")
  cat("  - The data for these tables was printed to the console in Sections 15, 16, and 19.\n")
  cat("  - For a report, re-run the dplyr summaries and pipe to `knitr::kable()` for well-formatted tables.\n")
  cat("  - Example for one neighborhood (e.g., '401'):\n")
  cat("    # nhood_401_summary_table <- error_analysis_df %>% filter(Neighborhood_Summary == '401') %>% ... [your summary code] ...\n")
  cat("    # print(knitr::kable(nhood_401_summary_table, caption = 'Summary for Neighborhood 401'))\n\n")

  cat("Visual 6: Maps\n")
  cat("  - Creating impactful maps requires geographic data (latitude/longitude for each property or shapefiles for neighborhoods) and R packages like `sf`, `leaflet`, or `ggmap`.\n")
  cat("  - A simpler approach could be to plot points for the anomalous neighborhoods if lat/long is available for them.\n")
  cat("  - For example, you could create a scatter plot of Avg_Longitude vs. Avg_Latitude for all neighborhoods, color-coding the ones we analyzed (401, 030512, 402, 121130) to show their relative locations.\n")
  cat("  - This is more advanced and can be considered for further development if detailed geographic analysis is a priority.\n")

}
cat("\n--- Section 20: Report Visualizations Complete ---\n")
```



