---
title: "Linear Regression (incl. Lasso)"
author: "B1_Pierce-House-Price"
date: "today"
format: 
  html:
    toc: true
    toc-depth: 3
    theme: cosmo 
    code-fold: true
    code-summary: "Show/Hide Code"
    df-print: kable # For nice table printing with knitr::kable
editor: source
execute:
  echo: true
  warning: false   # Suppress warnings globally for now
  message: false   # Suppress messages globally for now
  error: true      # Display errors if they occur
---

```{r}
library(dplyr)
library(readr)

full_data <- read_csv("../full_data_latest_valid.csv", show_col_types = FALSE)
```

```{r}
names(full_data)

```

```{r}
library(dplyr)

full_data <- full_data %>%
  dplyr::select(
    Sale_Date_Raw,
    Sale_Price_Raw,
    Square_Feet_Raw,
    Latitude_Raw,
    Longitude_Raw,
    Bedrooms_Raw,
    Bathrooms_Raw,
    Stories_Raw,
    Quality,
    Condition,
    Neighborhood,
    View_Quality,
    Street_Type,
    Utility_Water,
    Utility_Electric,
    Utility_Sewer,
    Improved_Vacant_Raw,
    Year_Built_Raw
  ) %>%
  mutate(
    Sale_Date_Raw       = as.Date(Sale_Date_Raw),
    Sale_Price_Raw      = as.numeric(Sale_Price_Raw),
    Square_Feet_Raw     = as.numeric(Square_Feet_Raw),
    Latitude_Raw        = as.numeric(Latitude_Raw),
    Longitude_Raw       = as.numeric(Longitude_Raw),
    Bedrooms_Raw        = as.numeric(Bedrooms_Raw),
    Bathrooms_Raw       = as.numeric(Bathrooms_Raw),
    Stories_Raw         = as.numeric(Stories_Raw),
    Quality             = as.factor(Quality),
    Condition           = as.factor(Condition),
    Neighborhood        = as.factor(Neighborhood),
    View_Quality        = as.factor(View_Quality),
    Street_Type         = as.factor(Street_Type),
    Utility_Water       = as.factor(Utility_Water),
    Utility_Electric    = as.factor(Utility_Electric),
    Utility_Sewer       = as.factor(Utility_Sewer),
    Improved_Vacant_Raw = as.factor(Improved_Vacant_Raw),
    Year_Built_Raw      = as.numeric(Year_Built_Raw)
  )
```


# Data Preprocessing

```{r}
# View structure and summary before preprocessing
str(full_data)
summary(full_data)
```


```{r}
# Count missing values per column
missing_summary <- sapply(full_data, function(x) sum(is.na(x)))
missing_summary <- sort(missing_summary, decreasing = TRUE)
print(missing_summary)

# Optionally show % missing
missing_pct <- round(missing_summary / nrow(full_data) * 100, 2)
data.frame(Column = names(missing_pct), Missing = missing_summary, Percent = missing_pct)

```



```{r}
# 1. Drop View_Quality entirely
full_data <- full_data %>% dplyr::select(-View_Quality)

# 2. Convert fields to numeric
full_data <- full_data %>%
  mutate(
    Bedrooms_Raw    = as.numeric(Bedrooms_Raw),
    Bathrooms_Raw   = as.numeric(Bathrooms_Raw),
    Stories_Raw     = as.numeric(Stories_Raw),
    Year_Built_Raw  = as.numeric(Year_Built_Raw),
    Square_Feet_Raw = as.numeric(Square_Feet_Raw),
    Latitude_Raw    = as.numeric(Latitude_Raw),
    Longitude_Raw   = as.numeric(Longitude_Raw)
  )

# 3. Drop rows with missing values in crucial fields
full_data <- full_data %>%
  filter(
    !is.na(Square_Feet_Raw),
    !is.na(Quality),
    !is.na(Condition),
    !is.na(Neighborhood),
    !is.na(Latitude_Raw),
    !is.na(Longitude_Raw)
  )

# 4. Impute remaining missing values with median
full_data$Bedrooms_Raw[is.na(full_data$Bedrooms_Raw)]     <- median(full_data$Bedrooms_Raw, na.rm = TRUE)
full_data$Bathrooms_Raw[is.na(full_data$Bathrooms_Raw)]   <- median(full_data$Bathrooms_Raw, na.rm = TRUE)
full_data$Stories_Raw[is.na(full_data$Stories_Raw)]       <- median(full_data$Stories_Raw, na.rm = TRUE)
full_data$Year_Built_Raw[is.na(full_data$Year_Built_Raw)] <- median(full_data$Year_Built_Raw, na.rm = TRUE)
```

```{r}
full_data
```


```{r}
# View structure and summary after preprocessing
str(full_data)
summary(full_data)
```


```{r}
# Remove factor columns with fewer than 2 levels
full_data <- full_data %>%
  dplyr::select(where(~ !(is.factor(.) && nlevels(.) < 2)))
```


### Standardization

Saving a copy before standardization

```{r}
full_data_raw <- full_data
```

```{r}
numeric_vars <- c("Square_Feet_Raw", "Bedrooms_Raw", "Bathrooms_Raw", 
                  "Stories_Raw", "Latitude_Raw", "Longitude_Raw", "Year_Built_Raw")

full_data[numeric_vars] <- scale(full_data[numeric_vars])
```


# Baseline Linear Model

```{r}
set.seed(123)  # for reproducibility

# Create a vector of row indices for training set (80% train, 20% test)
train_indices <- sample(seq_len(nrow(full_data)), size = 0.8 * nrow(full_data))

# Split the data
train_data <- full_data[train_indices, ]
test_data  <- full_data[-train_indices, ]
```


```{r}
lm_model <- lm(Sale_Price_Raw ~ ., data = train_data)
summary(lm_model)
```
This baseline linear regression model, trained on the uncollapsed training data with full neighborhood granularity, achieves an R² of 0.3157, indicating that approximately 31.57% of the variation in sale prices is explained by the included predictors. The residual standard error is around $1.09 million, reflecting the average deviation of predictions from actual sale prices.

Key variables such as Sale_Date_Raw, Square_Feet_Raw, and several Quality levels (e.g., Excellent, Very Good) are highly significant and contribute strongly to the model. Latitude has a statistically significant negative relationship with price, while Longitude is not significant. Interestingly, some bedrooms and bathroom counts show strong effects, but many neighborhoods do not show statistical significance—highlighting the sparsity and noise introduced by the large number of neighborhood categories.

In summary, while the model has slightly better explanatory power than the collapsed version, the inclusion of over 300 neighborhood dummies introduces noise and multicollinearity, suggesting potential overfitting and model complexity that could be reduced.


## Making Predictions and Evaluation

```{r}
preds <- predict(lm_model, newdata = test_data)
actuals <- test_data$Sale_Price_Raw

# Evaluate
RMSE <- sqrt(mean((preds - actuals)^2))
MAE <- mean(abs(preds - actuals))
R2 <- 1 - sum((preds - actuals)^2) / sum((actuals - mean(actuals))^2)

cat(sprintf("Test RMSE: %.2f\nMAE: %.2f\nR²: %.3f\n", RMSE, MAE, R2))
```

The linear regression model yielded a Root Mean Square Error (RMSE) of approximately $1.15 million, a Mean Absolute Error (MAE) of about $185,773, and an R-squared (R²) value of 0.348 on the test set. This indicates that the model captures roughly 35% of the variance in sale prices, meaning that while it explains some of the underlying trends, a substantial portion of the variation remains unaccounted for. The relatively high RMSE suggests that individual price predictions can deviate significantly from the actual values — especially for high-priced properties. These results imply that either the relationship between predictors and price is more complex than a linear model can capture, or that additional relevant variables and transformations (like log transformation or outlier treatment) are needed to improve accuracy.

Several predictors are highly significant (e.g., Square_Feet_Raw, Sale_Date_Raw, Quality, Year_Built_Raw, and Neighborhood), confirming these are strong drivers of price.

However, many neighborhood factors have high p-values, indicating they contribute little and may be noise, which can inflate model complexity without improving performance.


With 312 neighborhoods, the linear regression summary becomes nearly unreadable, and model interpretability and generalization both suffer due to:

- High dimensionality from one-hot encoding

- Multicollinearity across neighborhoods

- Many sparse levels (some with <50 observations)


## Collapse Rare Neighborhoods

```{r}
library(forcats)

# Collapse rare Neighborhood levels in full_data BEFORE splitting
full_data$Neighborhood <- fct_lump_min(full_data$Neighborhood, min = 100, other_level = "Other")

# Optional: limit to top 20 frequent levels only (will still include 'Other')
full_data$Neighborhood <- fct_lump_n(full_data$Neighborhood, n = 20)

# Re-split the collapsed dataset using the same indices
train_collapsed <- full_data[train_indices, ]
test_collapsed <- full_data[-train_indices, ]

```




## Model Refit

```{r}
# Refit after collapsing Neighborhood
lm_model_clean <- lm(Sale_Price_Raw ~ ., data = train_collapsed)
summary(lm_model_clean)
```
This linear regression model, trained on the collapsed training data, explains about 23% of the variance in sale prices (R² = 0.230), indicating limited predictive power. The residual standard error of approximately $1.15 million reflects substantial average prediction error. Key predictors like square footage, sale date, bathrooms, bedrooms, and various quality levels are highly significant, confirming their influence on home prices. Some neighborhood and utility variables also show significance, though many remain statistically insignificant, suggesting they could be simplified further. Overall, collapsing rare neighborhoods helped manage complexity, but model fit remains moderate.

## Predict and Evaluate


```{r}
# Predict and evaluate
preds <- predict(lm_model_clean, newdata = test_collapsed)
actuals <- test_collapsed$Sale_Price_Raw

RMSE <- sqrt(mean((preds - actuals)^2))
MAE <- mean(abs(preds - actuals))
R2 <- 1 - sum((preds - actuals)^2) / sum((actuals - mean(actuals))^2)

cat(sprintf("Cleaned LM Model — RMSE: %.2f\nMAE: %.2f\nR²: %.3f\n", RMSE, MAE, R2))
```
After simplifying the Neighborhood variable by grouping less common neighborhoods into a single "Other" category, the linear model's predictive performance declined:

- Root Mean Squared Error (RMSE) rose to $1,213,242, indicating the average prediction is off by over $1.2 million, heavily influenced by large outliers.

- Mean Absolute Error (MAE) increased to $209,550, showing the typical error magnitude in sale price predictions.

- R² dropped to 0.277, meaning the model now explains only 27.7% of the variance in home sale prices — a reduction in explanatory power.


By collapsing Neighborhood, we lost fine-grained location data — a key driver of housing prices. Real estate value is highly localized, and removing specific neighborhood identifiers blunts the model's ability to capture that detail.



# Feature Selection (Stepwise)

```{r}
if (!require(MASS)) install.packages("MASS")
library(MASS)
step_model <- stepAIC(lm_model_clean, direction = "both")
summary(step_model)
```
This linear regression model, trained on the collapsed training dataset using stepwise AIC selection, explains about 23% of the variance in sale prices (R² = 0.2297), indicating moderate explanatory power. The residual standard error is approximately $1.15 million, suggesting substantial variability in predictions—expected given the broad range of property values. Statistically significant predictors (p < 0.05) include sale date, square footage, number of bedrooms and bathrooms, story count, building quality categories, some neighborhoods, street type, sewer availability, and year built. For example, higher square footage and better quality categories (e.g., "Excellent", "Very Good Plus") are associated with substantially higher sale prices, while factors like "STREET UNPAVED" or lacking sewer infrastructure are associated with lower values. Geographic features like longitude remain significant, whereas latitude was dropped during model selection. Overall, the model captures several meaningful drivers of property value, though with room for improvement in predictive accuracy.

# Model Evaluation

```{r}
par(mfrow = c(2, 2))
plot(lm_model_clean)
```
The residual vs. fitted plot (top-left) shows non-random patterns and heteroskedasticity (i.e., increasing spread with fitted values), suggesting the model’s variance isn't constant across all price ranges — particularly at higher price points. The Q-Q plot (top-right) reveals non-normal residuals, with a heavy tail on the right, indicating the presence of outliers or skewness (especially in high-value sales). The scale-location plot (bottom-left) confirms the heteroskedasticity, as the red line trends upward, showing that residual variance increases with predicted price. Lastly, the residuals vs. leverage plot (bottom-right) identifies a few high-leverage and high-residual points (e.g., observations like 1852 and 14769), which could have a disproportionate impact on model estimates — these might be extreme or unusual sales.



# Residual Distribution

```{r}
hist(resid(step_model), breaks = 50, main = "Residual Histogram")
```


# Lasso + Outlier Handling
```{r}
library(glmnet)

# Remove outliers based on standardized residuals from a base linear model
base_lm <- lm(Sale_Price_Raw ~ ., data = full_data)
std_resid <- rstandard(base_lm)
full_data_clean <- full_data[abs(std_resid) < 3, ]

# Re-sample after cleaning
set.seed(123)
train_indices <- sample(seq_len(nrow(full_data_clean)), size = 0.8 * nrow(full_data_clean))
train <- full_data_clean[train_indices, ]
test  <- full_data_clean[-train_indices, ]

# Prepare data for glmnet (model.matrix automatically handles factors)
x_train <- model.matrix(~ ., data = train[, !names(train) %in% "Sale_Price_Raw"])[, -1]
y_train <- train$Sale_Price_Raw

x_test <- model.matrix(~ ., data = test[, !names(test) %in% "Sale_Price_Raw"])[, -1]
y_test <- test$Sale_Price_Raw

# Fit Lasso with cross-validation
set.seed(123)
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1)

# Optimal lambda
best_lambda <- cv_lasso$lambda.min

# Fit final Lasso model
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda)

# Predict and evaluate
pred_lasso <- predict(lasso_model, s = best_lambda, newx = x_test)

RMSE <- sqrt(mean((pred_lasso - y_test)^2))
MAE <- mean(abs(pred_lasso - y_test))
R2 <- 1 - sum((pred_lasso - y_test)^2) / sum((y_test - mean(y_test))^2)

cat(sprintf("Lasso RMSE: %.2f\nLasso MAE: %.2f\nLasso R²: %.3f\n\n", RMSE, MAE, R2))

# Show non-zero coefficients
coef_df <- as.matrix(coef(lasso_model, s = best_lambda))
non_zero <- coef_df[coef_df[, 1] != 0, , drop = FALSE]
cat("Selected predictors (non-zero coefficients):\n")
print(non_zero)
```

The Lasso regression model achieved strong predictive performance, with an RMSE of approximately $235,196, an MAE of $119,263, and an R² of 0.616, indicating that it explains about 61.6% of the variance in housing prices. This reflects a well-calibrated model capable of reasonably accurate predictions across a diverse dataset. Lasso’s regularization shrank many coefficients to zero, leaving a focused subset of influential predictors, including square footage (by far the most impactful), location (latitude, longitude, and specific neighborhoods), structural features (like number of bedrooms, stories, and year built), and detailed quality and condition ratings. Notably, high-quality homes (e.g., “Excellent” or “Very Good Plus”) and certain neighborhoods had large positive coefficients, while poor utility access (e.g., no sewer/septic or power) and lower condition ratings pulled prices down. Interestingly, properties lacking standard road access or with no power comments had elevated price signals, likely reflecting niche, high-value parcels. Overall, the model balances accuracy and interpretability, highlighting key drivers of home value while managing complexity and multicollinearity.


```{r}
summary(full_data$Sale_Price_Raw)
summary(full_data_clean$Sale_Price_Raw)
```
```{r}
# Remove rows with invalid sale prices
full_data <- full_data %>%
  filter(Sale_Price_Raw > 1000)  # Removes 0 and unrealistically low values

full_data_clean <- full_data_clean %>%
  filter(Sale_Price_Raw > 1000)  # Removes 0 and unrealistically low values
```

```{r}
summary(full_data$Sale_Price_Raw)
summary(full_data_clean$Sale_Price_Raw)

```
```{r}
# 1. Remove low sale prices and log-transform
full_datalog <- full_data_clean %>%
  mutate(Log_Sale_Price = log(Sale_Price_Raw)) %>%
  filter(is.finite(Log_Sale_Price))
```

```{r}
summary(full_datalog$Log_Sale_Price)
```
```{r}
summary(full_datalog)
```

# Stepwise + Log-Transformed Price

```{r}
# Ensure required libraries are available
library(dplyr)

set.seed(123)
split_idx <- sample(1:nrow(full_datalog), 0.8 * nrow(full_datalog))
train <- full_datalog[split_idx, ]
test  <- full_datalog[-split_idx, ]

# Step 4: Stepwise model selection (on log-transformed target)
model_full <- lm(Log_Sale_Price ~ ., data = train %>% dplyr::select(-Sale_Price_Raw))
model_step <- step(model_full, direction = "both", trace = FALSE)

preds <- predict(model_step, newdata = test)
actuals <- test$Log_Sale_Price

# Step 6: Metrics
RMSE <- sqrt(mean((preds - actuals)^2))
MAE  <- mean(abs(preds - actuals))
R2   <- 1 - sum((preds - actuals)^2) / sum((actuals - mean(actuals))^2)

cat(sprintf("Stepwise Log-LM RMSE: %.2f\nMAE: %.2f\nR²: %.3f\n", RMSE, MAE, R2))

```
```{r}
summary(model_full)
```

```{r}
summary(model_step)
```

The linear regression model using log-transformed sale prices achieved an R² of 0.484, meaning it explains approximately 48.4% of the variance in logged housing prices — a significant improvement over models on the raw price scale. The residual standard error of 0.574 (in log dollars) suggests reasonably tight clustering of predictions around the true values on the log scale. Many predictors are highly statistically significant, including square footage, sale date, latitude, number of bathrooms, and detailed quality and condition indicators. The model also captures meaningful geographic and infrastructure effects — for instance, certain neighborhoods and street types show strong associations, as do utility access variables like sewer/septic status and electrical service. Notably, lack of sewer or power access corresponds with substantial negative coefficients, while high-quality ratings (e.g., “Excellent” or “Very Good Plus”) are linked to large price premiums. Although this model doesn't regularize coefficients like Lasso and is more susceptible to multicollinearity, it performs well in terms of explanatory power and provides interpretable estimates of each variable's marginal impact. Overall, the log transformation clearly improves model behavior, and the results offer valuable insight into the structural and locational drivers of property value.


# Full Lasso Pipeline with Log-Transformed Target

```{r}
# Load required library
library(glmnet)

# Train-test split
set.seed(123)
split_idx <- sample(1:nrow(full_datalog), 0.8 * nrow(full_datalog))
train <- full_datalog[split_idx, ]
test <- full_datalog[-split_idx, ]

# Prepare matrices
x_train <- model.matrix(Log_Sale_Price ~ ., data = train[, !names(train) %in% c("Sale_Price_Raw")])[, -1]
y_train <- train$Log_Sale_Price

x_test <- model.matrix(Log_Sale_Price ~ ., data = test[, !names(test) %in% c("Sale_Price_Raw")])[, -1]
y_test <- test$Log_Sale_Price

# Lasso CV
set.seed(123)
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1)
best_lambda <- cv_lasso$lambda.min

# Final model
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda)

# Predict and transform back
preds <- predict(lasso_model, s = best_lambda, newx = x_test)
actuals <- y_test
RMSE <- sqrt(mean((preds - actuals)^2))
MAE <- mean(abs(preds - actuals))
R2 <- 1 - sum((preds - actuals)^2) / sum((actuals - mean(actuals))^2)

cat(sprintf("Log-Transformed Lasso RMSE: %.2f\nMAE: %.2f\nR²: %.3f\n", RMSE, MAE, R2))

# Non-zero coefficients
cat("Selected predictors (non-zero coefficients):\n")
print(coef(lasso_model)[coef(lasso_model) != 0])
```

The Lasso regression model trained on log-transformed sale prices achieved an RMSE of 0.57, an MAE of 0.32, and an R² of 0.495 on the log scale, indicating a reasonably strong fit in terms of explained variance. However, when the predictions were back-transformed to the original dollar scale, the model’s performance declined: RMSE rose to $269,625, MAE to $161,591, and R² dropped to 0.260. This was worse than the original Lasso model without log transformation, which achieved lower error metrics and a higher R² (~0.303), suggesting that the log transformation did not yield the expected improvement.

While log-transforming the target variable is often beneficial for stabilizing variance and mitigating skewness, in this case it may have introduced distortion upon back-transformation or obscured meaningful structure in the raw sale prices. It's also possible that outliers and heteroscedasticity were already being effectively managed by Lasso’s inherent regularization, making the transformation redundant or even counterproductive.

Nonetheless, the model retained a concise set of meaningful predictors with non-zero coefficients, including square footage, location, home features, quality ratings, and utility access. This reaffirms Lasso’s strength in variable selection and multicollinearity control, even if the log-scale modeling choice did not ultimately enhance performance.


# Multicollinearity Check

```{r}
library(car)
vif(step_model)
```
The multicollinearity analysis using GVIF indicates that the predictors in the linear regression model are generally well-behaved. All variables show GVIF-adjusted values below the commonly cited concern threshold of 2, suggesting that multicollinearity is not a significant issue. The highest observed values are for Bathrooms_Raw (GVIF ≈ 1.75) and Bedrooms_Raw (GVIF ≈ 1.52), reflecting moderate correlation — which is expected, as both are typically larger in more spacious homes.

Other numeric predictors such as Longitude_Raw, Stories_Raw, and Year_Built_Raw also fall well within safe ranges. Importantly, categorical variables like Quality, Neighborhood, and Street_Type have low adjusted GVIF values, indicating they contribute unique and non-redundant information to the model. This reinforces confidence in the model's structure and suggests that feature inflation is minimal.

While no corrective action is strictly necessary, the slight overlap among housing feature variables may still benefit from dimensionality reduction or regularization techniques (e.g., Lasso) to further improve model stability and interpretability. Overall, the model’s predictors are statistically independent enough to support reliable inference and prediction.

```{r}
# Load necessary library
library(ggcorrplot)

# Subset only numeric predictors
numeric_data <- full_data_clean[, c("Sale_Price_Raw", "Square_Feet_Raw", "Bedrooms_Raw", 
                              "Bathrooms_Raw", "Stories_Raw", "Latitude_Raw", "Longitude_Raw")]

# Compute correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")

# Plot correlation matrix
ggcorrplot(cor_matrix,
           method = "circle",
           type = "lower",
           lab = TRUE,
           lab_size = 3,
           colors = c("red", "white", "blue"),
           title = "Correlation Matrix of Numeric Predictors",
           ggtheme = theme_minimal())

```
Decision Rule
- Remove the variable with higher avg. abs. correlation.

- If they’re close, consider which one is more interpretable or has stronger individual correlation with sale price.

```{r}
# Compute correlation matrix
cor_matrix <- cor(full_data_clean[, c("Square_Feet_Raw", "Bedrooms_Raw", "Bathrooms_Raw",
                                "Stories_Raw", "Latitude_Raw", "Longitude_Raw", "Sale_Price_Raw")],
                  use = "complete.obs")

# Compute average absolute correlation for each variable
avg_abs_corr <- apply(cor_matrix, 2, function(x) mean(abs(x[x != 1])))

# Sort descending
sort(avg_abs_corr, decreasing = TRUE)
```
Decision: Remove Bathrooms_Raw from your model going forward. It’s moderately (to even strongly) correlated with Bedrooms_Raw, and contributes more to collinearity.

```{r}
library(dplyr)

full_data_clean <- full_data_clean %>% dplyr::select(-Bathrooms_Raw)

```

```{r}
str(full_data_clean)
summary(full_data_clean$Sale_Price_Raw)
```


# Updated Linear Model (OLS) Without Bathrooms_Raw

```{r}

# Train-test split
set.seed(123)
split_idx <- sample(1:nrow(full_data_clean), 0.8 * nrow(full_data_clean))
train <- full_data_clean[split_idx, ]
test <- full_data_clean[-split_idx, ]

# Fit OLS model
lm_model <- lm(Sale_Price_Raw ~ ., data = train)
pred <- predict(lm_model, newdata = test)

# Evaluate
RMSE <- sqrt(mean((pred - test$Sale_Price_Raw)^2))
MAE <- mean(abs(pred - test$Sale_Price_Raw))
R2 <- 1 - sum((pred - test$Sale_Price_Raw)^2) / sum((test$Sale_Price_Raw - mean(test$Sale_Price_Raw))^2)

cat(sprintf("OLS (no Bathrooms) RMSE: %.2f\nMAE: %.2f\nR²: %.3f\n", RMSE, MAE, R2))

```
After removing Bathrooms_Raw—which exhibited moderate multicollinearity with Bedrooms_Raw—the ordinary least squares (OLS) regression model showed improved simplicity but only modest gains in performance. The model achieved an RMSE of $241,245, an MAE of $119,560, and an R² of 0.594, meaning it explains about 59.4% of the variance in housing prices. Compared to earlier versions, these results represent a moderate improvement in explanatory power and similar predictive accuracy, suggesting that eliminating the redundant bathroom variable slightly enhanced model stability without significantly changing its overall behavior.

This outcome supports the idea that reducing multicollinearity—by removing highly correlated predictors—can help streamline the model and improve interpretability, even without relying on more complex techniques like regularization or tuning. While the effect is not dramatic, it confirms that careful feature selection remains a valuable tool for building robust linear models.

# Updated Lasso Model Without Bathrooms_Raw

```{r}
# Split
set.seed(123)
split_idx <- sample(1:nrow(full_data_clean), 0.8 * nrow(full_data_clean))
train <- full_data_clean[split_idx, ]
test <- full_data_clean[-split_idx, ]

# Prepare data
x_train <- model.matrix(Sale_Price_Raw ~ ., data = train)[, -1]
y_train <- train$Sale_Price_Raw
x_test <- model.matrix(Sale_Price_Raw ~ ., data = test)[, -1]
y_test <- test$Sale_Price_Raw

# Cross-validated Lasso
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1)
best_lambda <- cv_lasso$lambda.min
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda)

# Predict & evaluate
pred <- predict(lasso_model, s = best_lambda, newx = x_test)
RMSE <- sqrt(mean((pred - y_test)^2))
MAE <- mean(abs(pred - y_test))
R2 <- 1 - sum((pred - y_test)^2) / sum((y_test - mean(y_test))^2)

cat(sprintf("Lasso (no Bathrooms) RMSE: %.2f\nMAE: %.2f\nR²: %.3f\n", RMSE, MAE, R2))

# Optional: show non-zero coefficients
cat("Selected predictors:\n")
print(coef(lasso_model)[coef(lasso_model) != 0])
```
After removing the multicollinear Bathrooms_Raw variable, the Lasso regression model achieved solid and stable performance, with an RMSE of $241,187, MAE of $119,425, and an R² of 0.595. These results are nearly identical to the corresponding simplified OLS model, indicating that Lasso remains competitive even when redundancy is manually addressed.

The model retains its core advantage: automatic feature selection through regularization, which results in a parsimonious set of predictors that captures key drivers of sale price without overfitting. The selected variables include a mix of structural attributes (e.g., square footage, stories), geographic features (latitude, longitude), quality ratings, and neighborhood indicators—reinforcing the model’s ability to isolate the most influential factors.

While the performance metrics are not dramatically better than OLS, Lasso's ability to handle multicollinearity implicitly and streamline the feature set makes it especially valuable for high-dimensional or noisy datasets. In this case, it offers a well-balanced trade-off between interpretability, robustness, and predictive accuracy.


# Removing Sale Price Outliers Using Quantiles

```{r}
# Remove top and bottom 1% of prices
q_low <- quantile(full_data_clean$Sale_Price_Raw, 0.01)
q_high <- quantile(full_data_clean$Sale_Price_Raw, 0.99)

trimmed_data <- full_data_clean %>%
  filter(Sale_Price_Raw >= q_low, Sale_Price_Raw <= q_high)
```

```{r}
# Load necessary libraries
library(caret)
library(Metrics)

set.seed(123)

trimmed_data <- droplevels(trimmed_data)

#Train/test split
split_idx <- sample(1:nrow(trimmed_data), 0.8 * nrow(trimmed_data))
train <- trimmed_data[split_idx, ]
test <- trimmed_data[-split_idx, ]


```

```{r}
summary(trimmed_data)
```

```{r}
# OLS model
ols_model <- lm(Sale_Price_Raw ~ ., data = train)
ols_pred <- predict(ols_model, newdata = test)

ols_rmse <- rmse(test$Sale_Price_Raw, ols_pred)
ols_mae <- mae(test$Sale_Price_Raw, ols_pred)
ols_r2 <- R2(ols_pred, test$Sale_Price_Raw)

cat(sprintf("OLS (outliers removed)\nRMSE: %.2f\nMAE: %.2f\nR²: %.3f\n\n", 
            ols_rmse, ols_mae, ols_r2))

```


```{r}
# Lasso model
x_train <- model.matrix(Sale_Price_Raw ~ ., train)[, -1]
y_train <- train$Sale_Price_Raw
x_test <- model.matrix(Sale_Price_Raw ~ ., test)[, -1]
y_test <- test$Sale_Price_Raw
# Already standardized manually — disable glmnet's internal standardization
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1, standardize = FALSE)
lasso_pred <- predict(cv_lasso, s = "lambda.min", newx = x_test)

lasso_rmse <- rmse(y_test, lasso_pred)
lasso_mae <- mae(y_test, lasso_pred)
lasso_r2 <- R2(lasso_pred, y_test)

cat(sprintf("Lasso (outliers removed)\nRMSE: %.2f\nMAE: %.2f\nR²: %.3f\n", 
            lasso_rmse, lasso_mae, lasso_r2))

```
After removing sale price outliers using quantile-based filtering, the performance of the OLS and Lasso models shifted notably. The OLS model improved, achieving an RMSE of $148,404, an MAE of $98,699, and an R² of 0.592, indicating that it now explains nearly 60% of the variance in prices. The reduction in both error metrics suggests that removing extreme values helped the linear model fit the central distribution more effectively, resulting in more stable and accurate predictions. In contrast, the Lasso model's performance declined, with an RMSE of $195,204, an MAE of $132,600, and an R² of just 0.294. This drop may indicate that Lasso had previously relied on the variance introduced by outliers to identify predictive structure, and that its regularization now overly penalizes the model in the absence of that variation. Overall, this comparison shows that OLS benefits significantly from outlier removal, while Lasso's advantage diminishes, highlighting the importance of aligning preprocessing choices with the strengths of the modeling technique.


# Conclusion

Following a comprehensive modeling process—spanning data cleaning, feature engineering, outlier removal, and algorithm selection—the Lasso regression model with standardized residual-based outlier filtering emerged as the best performer. This final model achieved an RMSE of approximately $235,196, MAE of $119,263, and an R² of 0.616, meaning it explains about 61.6% of the variance in residential sale prices. These results represent a strong balance between predictive accuracy and model simplicity, particularly given the real-world variability of property data.

The most impactful improvement came from removing outliers based on standardized residuals, which allowed the model to focus on more typical market behavior and avoid distortion from extreme values. While earlier models—especially those trained on unfiltered or unscaled data—suffered from inflated error metrics and unstable coefficients, this cleaned and refined dataset provided a much more stable learning foundation.

Notably, Lasso’s regularization played a key role by automatically shrinking less important variables and selecting a sparse set of predictors that included core structural features (e.g., square footage, stories), locational attributes (latitude, longitude, neighborhood), and detailed quality and condition indicators. Even with outliers removed and multicollinearity reduced, Lasso continued to deliver robust performance and improved interpretability over more complex models.

In conclusion, this Lasso model—built on rigorous preprocessing and smart regularization—offers a reliable, interpretable, and generalizable framework for predicting residential property values. Future enhancements might include non-linear models, interaction terms, or spatial modeling layers, but this approach already provides a solid baseline for practical applications.

Full formula of the final model of choice:

y =−32,872.71+46.44⋅Sale_Date_Raw+615,281.25⋅Square_Feet_Raw+27,144.44⋅Latitude_Raw−1,576.82⋅Longitude_Raw−26,897.08⋅Bedrooms_Raw−8,733.02⋅Stories_Raw+77,804.62⋅QualityAverage Plus+551,105.24⋅QualityExcellent−48,172.89⋅QualityFair−31,559.97⋅QualityFair Plus+141,495.64⋅QualityGood+285,763.34⋅QualityGood Plus−81,765.66⋅QualityLow−72,437.97⋅QualityLow Plus+341,569.84⋅QualityVery Good+814,971.25⋅QualityVery Good Plus−65,935.43⋅ConditionExtra Poor−3,581.34⋅ConditionFair+14,527.32⋅ConditionGood−17,954.62⋅ConditionPoor−33,236.11⋅ConditionUninhabitable−2,829.11⋅ConditionVery Poor+Neighborhood Coefficients (various)+75,015.49⋅Street_TypeSTREET NO ROAD+14,961.43⋅Street_TypeSTREET UNPAVED+13,493.34⋅Utility_ElectricPOWER INSTALLED−145,577.25⋅Utility_ElectricPOWER NO - COMMENT−444,262.58⋅Utility_SewerSEWER/SEPTIC AVAIL−527,653.39⋅Utility_SewerSEWER/SEPTIC INSTALLED−406,656.20⋅Utility_SewerSEWER/SEPTIC NO−251,631.65⋅Utility_SewerSEWER/SEPTIC NO PERC+184,513.50⋅Improved_Vacant_Raw1−9,154.39⋅Year_Built_Raw








