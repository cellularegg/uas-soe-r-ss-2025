---
title: "Ensamble"
author: "Karol Topor"
format:
  html:
    toc: true
    toc-depth: 6         
    toc-title: "Outline"
    number-sections: true
---

# Load libraries

```{r message=FALSE, warning=FALSE}
if (!require(readr)) install.packages("readr")
if (!require(data.table)) install.packages("data.table")
if (!require(dplyr)) install.packages("dplyr")
if (!require(caret)) install.packages("caret")
if (!require(tictoc)) install.packages("tictoc")
if (!require(Metrics)) install.packages("Metrics")
if (!require(MetricsWeighted)) install.packages("MetricsWeighted")
if (!require(here)) install.packages("here")

library(readr)
library(data.table)
library(dplyr)
library(caret)
library(catboost)
library(tictoc)
library(Metrics) 
library(MetricsWeighted)
library(here)
```

# Load data

```{r}
# Load data
data <- read_csv(here("data", "final_data.csv"),
                 col_types = cols(Sale_Date_Raw = col_date(format = "%Y-%m-%d")))

data <- data %>% select(-Price_Per_SqFt) 
```

## Columns names

```{r}
colnames(data)
```

## Describe the data

```{r}
summary(data)
```

# Modeling

## Prepare data

```{r}
set.seed(42)
deterministic = TRUE
idx   <- createDataPartition(data$Sale_Price_Raw, p = 0.80, list = FALSE)
train <- data[idx,  ]
test  <- data[-idx,  ]

#── feature prep helper ──────────────────────────────────────
prep <- function(df) {
  df %>%
    mutate(
      Sale_Date_Raw = as.numeric(Sale_Date_Raw),
      across(where(is.character), as.factor)
    ) %>%
    select(-Sale_Price_Raw)
}

X_tr <- prep(train); 
y_tr <- train$Sale_Price_Raw

X_te <- prep(test);  
y_te <- test$Sale_Price_Raw

train_pool <- catboost.load_pool(X_tr, label = y_tr)
test_pool <- catboost.load_pool(X_te, label = y_te)
```

## Helper functions
```{r}
evaluate_model <- function(actual, predicted) {
  mape_val <- mape(actual, predicted)
  mae_val  <- mae(actual, predicted)
  rmse_val <- rmse(actual, predicted)
  r2_val   <- R2(predicted, actual)
  
  cat(sprintf(
    "MAE: %.4f\nMAPE: %.4f\nRMSE: %.4f\nR²: %.4f\n",
    mae_val, mape_val, rmse_val, r2_val
  ))
  
  return(invisible(list(
    MAE  = mae_val,
    MAPE = mape_val,
    RMSE = rmse_val,
    R2   = r2_val
  )))
}

plot_predictions <- function(actual, predicted, title = "Predicted vs. Actual Sale Prices") {
  plot_df <- data.frame(
    Actual    = actual,
    Predicted = predicted
  )
  
  ggplot(plot_df, aes(x = Actual, y = Predicted)) +
    geom_point(alpha = 0.25) +
    geom_abline(slope = 1, intercept = 0,
                linetype = "dashed", color = "red", size = 0.8) +
    labs(
      title = title,
      x     = "Actual Sale_Price_Raw",
      y     = "Predicted Sale_Price_Raw"
    ) +
    theme_minimal()
}

```


## Simple CatBoost model

```{r}
tic("CatBoost training on CPU")

model_cpu <- catboost.train(
  learn_pool = train_pool,
  params = list(
    loss_function = "RMSE",
    iterations    = 64,
    learning_rate = 0.1,
    depth         = 3,
    l2_leaf_reg   = 3,
    logging_level = "Silent"
))

pred  <- catboost.predict(model_cpu, test_pool)
toc()
evaluate_model(y_te, pred)
plot_predictions(y_te, pred)
```


```{r}
catboost.get_feature_importance(model_cpu,
                                pool = NULL,
                                type = "FeatureImportance",
                                thread_count = -1)
```


# Cross Validation
The cross validation was done on the FH High-Performance Computing Cluster (HPC).

1. Creating a param grid
```bash
Rscript - <<'R'
library(data.table)
grid <- CJ(depth = c(10,11,12,13),
           learning_rate = c(0.03,0.02),
           iterations = c(1000,1200),
           l2_leaf_reg = c(3,4),
           rsm = 1,
           border_count = c(1024,2048,4096,8192))
fwrite(grid, "grid_03.csv")
R
```

2. create a slurm batchjob `catboost_array.sh`
```bash
#!/bin/sh
#SBATCH --job-name=catgrid
#SBATCH --nodes=10                 # use 10 of the 19 PCs
#SBATCH --ntasks-per-node=8        # 8 cores each  → 80 slots
#SBATCH -a 1-720%80                # 720 tasks, max 80 run at once
#SBATCH --time=12:00:00
#SBATCH --output=slurm-%A_%a.out

export R_LIBS_USER=$HOME/R/x86_64-pc-linux-gnu-library/4.3
.libPaths("$R_LIBS_USER")        # if you embed R inline


srun R --slave <<'EOF'
idx <- as.integer(Sys.getenv("SLURM_ARRAY_TASK_ID"))

library(data.table); library(catboost); library(dplyr); library(readr)

hp   <- fread("grid.csv")[idx]

data <- read_csv("~/rjobs/data.csv",
                 col_types = cols(Sale_Date_Raw = col_date("%Y-%m-%d")))
X <- data %>% select(-Sale_Price_Raw) %>%
        mutate(Sale_Date_Raw = as.numeric(Sale_Date_Raw),
               across(where(is.character), as.factor))
y <- data$Sale_Price_Raw
pool <- catboost.load_pool(X, label = y)

m <- catboost.train(pool, NULL, params = list(
         depth         = hp$depth,
         learning_rate = hp$learning_rate,
         iterations    = hp$iterations,
         l2_leaf_reg   = hp$l2_leaf_reg,
         rsm           = hp$rsm,
         border_count  = hp$border_count,
         loss_function = "RMSE",
         task_type     = "CPU",
         logging_level = "Silent"))

rmse <- sqrt(mean((catboost.predict(m, pool) - y)^2))
write.table(cbind(idx, rmse), file = sprintf("%03d.res", idx),
            row.names = FALSE, col.names = FALSE, sep = ",")
EOF
```

3. Submit batch job
```bash
sbatch catboost_array_02.sh
```

## First CV round

```{r, echo=FALSE, warning=FALSE}
files   <- list.files(here("models", "catboost_regressor", "rjobs", "res"), pattern = "\\.res$", full.names = TRUE)

results <- rbindlist(lapply(files, fread), fill = TRUE)

p <- ggplot(
  results,
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

After a first run with 5 hyperparameters the result is hard to interpret but we can clearly see that higher **border_count** yields overall lower RMSE. Therefor taking a closer look at border count 128.

```{r, echo=FALSE, warning=FALSE}
p <- ggplot(
  results[border_count == 128],
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

The filtered results for border_count 128 reveal that **tree depth** 9,10 and 11 show the most promising RMSE values. Furthermore **learning rate** of 0.03 performs best among these three tree depths. **l2_leaf_reg** of 3 is also performing best among these three.

Get model with best RMSE:

```{r, echo=FALSE}
best <- results[which.min(RMSE)]
best[,1:9]
```

## Second CV round
After identifying the best hyper-parameters in the first round, we can now run a second round with higher border count and fewer combinations for other hyper parameters.

```{r, echo=FALSE, warning=FALSE}
files   <- list.files(here("models", "catboost_regressor", "rjobs", "res_02"), pattern = "\\.res$", full.names = TRUE)

results <- rbindlist(lapply(files, fread), fill = TRUE)

p <- ggplot(
  results,
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

Again we can see that higher **border_count** yields overall lower RMSE. Therefor taking a closer look at border count 512.

```{r, echo=FALSE, warning=FALSE}
p <- ggplot(
  results[border_count == 512],
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

Surprisingly, this time a **learning rate** of 0.2 performs best.

Get model with lowest RMSE

```{r, echo=FALSE}
best <- results[which.min(RMSE)]
best[,1:9]
```

## Third VC round
After seeing the big drop between border_count of 256 and 512 I am convinced that we can go further with the border_count.

```{r, echo=FALSE, warning=FALSE}
files   <- list.files(here("models", "catboost_regressor", "rjobs", "res_03"), pattern = "\\.res$", full.names = TRUE)

results <- rbindlist(lapply(files, fread), fill = TRUE)

p <- ggplot(
  results,
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

First time we see that RMSE does not drop substantially with higher border_count. But we can see that **learning rate** of 0.03 performs best among the three tree depths. **l2_leaf_reg** of 3 is also performing best among these three.

```{r, echo=FALSE, warning=FALSE}
p <- ggplot(
  results[border_count == 8192],
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

Get model with lowest RMSE

```{r, echo=FALSE}
best <- results[which.min(RMSE)]
best[,1:9]
```

## Fourth VC round
Further exploring higher border_count.

```{r, echo=FALSE, warning=FALSE}
files   <- list.files(here("models", "catboost_regressor", "rjobs", "res_04"), pattern = "\\.res$", full.names = TRUE)

results <- rbindlist(lapply(files, fread), fill = TRUE)

p <- ggplot(
  results,
  aes(
    x      = factor(depth),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
    shape  = factor(l2_leaf_reg)
  )
) +
  geom_point(alpha = 0.85) +
  facet_wrap(~ border_count, nrow = 1) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

Get model with lowest RMSE

```{r, echo=FALSE}
best <- results[which.min(RMSE)]
best[,1:9]
```

## Fitfth VC round
After going back and forth and looking at the results I decided to run a final CV round with a range or **border_count** raging from 1024 (2^10) to 65535 (2^16 - 1 - which is the maximum for this parameter). A **l2_leaf_reg** of 3, **iterations** of 1000 and 1200, **learning rate** of 0.02 and 0.03 and a **three depth** of 10. 

```{r, echo=FALSE, warning=FALSE}
files   <- list.files(here("models", "catboost_regressor", "rjobs", "res_05"), pattern = "\\.res$", full.names = TRUE)

results <- rbindlist(lapply(files, fread), fill = TRUE)

p <- ggplot(
  results,
  aes(
    x      = factor(border_count),
    y      = RMSE,
    colour = factor(learning_rate),
    size   = factor(iterations),
  )
) +
  geom_point(alpha = 0.85) +
  labs(
    x      = "Tree depth",
    y      = "CV-RMSE",
    colour = "LR",
    size   = "Iter",
    shape  = "l2_leaf_reg"
  ) +
  theme_minimal()

p 
```

Get model with lowest RMSE

```{r echo=TRUE}
best <- results[which.min(RMSE)]
best[,1:9]
```

# Final Model

Based on the results of the CV rounds, we will use the following hyper-parameters for the final model:

```{r}
  params = list(
    depth         = 10,
    learning_rate = 0.03,
    iterations    = 10,
    loss_function = "RMSE",
    l2_leaf_reg   = 3,
    task_type     = "CPU",
    border_count  = 10	,
    logging_level = "Silent"
  )
```


```{r}
final_model <- catboost.train(
  learn_pool = train_pool,
  test_pool  = test_pool,
  params = params
)
toc()
evaluate_model(y_te, pred)
plot_predictions(y_te, pred)

# save model
catboost.save_model(final_model, 
                    model_path = here("models", "catboost_regressor", "model.cbm"))
```


```{r}

```

